# -*- coding: utf-8 -*-
"""
íšŒì‚¬ ë§ì¶¤ ë©´ì ‘ ì½”ì¹˜ (ì „ ë°©ì‹ ë³µêµ¬ + ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ëŸ¬ + ìì†Œì„œ ìœ ì§€ + í‰ê°€ í™•ì¥ + ì‹œê°í™” ê°œì„  + ìˆ˜ì •ë³¸ ì œê±°)

ìš”ì•½:
1) ì±„ìš© ê³µê³  URL â†’ ì›ë¬¸ ìˆ˜ì§‘(3ë‹¨ í´ë°±: Jina â†’ ì¼ë°˜ HTMLâ†’ bs4) â†’ LLM êµ¬ì¡°í™”(JSON) â†’ ê·œì¹™ íŒŒì„œ ë³´ì •
   + ì›í‹°ë“œ/ì‚¬ëŒì¸/ì¡ì½”ë¦¬ì•„ ì •ë°€ í¬ë¡¤ëŸ¬(ê²½ëŸ‰): í—¤ë”/ë¦¬ìŠ¤íŠ¸ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­ ì§ì ‘ ì¶”ì¶œ
2) íšŒì‚¬ ìš”ì•½: êµ¬ì¡°í™” ê²°ê³¼ í‘œì‹œ (íšŒì‚¬ëª…/ì†Œê°œ/ì§ë¬´/ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­)
3) ì´ë ¥ì„œ ì—…ë¡œë“œ(pdf/txt/md/docx) â†’ ë‚´ë¶€ ìë™ RAG ì¸ë±ìŠ¤
4) ìì†Œì„œ ìƒì„±: ê²°ê³¼ë¥¼ session_stateì— ì €ì¥í•˜ì—¬ ë‹¤ë¥¸ ë‹¨ê³„ í›„ì—ë„ ì‚¬ë¼ì§€ì§€ ì•ŠìŒ
5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ(RAG ê²°í•©)
6) ì±„ì  & ì½”ì¹­(ì—„ê²©) â€” 'ìˆ˜ì •ë³¸(STAR)' ì™„ì „ ì œê±°, í‰ê°€í•­ëª© í™•ì¥(ì´10ê°œ, ê°0~10ì , í•©ê³„100)
7) íŒ”ë¡œì—… ì§ˆë¬¸ & ë‹µë³€ & í”¼ë“œë°± â€” ë™ì¼í•œ í‰ê°€ ìŠ¤í‚¤ë§ˆ, 'ìˆ˜ì •ë³¸' ì—†ìŒ
8) í”¼ë“œë°±/ì‹œê°í™”: ê°œë³„ ë‹µë³€ ì„ íƒ ì‹œ ê¶¤ì (ë ˆì´ë”) ê°ê° í‘œì‹œ, ë¯¸ì„ íƒ ì‹œ í‰ê·  í‘œì‹œ
9) ë¦¬í¬íŠ¸/ë‹¤ìš´ë¡œë“œ: í•œê¸€ ì»¬ëŸ¼(ì§ˆë¬¸/í•©ê³„ ë“±)ë¡œ í‘œì¤€í™”

í•„ìˆ˜ íŒ¨í‚¤ì§€:
- openai, requests, beautifulsoup4, html2text, pypdf, docx2txt, lxml, numpy, pandas, plotly, streamlit
"""

import os, re, io, json, urllib.parse, tempfile
from typing import Optional, Tuple, Dict, List

import requests
from bs4 import BeautifulSoup
import html2text
import streamlit as st
import numpy as np
import pandas as pd

# Plotly (ë ˆì´ë” ì°¨íŠ¸)
try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

# ----------------------------- ìŠ¤íŠ¸ë¦¼ë¦¿ í˜ì´ì§€ ì„¤ì • -----------------------------
st.set_page_config(page_title="íšŒì‚¬ ë§ì¶¤ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")
st.title("íšŒì‚¬ ë§ì¶¤ ë©´ì ‘ ì½”ì¹˜ Â· URL ì •ì œ â†’ ìì†Œì„œ â†’ ì§ˆë¬¸/RAG/ì±„ì  â†’ íŒ”ë¡œì—…")

# ----------------------------- OpenAI í´ë¼ì´ì–¸íŠ¸ -----------------------------
try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”. requirements.txtì— openai ì¶”ê°€.")
    st.stop()

API_KEY = os.getenv("OPENAI_API_KEY") or (st.secrets.get("OPENAI_API_KEY", None) if hasattr(st, "secrets") else None)
if not API_KEY:
    API_KEY = st.text_input("OPENAI_API_KEY ì…ë ¥", type="password")
if not API_KEY:
    st.stop()

client = OpenAI(api_key=API_KEY)

with st.sidebar:
    st.subheader("ëª¨ë¸ ì„¤ì •")
    CHAT_MODEL = st.selectbox("ëŒ€í™”/ìƒì„± ëª¨ë¸", ["gpt-4o-mini","gpt-4o"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸(ë‚´ë¶€)", ["text-embedding-3-small","text-embedding-3-large"], index=0)

# ----------------------------- ê³µí†µ ìœ í‹¸ -----------------------------
def normalize_url(u: str) -> Optional[str]:
    """URLì„ https:// í˜•íƒœë¡œ í‘œì¤€í™”í•˜ê³ , #fragment ì œê±°."""
    if not u: return None
    u = u.strip()
    if not re.match(r"^https?://", u): u = "https://" + u
    parts = urllib.parse.urlsplit(u)
    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))

def http_get(url: str, timeout: int = 12) -> Optional[requests.Response]:
    """200 & text/htmlì¼ ë•Œë§Œ ë°˜í™˜. (ë¡œê·¸ì¸/ë™ì í˜ì´ì§€ëŠ” ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ)"""
    try:
        r = requests.get(
            url,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                              "(KHTML, like Gecko) Chrome/124 Safari/537.36",
                "Accept-Language": "ko, en;q=0.9",
            },
            timeout=timeout,
        )
        if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
            return r
    except Exception:
        pass
    return None

# ----------------------------- ì›ë¬¸ ìˆ˜ì§‘(3ë‹¨ í´ë°±) -----------------------------
def fetch_jina_text(url: str, timeout: int = 15) -> str:
    """
    1) Jina Reader í”„ë¡ì‹œ (ì •ì  ìŠ¤ëƒ…ìƒ· ë¹„ìŠ·í•œ ë·°) â€” ë¡œê·¸ì¸/ë´‡ì°¨ë‹¨ ìš°íšŒì— ìœ ë¦¬í•  ë•Œê°€ ìˆìŒ
    """
    try:
        parts = urllib.parse.urlsplit(url)
        prox = f"https://r.jina.ai/http://{parts.netloc}{parts.path}"
        if parts.query: prox += f"?{parts.query}"
        r = http_get(prox, timeout=timeout)
        return r.text.strip() if r else ""
    except Exception:
        return ""

def html_to_text(html_str: str) -> str:
    """2) ì¼ë°˜ HTML â†’ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ë³€í™˜(ë§í¬/ì´ë¯¸ì§€ ë¬´ì‹œ, ë„ˆë¹„ì œí•œ í•´ì œ)"""
    conv = html2text.HTML2Text()
    conv.ignore_links = True
    conv.ignore_images = True
    conv.body_width = 0
    txt = conv.handle(html_str)
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return txt.strip()

def fetch_webbase_text(url: str) -> str:
    r = http_get(url, timeout=12)
    if not r: return ""
    return html_to_text(r.text)

def fetch_bs4_text(url: str) -> Tuple[str, Optional[BeautifulSoup]]:
    """
    3) bs4 DOM íŒŒì‹± â†’ article/section/main/div/ul/ol í° ë©ì–´ë¦¬ ì¶”ì¶œ â†’ í•©ì¹¨
       soupì€ ë©”íƒ€(íšŒì‚¬ëª…/ì„¤ëª…/ì œëª©) ì¶”ì¶œì— ì‚¬ìš©
    """
    r = http_get(url, timeout=12)
    if not r: return "", None
    soup = BeautifulSoup(r.text, "lxml")
    blocks = []
    for sel in ["article","section","main","div","ul","ol"]:
        for el in soup.select(sel):
            txt = el.get_text(" ", strip=True)
            if txt and len(txt) > 300:
                txt = re.sub(r"\s+"," ", txt)
                blocks.append(txt)
    if not blocks:
        return soup.get_text(" ", strip=True)[:120000], soup
    seen, out = set(), []
    for b in blocks:
        if b not in seen:
            seen.add(b); out.append(b)
    return ("\n\n".join(out)[:120000], soup)

def fetch_all_text(url: str):
    """Jina â†’ ì¼ë°˜ HTML â†’ bs4 ìˆœì„œë¡œ ì‹œë„. (í…ìŠ¤íŠ¸, ë©”íƒ€ì •ë³´, soup ë°˜í™˜)"""
    url = normalize_url(url)
    if not url: return "", {"error":"invalid_url"}, None
    jina = fetch_jina_text(url)
    if jina:
        _, soup = fetch_bs4_text(url)
        return jina, {"source":"jina","len":len(jina),"url_final":url}, soup
    web = fetch_webbase_text(url)
    if web:
        _, soup = fetch_bs4_text(url)
        return web, {"source":"webbase","len":len(web),"url_final":url}, soup
    bs, soup = fetch_bs4_text(url)
    return bs, {"source":"bs4","len":len(bs),"url_final":url}, soup

# ----------------------------- ë©”íƒ€ ì¶”ì¶œ(íšŒì‚¬ëª…/ì†Œê°œ/ì œëª©) -----------------------------
def extract_company_meta(soup: Optional[BeautifulSoup]) -> Dict[str,str]:
    meta = {"company_name":"","company_intro":"","job_title":""}
    if not soup: return meta
    cand = []
    og = soup.find("meta", {"property":"og:site_name"})
    if og and og.get("content"): cand.append(og["content"])
    app = soup.find("meta", {"name":"application-name"})
    if app and app.get("content"): cand.append(app["content"])
    if soup.title and soup.title.string: cand.append(soup.title.string)
    cand = [re.split(r"[\-\|\Â·\â€”]", c)[0].strip() for c in cand if c]
    cand = [c for c in cand if 2 <= len(c) <= 40]
    meta["company_name"] = cand[0] if cand else ""
    md = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
    if md and md.get("content"):
        meta["company_intro"] = re.sub(r"\s+"," ", md["content"]).strip()[:500]
    jt = ""
    ogt = soup.find("meta", {"property":"og:title"})
    if ogt and ogt.get("content"): jt = ogt["content"]
    if not jt:
        h1 = soup.find("h1")
        if h1 and h1.get_text(): jt = h1.get_text(strip=True)
    if not jt:
        h2 = soup.find("h2")
        if h2 and h2.get_text(): jt = h2.get_text(strip=True)
    meta["job_title"] = re.sub(r"\s+"," ", jt).strip()[:120]
    return meta

# ----------------------------- ì‚¬ì´íŠ¸ë³„ ì •ë°€ í¬ë¡¤ëŸ¬ (ê²½ëŸ‰) -----------------------------
def _find_section_by_header(soup: BeautifulSoup, header_keywords, max_take=20) -> List[str]:
    """í—¤ë” í…ìŠ¤íŠ¸(ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­ ë“±)ë¥¼ ì°¾ì•„ ë‹¤ìŒ í˜•ì œ ìš”ì†Œì˜ ë¦¬ìŠ¤íŠ¸/ë¬¸ë‹¨ì„ ìˆ˜ì§‘."""
    if not soup: return []
    # í—¤ë” í›„ë³´: h1~h4, strong, b, span(ç²—)
    headers = soup.find_all(["h1","h2","h3","h4","strong","b","span"])
    out=[]
    for h in headers:
        ht = (h.get_text(" ", strip=True) or "").lower()
        if any(k in ht for k in header_keywords):
            # í˜•ì œ ë°©í–¥ìœ¼ë¡œ ul/ol/li/pë¥¼ ë”°ë¼ê°€ë©° bullet ìˆ˜ì§‘
            sib = h
            take=[]
            steps=0
            while sib and steps<10:
                sib = sib.find_next_sibling()
                if not sib: break
                steps += 1
                txts=[]
                if sib.name in ["ul","ol"]:
                    for li in sib.find_all("li", recursive=False):
                        tv = li.get_text(" ", strip=True)
                        if tv: txts.append(tv)
                elif sib.name in ["p","div"]:
                    tv = sib.get_text(" ", strip=True)
                    if tv: txts.append(tv)
                for t in txts:
                    t = re.sub(r"\s+"," ", t).strip(" -â€¢Â·â–¶â–ªï¸").strip()
                    if 3 <= len(t) <= 300:
                        out.append(t)
                if len(out) >= max_take: break
            if out: break
    # ì¤‘ë³µ ì œê±°
    seen=set(); clean=[]
    for s in out:
        if s not in seen:
            seen.add(s); clean.append(s[:180])
    return clean

def parse_wanted(soup: BeautifulSoup) -> Dict[str, List[str]]:
    """ì›í‹°ë“œ: í—¤ë” ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±(ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­)"""
    return {
        "responsibilities": _find_section_by_header(soup, ["ì£¼ìš” ì—…ë¬´","ë‹´ë‹¹ ì—…ë¬´","responsibilities","what you will do"]),
        "qualifications":   _find_section_by_header(soup, ["ìê²© ìš”ê±´","ì§€ì› ìê²©","requirements","qualifications","must have"]),
        "preferences":      _find_section_by_header(soup, ["ìš°ëŒ€ ì‚¬í•­","ìš°ëŒ€","preferred","nice to have","plus"]),
    }

def parse_saramin(soup: BeautifulSoup) -> Dict[str, List[str]]:
    """ì‚¬ëŒì¸: ìœ ì‚¬ íœ´ë¦¬ìŠ¤í‹±"""
    return {
        "responsibilities": _find_section_by_header(soup, ["ë‹´ë‹¹ì—…ë¬´","ë‹´ë‹¹ ì—…ë¬´","ì£¼ìš” ì—…ë¬´","ì—…ë¬´ë‚´ìš©"]),
        "qualifications":   _find_section_by_header(soup, ["ìê²©ìš”ê±´","ìê²© ìš”ê±´","ì§€ì›ìê²©","í•™ë ¥/ê²½ë ¥","í•„ìˆ˜"]),
        "preferences":      _find_section_by_header(soup, ["ìš°ëŒ€ì‚¬í•­","ìš°ëŒ€ ì‚¬í•­","ìš°ëŒ€","ê°€ì‚°ì ","ìš°ëŒ€ì¡°ê±´"]),
    }

def parse_jobkorea(soup: BeautifulSoup) -> Dict[str, List[str]]:
    """ì¡ì½”ë¦¬ì•„: ìœ ì‚¬ íœ´ë¦¬ìŠ¤í‹±"""
    return {
        "responsibilities": _find_section_by_header(soup, ["ë‹´ë‹¹ì—…ë¬´","ì£¼ìš”ì—…ë¬´","ì—…ë¬´ë‚´ìš©"]),
        "qualifications":   _find_section_by_header(soup, ["ìê²©ìš”ê±´","ì§€ì›ìê²©","í•„ìˆ˜"]),
        "preferences":      _find_section_by_header(soup, ["ìš°ëŒ€ì‚¬í•­","ìš°ëŒ€","ìš°ëŒ€ì¡°ê±´"]),
    }

def try_site_specific(url: str, soup: Optional[BeautifulSoup]) -> Dict[str, List[str]]:
    """
    ë„ë©”ì¸ ì¸ì‹ í›„ ì‚¬ì´íŠ¸ë³„ íŒŒì„œ ì‹œë„ â†’ ì¼ë¶€ë¼ë„ ì–»ìœ¼ë©´ ë°˜í™˜, ì•„ë‹ˆë©´ ë¹ˆ dict
    """
    if not soup: return {}
    dom = urllib.parse.urlsplit(url).netloc.lower()
    if "wanted.co.kr" in dom:
        return parse_wanted(soup)
    if "saramin.co.kr" in dom:
        return parse_saramin(soup)
    if "jobkorea.co.kr" in dom:
        return parse_jobkorea(soup)
    return {}

# ----------------------------- ê·œì¹™ íŒŒì„œ(ìš°ëŒ€ì‚¬í•­ ë³´ì •/ë³´ì™„) -----------------------------
def rule_based_sections(raw_text: str) -> dict:
    """
    í—¤ë”/í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ responsibilities/qualifications/preferencesë¥¼ ìµœëŒ€í•œ ì±„ì›€.
    LLM êµ¬ì¡°í™”ê°€ ë¹ˆì•½í•  ë•Œ ë³´ì™„ ì—­í• .
    """
    txt = re.sub(r"\r", "", raw_text or "").strip()
    lines = [re.sub(r"\s+", " ", l).strip(" -â€¢Â·â–¶â–ªï¸") for l in txt.split("\n")]
    lines = [l for l in lines if l]

    hdr_resp = re.compile(r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Role|Responsibilities?)", re.I)
    hdr_qual = re.compile(r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements?|Qualifications?)", re.I)
    hdr_pref = re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|Preferred|Nice\s*to\s*have|Plus)", re.I)

    bucket = None
    out = {"responsibilities": [], "qualifications": [], "preferences": []}

    def push(line, b):
        if line and len(line) > 1 and line not in out[b]:
            out[b].append(line[:180])

    for l in lines:
        if hdr_resp.search(l): bucket = "responsibilities"; continue
        if hdr_qual.search(l): bucket = "qualifications"; continue
        if hdr_pref.search(l): bucket = "preferences"; continue

        if bucket is None:
            if re.search(r"(ìš°ëŒ€|preferred|nice to have|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", l, re.I):
                bucket = "preferences"
            elif any(k in l.lower() for k in ["java","python","spark","airflow","kafka","ml","sql","aws","docker","k8s"]):
                bucket = "responsibilities"
            else:
                continue
        push(l, bucket)

    # ìê²©ìš”ê±´ì— ì„ì¸ ìš°ëŒ€ ì¤„ ì´ë™
    kw_pref = re.compile(r"(ìš°ëŒ€|preferred|nice to have|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
    remain_qual = []
    for q in out["qualifications"]:
        if kw_pref.search(q):
            out["preferences"].append(q)
        else:
            remain_qual.append(q)
    out["qualifications"] = remain_qual

    # ì¤‘ë³µ/ê¸¸ì´ ì œí•œ
    for k in out:
        seen=set(); clean=[]
        for s in out[k]:
            s = re.sub(r"\s+", " ", s).strip(" -â€¢Â·â–¶â–ªï¸").strip()
            if s and s not in seen:
                seen.add(s); clean.append(s)
        out[k] = clean[:20]
    return out

# ----------------------------- LLM êµ¬ì¡°í™” (ì „ ë°©ì‹ ë³µêµ¬) -----------------------------
PROMPT_SYSTEM_STRUCT = (
    "ë„ˆëŠ” ì±„ìš© ê³µê³ ë¥¼ ê¹”ë”í•˜ê²Œ êµ¬ì¡°í™”í•˜ëŠ” ë³´ì¡°ì›ì´ë‹¤. "
    "ì…ë ¥ í…ìŠ¤íŠ¸ëŠ” í¬í„¸ ê´‘ê³  ë¬¸êµ¬, UIì”ì¬, ë³µìˆ˜ ì§ë¬´ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆë‹¤. "
    "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì¤‘ë³µì—†ì´ ì •ì œí•˜ë¼."
)

def llm_structurize(raw_text: str, meta_hint: Dict[str,str], model: str) -> Dict:
    """
    ì›ë¬¸ â†’ JSON(íšŒì‚¬ëª…, ì†Œê°œ, ì§ë¬´ëª…, ì£¼ìš”ì—…ë¬´, ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­)
    - LLMìœ¼ë¡œ 1ì°¨ êµ¬ì¡°í™”
    - ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ëŸ¬/ê·œì¹™ íŒŒì„œë¡œ ë³´ê°• (íŠ¹íˆ ìš°ëŒ€ì‚¬í•­)
    """
    ctx = (raw_text or "").strip()
    if len(ctx) > 14000:
        ctx = ctx[:14000]

    user_msg = {
        "role": "user",
        "content": (
            "ë‹¤ìŒ ì±„ìš© ê³µê³  ì›ë¬¸ì„ êµ¬ì¡°í™”í•´ì¤˜.\n\n"
            f"[íŒíŠ¸] íšŒì‚¬ëª… í›„ë³´: {meta_hint.get('company_name','')}\n"
            f"[íŒíŠ¸] ì§ë¬´ëª… í›„ë³´: {meta_hint.get('job_title','')}\n"
            "--- ì›ë¬¸ ì‹œì‘ ---\n"
            f"{ctx}\n"
            "--- ì›ë¬¸ ë ---\n\n"
            "JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , í‚¤ëŠ” ë°˜ë“œì‹œ ì•„ë˜ë§Œ í¬í•¨:\n"
            "{"
            "\"company_name\": str, "
            "\"company_intro\": str, "
            "\"job_title\": str, "
            "\"responsibilities\": [str], "
            "\"qualifications\": [str], "
            "\"preferences\": [str]"
            "}\n"
            "- 'ìš°ëŒ€ ì‚¬í•­(preferences)'ì€ ë¹„ì›Œë‘ì§€ ë§ê³ , ì›ë¬¸ì—ì„œ 'ìš°ëŒ€/ì„ í˜¸/Preferred/Nice to have/Plus' ë“± í‘œì‹œê°€ ìˆëŠ” í•­ëª©ì„ ê·¸ëŒ€ë¡œ ë‹´ì•„ë¼.\n"
            "- ë¶ˆë¦¿/ë§ˆì»¤/ì´ëª¨ì§€ ì œê±°, ë¬¸ì¥ ê°„ê²°í™”, ì¤‘ë³µ ì œê±°."
        ),
    }

    data = {
        "company_name": meta_hint.get("company_name",""),
        "company_intro": meta_hint.get("company_intro",""),
        "job_title": meta_hint.get("job_title",""),
        "responsibilities": [],
        "qualifications": [],
        "preferences": [],
    }
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2,
            response_format={"type": "json_object"},
            messages=[{"role":"system","content":PROMPT_SYSTEM_STRUCT}, user_msg],
        )
        data_llm = json.loads(resp.choices[0].message.content)
        # ë³‘í•©(LLM ê²°ê³¼ ìš°ì„ , íŒíŠ¸ë¡œ ë¹ˆ ê°’ ì±„ì›€)
        for k in data:
            if k in data_llm and data_llm[k]:
                data[k] = data_llm[k]
    except Exception:
        pass

    # 1ì°¨ í´ë¦°ì—…
    for k in ["responsibilities","qualifications","preferences"]:
        arr = data.get(k, [])
        if not isinstance(arr, list): arr = []
        clean=[]; seen=set()
        for it in arr:
            t = re.sub(r"\s+"," ", str(it)).strip(" -â€¢Â·â–¶â–ªï¸").strip()
            if t and t not in seen:
                seen.add(t); clean.append(t[:200])
        data[k] = clean[:20]

    for k in ["company_name","company_intro","job_title"]:
        if k in data and isinstance(data[k], str):
            data[k] = re.sub(r"\s+"," ", data[k]).strip()

    return data

# ----------------------------- íŒŒì¼ ë¦¬ë” (pdf/txt/md/docx) -----------------------------
try:
    import pypdf
except Exception:
    pypdf = None

def read_pdf(data: bytes) -> str:
    if pypdf is None:
        return ""
    try:
        reader = pypdf.PdfReader(io.BytesIO(data))
        return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
    except Exception:
        return ""

def read_docx(data: bytes) -> str:
    try:
        import docx2txt
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=True) as tmp:
            tmp.write(data); tmp.flush()
            return docx2txt.process(tmp.name) or ""
    except Exception:
        return ""

def read_file_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt",".md")):
        for enc in ("utf-8","cp949","euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        return read_pdf(data)
    elif name.endswith(".docx"):
        return read_docx(data)
    return ""

# ----------------------------- ê°„ë‹¨ RAG (ë‚´ë¶€ ìë™ íŒŒë¼ë¯¸í„°) -----------------------------
def chunk(text: str, size: int = 600, overlap: int = 120) -> List[str]:
    t = re.sub(r"\s+"," ", text).strip()
    if not t: return []
    out, start = [], 0
    while start < len(t):
        end = min(len(t), start+size)
        out.append(t[start:end])
        if end == len(t): break
        start = max(0, end-overlap)
    return out

def embed_texts(texts: List[str], model_name: str) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=model_name, input=texts)
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    return vecs

def cosine_topk(matrix: np.ndarray, query_vec: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query_vec / (np.linalg.norm(query_vec, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_resume_chunks(query: str, k: int = 4):
    chs, embs = st.session_state.get("resume_chunks", []), st.session_state.get("resume_embeds", None)
    if not chs or embs is None:
        return []
    qv = embed_texts([query], EMBED_MODEL)
    scores, idxs = cosine_topk(embs, qv, k=k)
    return [(float(s), chs[int(i)]) for s, i in zip(scores, idxs)]

# ----------------------------- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -----------------------------
def _init_state():
    defaults = dict(
        clean_struct=None,
        raw_source_text="",
        site_sections=None,
        resume_raw="",
        resume_chunks=[],
        resume_embeds=None,
        cover_letter="",
        current_question="",
        answer_text="",
        records=[],  # [{ì§ˆë¬¸, í•©ê³„, criteria:{ê¸°ì¤€ëª…:ì ìˆ˜}, ì½”ë©˜íŠ¸ë“¤...}]
        followups=[],  # ì œì•ˆ ë¦¬ìŠ¤íŠ¸
        selected_followup="",
        followup_answer="",
        last_result=None,
        last_followup_result=None,
    )
    for k,v in defaults.items():
        if k not in st.session_state: st.session_state[k]=v
_init_state()

# ----------------------------- 1) ì±„ìš© ê³µê³  URL â†’ ì •ì œ -----------------------------
st.header("1) ì±„ìš© ê³µê³  URL â†’ ì •ì œ")
job_url = st.text_input("ì±„ìš© ê³µê³  ìƒì„¸ URL", placeholder="ì˜ˆ: https://www.wanted.co.kr/wd/123456")
if st.button("ì›ë¬¸ ìˆ˜ì§‘ â†’ ì •ì œ", type="primary"):
    if not job_url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ì›ë¬¸ ìˆ˜ì§‘ ì¤‘..."):
            raw, meta, soup = fetch_all_text(job_url.strip())
            st.session_state.raw_source_text = raw or ""
            site_parts = try_site_specific(job_url.strip(), soup)  # ì‚¬ì´íŠ¸ë³„ ì •ë°€ ì¶”ì¶œ(ê°€ëŠ¥í•œ ê²½ìš°)
        if not raw:
            st.error("ì›ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¡œê·¸ì¸/ë™ì  ë Œë”ë§/ë´‡ ì°¨ë‹¨ ê°€ëŠ¥)")
        else:
            # LLM êµ¬ì¡°í™” (ì „ ë°©ì‹)
            with st.spinner("LLMìœ¼ë¡œ êµ¬ì¡°í™” ì¤‘..."):
                hint = extract_company_meta(soup)
                clean = llm_structurize(raw, hint, CHAT_MODEL)

            # ì‚¬ì´íŠ¸ë³„ íŒŒì‹± ê²°ê³¼ë¥¼ LLM êµ¬ì¡°í™”ì— ë³´ê°•(ìš°ì„  ì ìš©)
            if site_parts:
                for k in ["responsibilities","qualifications","preferences"]:
                    lst = (site_parts.get(k) or []) + (clean.get(k) or [])
                    seen=set(); merged=[]
                    for x in lst:
                        x=x.strip()
                        if x and x not in seen:
                            seen.add(x); merged.append(x)
                    clean[k] = merged[:20]

            # ê·œì¹™ íŒŒì„œë¡œ ìµœì¢… ë³´ê°•(íŠ¹íˆ ìš°ëŒ€ì‚¬í•­)
            rb = rule_based_sections(raw)
            for k in ["responsibilities","qualifications","preferences"]:
                lst = (clean.get(k) or []) + (rb.get(k) or [])
                seen=set(); merged=[]
                for x in lst:
                    x=x.strip()
                    if x and x not in seen:
                        seen.add(x); merged.append(x)
                clean[k] = merged[:20]

            st.session_state.clean_struct = clean
            st.session_state.site_sections = site_parts or {}
            st.success("ì •ì œ ì™„ë£Œ!")

# ----------------------------- 2) íšŒì‚¬ ìš”ì•½ (ì •ì œ ê²°ê³¼ í‘œì‹œ) -----------------------------
st.header("2) íšŒì‚¬ ìš”ì•½ (ì •ì œ ê²°ê³¼)")
clean = st.session_state.clean_struct
if clean:
    st.markdown(f"**íšŒì‚¬ëª…:** {clean.get('company_name','-')}")
    st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½):** {clean.get('company_intro','-')}")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼(ì§ë¬´ëª…):** {clean.get('job_title','-')}")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´**")
        rs = clean.get("responsibilities", [])
        if rs:
            for b in rs: st.markdown(f"- {b}")
        else:
            st.caption("ì£¼ìš” ì—…ë¬´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    with c2:
        st.markdown("**ìê²© ìš”ê±´**")
        qs = clean.get("qualifications", [])
        if qs:
            for b in qs: st.markdown(f"- {b}")
        else:
            st.caption("ìê²© ìš”ê±´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­**")
        ps = clean.get("preferences", [])
        if ps:
            for b in ps: st.markdown(f"- {b}")
        else:
            st.caption("ìš°ëŒ€ ì‚¬í•­ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("ë¨¼ì € URLì„ ì •ì œí•´ ì£¼ì„¸ìš”.")

st.divider()

# ----------------------------- 3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ -----------------------------
st.header("3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ")
uploads = st.file_uploader("ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ ê°€ëŠ¥", type=["pdf","txt","md","docx"], accept_multiple_files=True)
if st.button("ì´ë ¥ì„œ ì¸ë±ì‹±(ìë™)", type="secondary"):
    if not uploads:
        st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        all_text=[]
        for up in uploads:
            t = read_file_text(up)
            if t: all_text.append(t)
        resume_text = "\n\n".join(all_text)
        if not resume_text.strip():
            st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            chunks = chunk(resume_text, size=600, overlap=120)  # ë‚´ë¶€ ìë™ íŒŒë¼ë¯¸í„°
            with st.spinner("ì´ë ¥ì„œ ë²¡í„°í™” ì¤‘..."):
                embeds = embed_texts(chunks, EMBED_MODEL)
            st.session_state.resume_raw = resume_text
            st.session_state.resume_chunks = chunks
            st.session_state.resume_embeds = embeds
            st.success(f"ì¸ë±ì‹± ì™„ë£Œ (ì²­í¬ {len(chunks)}ê°œ)")

st.divider()

# ----------------------------- 4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„± (ì„¸ì…˜ ìœ ì§€) -----------------------------
st.header("4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„±")
topic = st.text_input("íšŒì‚¬ ìš”ì²­ ì£¼ì œ(ì„ íƒ)", placeholder="ì˜ˆ: ì§ë¬´ ì§€ì›ë™ê¸° / í˜‘ì—… ê²½í—˜ / ë¬¸ì œí•´ê²° ì‚¬ë¡€ ë“±")

def build_cover_letter(clean_struct: Dict, resume_text: str, topic_hint: str, model: str) -> str:
    """íšŒì‚¬ ìš”ì•½ + ì´ë ¥ì„œ â†’ íšŒì‚¬ íŠ¹í™” ìì†Œì„œ(600~900ì) â€” ê²°ê³¼ëŠ” ì„¸ì…˜ì— ì˜êµ¬ ì €ì¥"""
    company = json.dumps(clean_struct or {}, ensure_ascii=False)
    resume_snippet = (resume_text or "").strip()
    if len(resume_snippet) > 9000: resume_snippet = resume_snippet[:9000]
    system = (
        "ë„ˆëŠ” í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì „ë¬¸ê°€ë‹¤. ì±„ìš© ê³µê³ ì˜ íšŒì‚¬/ì§ë¬´ ìš”ê±´ê³¼ í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ì°¸ê³ í•´ "
        "íšŒì‚¬ íŠ¹í™” ìì†Œì„œë¥¼ ì‘ì„±í•œë‹¤. ê³¼ì¥/í—ˆìœ„ëŠ” ê¸ˆì§€í•˜ê³ , ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ì„íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´í™”í•œë‹¤."
    )
    req = f"íšŒì‚¬ ì¸¡ ìš”ì²­ ì£¼ì œëŠ” '{topic_hint.strip()}' ì´ë‹¤. ì´ ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•˜ë¼." if topic_hint else \
          "íŠ¹ì • ì£¼ì œê°€ ì—†ìœ¼ë¯€ë¡œ, ì±„ìš© ê³µê³ ì™€ ì§ë¬´ì í•©ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì§€ì›ë™ê¸°ì™€ í•µì‹¬ì—­ëŸ‰ì„ ê°•ì¡°í•˜ë¼."
    user = (
        f"[íšŒì‚¬/ì§ë¬´ ìš”ì•½(JSON)]\n{company}\n\n"
        f"[í›„ë³´ì ì´ë ¥ì„œ]\n{resume_snippet}\n\n"
        f"[ì‘ì„± ì§€ì‹œ]\n- {req}\n"
        "- ë¶„ëŸ‰: 600~900ì\n"
        "- êµ¬ì„±: 1) ì§€ì› ë™ê¸° 2) ì§ë¬´ ê´€ë ¨ í•µì‹¬ ì—­ëŸ‰Â·ê²½í—˜ 3) ì„±ê³¼/ì§€í‘œ 4) ì…ì‚¬ í›„ ê¸°ì—¬ ë°©ì•ˆ 5) ë§ˆë¬´ë¦¬\n"
        "- ë¶ˆí•„ìš”í•œ ë¯¸ì‚¬ì—¬êµ¬/ì¤‘ë³µ/ê´‘ê³  ë¬¸êµ¬ ì‚­ì œ."
    )
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.4,
            messages=[{"role":"system","content":system},{"role":"user","content":user}]
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(ìì†Œì„œ ìƒì„± ì‹¤íŒ¨: {e})"

if st.button("ìì†Œì„œ ìƒì„±", type="primary"):
    if not st.session_state.clean_struct:
        st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
    elif not st.session_state.resume_raw.strip():
        st.warning("ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ì´ë ¥ì„œ ì¸ë±ì‹±(ìë™)'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ìì†Œì„œ ìƒì„± ì¤‘..."):
            cover = build_cover_letter(st.session_state.clean_struct, st.session_state.resume_raw, topic, CHAT_MODEL)
        st.session_state.cover_letter = cover  # âœ… ì„¸ì…˜ì— ì €ì¥ â€” ì´í›„ ë‹¨ê³„ì—ì„œë„ ìœ ì§€
        st.success("ìì†Œì„œ ìƒì„± ì™„ë£Œ!")

# âœ… ìì†Œì„œëŠ” í•­ìƒ í™”ë©´ì— ìœ ì§€í•´ì„œ, ì´í›„ ë‹¨ê³„(ì§ˆë¬¸/ì±„ì  ë“±) ì§„í–‰í•´ë„ ì‚¬ë¼ì§€ì§€ ì•ŠìŒ
if st.session_state.cover_letter:
    st.subheader("ìì†Œì„œ (ìƒì„± ê²°ê³¼)")
    st.write(st.session_state.cover_letter)
    st.download_button("ìì†Œì„œ TXT ë‹¤ìš´ë¡œë“œ", data=st.session_state.cover_letter.encode("utf-8"),
                       file_name="cover_letter.txt", mime="text/plain")

st.divider()

# ----------------------------- 5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ (RAG ê²°í•©) -----------------------------
st.header("5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ (RAG ê²°í•©)")
level = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"], index=0)

def llm_generate_one_question_with_resume(clean: Dict, level: str, model: str) -> str:
    hits = retrieve_resume_chunks("í•µì‹¬ í”„ë¡œì íŠ¸ì™€ ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½", k=4)
    resume_context = "\n".join([f"- {t[:350]}" for _, t in hits])[:1200]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {
        "role": "user",
        "content": (
            f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´]\n{ctx}\n\n"
            f"[ì§€ì›ì ì´ë ¥ì„œ ìš”ì•½(ë°œì·Œ)]\n{resume_context}\n\n"
            f"[ìš”ì²­]\n- ë‚œì´ë„/ì—°ì°¨: {level}\n"
            f"- ì¤‘ë³µ/ìœ ì‚¬ë„ ì§€ì–‘, íšŒì‚¬ ìš”ê±´ê³¼ ì´ë ¥ì„œì˜ êµì§‘í•© ë˜ëŠ” ê³µë°±ì˜ì—­ ê²¨ëƒ¥\n"
            f"- í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë§Œ í•œ ì¤„ë¡œ ì¶œë ¥"
        ),
    }
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.85,
            messages=[{"role":"system","content":"ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸°"}, user_msg],
        )
        q = resp.choices[0].message.content.strip()
        q = re.sub(r"^\s*\d+[\).\s-]*","", q).strip()
        q = q.split("\n")[0].strip()
        return q
    except Exception:
        return ""

def llm_draft_answer(clean: Dict, question: str, model: str) -> str:
    hits = retrieve_resume_chunks(question, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {
        "role":"user",
        "content": (
            f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
            f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
            f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
            "STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼) ê¸°ë°˜ í•œêµ­ì–´ ë‹µë³€ **ì´ˆì•ˆ**ì„ 8~12ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. "
            "ê°€ëŠ¥í•˜ë©´ ì§€í‘œ/ìˆ˜ì¹˜/ê¸°ê°„/ì„íŒ©íŠ¸ë¥¼ í¬í•¨."
        )
    }
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.5,
            messages=[{"role":"system","content":"ë©´ì ‘ ë‹µë³€ ì´ˆì•ˆê¸°"}, user_msg],
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        return ""

cols_q = st.columns(2)
with cols_q[0]:
    if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", type="primary"):
        if not st.session_state.clean_struct:
            st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
        else:
            q = llm_generate_one_question_with_resume(st.session_state.clean_struct, level, CHAT_MODEL)
            if q:
                st.session_state.current_question = q
                st.session_state.answer_text = ""   # ì´ì „ ë‹µë³€ ì´ˆê¸°í™”
                st.session_state.last_result = None
                st.session_state.followups = []
                st.session_state.selected_followup = ""
                st.session_state.followup_answer = ""
                st.success("ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
with cols_q[1]:
    if st.button("RAGë¡œ ë‹µë³€ ì´ˆì•ˆ ìƒì„±", type="secondary"):
        if not st.session_state.current_question:
            st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        else:
            draft = llm_draft_answer(st.session_state.clean_struct, st.session_state.current_question, CHAT_MODEL)
            if draft:
                st.session_state.answer_text = draft
                st.success("ì´ˆì•ˆ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨")

st.text_area("ì§ˆë¬¸", value=st.session_state.current_question, height=100)
st.text_area("ë‚˜ì˜ ë‹µë³€ (ì´ˆì•ˆì„ í¸ì§‘í•´ ì™„ì„±í•˜ì„¸ìš”)", height=200, key="answer_text")

st.divider()

# ----------------------------- 6) ì±„ì  & ì½”ì¹­ (í™•ì¥ëœ í‰ê°€ í•­ëª©, ìˆ˜ì •ë³¸ ì œê±°) -----------------------------
st.header("6) ì±„ì  & ì½”ì¹­ (í™•ì¥ ê¸°ì¤€, ì—„ê²© ëª¨ë“œ)")

# í‰ê°€ í•­ëª© í™•ì¥ (ì´ 10ê°œ, ê° 0~10ì , í•©ê³„ 100)
CRITERIA = [
    "ë¬¸ì œì •ì˜", "ë°ì´í„°/ì§€í‘œ", "ì‹¤í–‰ë ¥/ì£¼ë„ì„±", "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ê³ ê°ê°€ì¹˜",
    "ë¬¸ì œí•´ê²°", "ë¦¬ìŠ¤í¬/í’ˆì§ˆ", "ë¹„ì¦ˆë‹ˆìŠ¤ì„íŒ©íŠ¸", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ëª…ë£Œì„±", "êµ¬ì¡°í™”/ë…¼ë¦¬"
]

PROMPT_SYSTEM_SCORE_STRICT = (
    "ë„ˆëŠ” ë§¤ìš° ì—„ê²©í•œ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë§Œ ì¶œë ¥í•˜ë¼. "
    "ê° ê¸°ì¤€ì€ 0~10 ì •ìˆ˜ì´ë©°, ì´ì (overall)ì€ ê¸°ì¤€ í•©ê³„(ìµœëŒ€ 100)ì™€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•œë‹¤. "
    "ê³¼ì¥/ëª¨í˜¸í•¨/ê·¼ê±° ë¶€ì¬/ìˆ«ì ì—†ëŠ” ì£¼ì¥/ì±…ì„ íšŒí”¼/ëª¨í˜¸í•œ ì£¼ì–´ ì‚¬ìš© ë“±ì„ ê°•í•˜ê²Œ ê°ì í•˜ë¼. "
    "ê° ê¸°ì¤€ì— ëŒ€í•´ ì§§ê³  êµ¬ì²´ì ì¸ ì½”ë©˜íŠ¸ë¥¼ ì œê³µí•˜ë¼. ìˆ˜ì •ë³¸ ë‹µë³€ì€ ì œê³µí•˜ì§€ ë§ˆë¼."
)

def llm_score_and_coach_strict(clean: Dict, question: str, answer: str, model: str) -> Dict:
    hits = retrieve_resume_chunks(question + "\n" + answer[:800], k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)

    # â— ìˆ˜ì •ë³¸ ë‹µë³€ ì œê±°: ìŠ¤í‚¤ë§ˆì—ì„œ ì œì™¸
    schema = (
        "{"
        "\"overall\": 0~100 ì •ìˆ˜,"
        "\"criteria\": ["
        + ",".join([f"{{\"name\":\"{c}\",\"score\":0~10,\"comment\":\"...\"}}" for c in CRITERIA]) +
        "],"
        "\"strengths\": [\"...\",\"...\"],"
        "\"risks\": [\"...\",\"...\"],"
        "\"improvements\": [\"...\",\"...\",\"...\"]"
        "}"
    )

    user_msg = {
        "role":"user",
        "content": (
            f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
            f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
            f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
            f"[ì§€ì›ì ë‹µë³€]\n{answer}\n\n"
            f"ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ í•œêµ­ì–´ ì‘ë‹µ:\n{schema}"
        )
    }
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2,
            response_format={"type":"json_object"},
            messages=[{"role":"system","content":PROMPT_SYSTEM_SCORE_STRICT}, user_msg]
        )
        data = json.loads(resp.choices[0].message.content)
        crit = data.get("criteria", [])
        # ê¸°ì¤€ ë³´ì •/ì •ë ¬
        fixed=[]
        # ë“¤ì–´ì˜¨ ê¸°ì¤€ì„ ë”•íŠ¸í™”
        got = {str(it.get("name","")).strip(): it for it in crit if isinstance(it, dict)}
        for name in CRITERIA:
            it = got.get(name, {"name":name, "score":0, "comment":""})
            sc = int(it.get("score",0)); sc=max(0,min(10, sc))
            fixed.append({"name":name, "score":sc, "comment":str(it.get("comment","")).strip()})
        total = sum(x["score"] for x in fixed)  # 0~100
        data = {
            "overall": int(total),
            "criteria": fixed,
            "strengths": [s for s in data.get("strengths", []) if str(s).strip()][:5],
            "risks": [s for s in data.get("risks", []) if str(s).strip()][:5],
            "improvements": [s for s in data.get("improvements", []) if str(s).strip()][:5],
        }
        return data
    except Exception as e:
        return {
            "overall": 0,
            "criteria": [{"name": n, "score": 0, "comment": ""} for n in CRITERIA],
            "strengths": [], "risks": [], "improvements": [],
            "error": str(e),
        }

if st.button("ì±„ì  & ì½”ì¹­ ì‹¤í–‰", type="primary"):
    if not st.session_state.current_question:
        st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì±„ì /ì½”ì¹­ ì¤‘..."):
            res = llm_score_and_coach_strict(
                st.session_state.clean_struct,
                st.session_state.current_question,
                st.session_state.answer_text,
                CHAT_MODEL
            )
        st.session_state.last_result = res
        # ê¸°ë¡ìš©(ì§ˆë¬¸/í•©ê³„/ê°œë³„ ê¸°ì¤€ ì ìˆ˜)
        crit_map = {c["name"]: c["score"] for c in res.get("criteria", [])}
        st.session_state.records.append({
            "ì§ˆë¬¸": st.session_state.current_question,
            "í•©ê³„": res.get("overall", 0),
            **crit_map,
            "ê°•ì ": res.get("strengths", []),
            "ê°ì ": res.get("risks", []),
            "ê°œì„ ": res.get("improvements", []),
        })
        st.success("ì±„ì /ì½”ì¹­ ì™„ë£Œ!")

# ----------------------------- 7) íŒ”ë¡œì—… ì§ˆë¬¸ & ë‹µë³€ & í”¼ë“œë°± (ìˆ˜ì •ë³¸ ì œê±°) -----------------------------
st.header("7) íŒ”ë¡œì—… ì§ˆë¬¸ & ë‹µë³€")
# ë©”ì¸ í”¼ë“œë°± ì¡´ì¬ ì‹œ, íŒ”ë¡œì—… ì œì•ˆ
if st.session_state.last_result and not st.session_state.followups:
    try:
        ctx = json.dumps(st.session_state.clean_struct or {}, ensure_ascii=False)
        msg = {
            "role":"user",
            "content":(
                f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´]\n{ctx}\n\n"
                f"[ê¸°ì¡´ ì§ˆë¬¸]\n{st.session_state.current_question}\n\n"
                f"[ê¸°ì¡´ ë‹µë³€]\n{st.session_state.answer_text}\n\n"
                "ë©´ì ‘ê´€ ê´€ì ì—ì„œ íŒ”ë¡œì—… ì§ˆë¬¸ 3ê°œë¥¼ í•œ ì¤„ì”© í•œêµ­ì–´ë¡œ ì œì•ˆí•´ì¤˜. "
                "ê¸°ì¡´ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê²Œ, ì§€í‘œ/ë¦¬ìŠ¤í¬/ì˜ì‚¬ê²°ì • ê·¼ê±°ë¥¼ ì„ì–´ì¤˜."
            )
        }
        r = client.chat.completions.create(model=CHAT_MODEL, temperature=0.7,
                                           messages=[{"role":"system","content":"íŒ”ë¡œì—… ì§ˆë¬¸ ìƒì„±ê¸°"}, msg])
        followups = [re.sub(r'^\s*\d+[\).\s-]*','', l).strip()
                     for l in r.choices[0].message.content.splitlines() if l.strip()]
        st.session_state.followups = followups[:3]
    except Exception:
        st.session_state.followups = []

if st.session_state.followups:
    st.markdown("**íŒ”ë¡œì—… ì§ˆë¬¸ ì œì•ˆ**")
    for i, f in enumerate(st.session_state.followups, 1):
        st.markdown(f"- ({i}) {f}")
    st.selectbox("ì±„ì  ë°›ì„ íŒ”ë¡œì—… ì§ˆë¬¸ ì„ íƒ", st.session_state.followups, index=0, key="selected_followup")
    st.text_area("íŒ”ë¡œì—… ì§ˆë¬¸ì— ëŒ€í•œ ë‚˜ì˜ ë‹µë³€", height=160, key="followup_answer")

    if st.button("íŒ”ë¡œì—… ì±„ì  & í”¼ë“œë°±", type="secondary"):
        fu_q = st.session_state.get("selected_followup", "")
        fu_ans = st.session_state.get("followup_answer", "")
        if not fu_q:
            st.warning("íŒ”ë¡œì—… ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        elif not fu_ans.strip():
            st.warning("íŒ”ë¡œì—… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.")
        else:
            with st.spinner("íŒ”ë¡œì—… ì±„ì  ì¤‘..."):
                res_fu = llm_score_and_coach_strict(
                    st.session_state.clean_struct, fu_q, fu_ans, CHAT_MODEL
                )
            st.session_state.last_followup_result = res_fu
            crit_map = {c["name"]: c["score"] for c in res_fu.get("criteria", [])}
            st.session_state.records.append({
                "ì§ˆë¬¸": f"(íŒ”ë¡œì—…) {fu_q}",
                "í•©ê³„": res_fu.get("overall", 0),
                **crit_map,
                "ê°•ì ": res_fu.get("strengths", []),
                "ê°ì ": res_fu.get("risks", []),
                "ê°œì„ ": res_fu.get("improvements", []),
            })
            st.success("íŒ”ë¡œì—… ì±„ì  ì™„ë£Œ!")

st.divider()

# ----------------------------- 8) í”¼ë“œë°± & ì‹œê°í™” -----------------------------
st.header("8) í”¼ë“œë°±")
last = st.session_state.last_result
if last:
    c1, c2 = st.columns([1,3])
    with c1:
        st.metric("í•©ê³„(/100)", last.get("overall", 0))
    with c2:
        st.markdown("**ê¸°ì¤€ë³„ ì ìˆ˜ & ì½”ë©˜íŠ¸**")
        for it in last.get("criteria", []):
            st.markdown(f"- **{it['name']}**: {it['score']}/10 â€” {it.get('comment','')}")
        if last.get("strengths"):
            st.markdown("**ê°•ì **")
            for s in last["strengths"]: st.markdown(f"- {s}")
        if last.get("risks"):
            st.markdown("**ê°ì  ìš”ì¸/ë¦¬ìŠ¤í¬**")
            for r in last["risks"]: st.markdown(f"- {r}")
        if last.get("improvements"):
            st.markdown("**ê°œì„  í¬ì¸íŠ¸**")
            for im in last["improvements"]: st.markdown(f"- {im}")
else:
    st.info("ì•„ì§ ë©”ì¸ ì±„ì  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# -------- ë ˆì´ë” ì°¨íŠ¸: ê°œë³„ ë‹µë³€ ì„ íƒ ì‹œ ê°ê° í‘œì‹œ / ë¯¸ì„ íƒ ì‹œ í‰ê· ë§Œ --------
st.subheader("ì—­ëŸ‰ ë ˆì´ë”")
def records_to_df(records: List[Dict]) -> pd.DataFrame:
    rows=[]
    for rec in records:
        row = {"ì§ˆë¬¸": rec.get("ì§ˆë¬¸",""), "í•©ê³„": rec.get("í•©ê³„",0)}
        for c in CRITERIA:
            row[c] = rec.get(c, None)
        rows.append(row)
    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=["ì§ˆë¬¸","í•©ê³„"]+CRITERIA)

df = records_to_df(st.session_state.records)
if not df.empty:
    # ë©€í‹° ì„ íƒ: ì„ íƒ ì‹œ í•´ë‹¹ ë‹µë³€ì˜ ê¶¤ì  í‘œì‹œ, ë¯¸ì„ íƒ ì‹œ í‰ê· ë§Œ í‘œì‹œ
    options = df["ì§ˆë¬¸"].tolist()
    picked = st.multiselect("ê°œë³„ ë‹µë³€ ì„ íƒ(ë¯¸ì„ íƒ ì‹œ í‰ê· ë§Œ í‘œì‹œ)", options, default=[])
    # í‰ê· 
    avg = df[CRITERIA].mean(numeric_only=True)
    traces = []
    if PLOTLY_OK:
        fig = go.Figure()
        if not picked:
            r = avg.values.tolist()
            fig.add_trace(go.Scatterpolar(
                r=r + [r[0]], theta=CRITERIA + [CRITERIA[0]],
                fill='toself', name="í‰ê· "
            ))
        else:
            # ì„ íƒëœ í•­ëª© ê°ê° ì¶”ê°€
            for label in picked:
                row = df[df["ì§ˆë¬¸"]==label].iloc[0]
                r = [row[c] if pd.notnull(row[c]) else 0 for c in CRITERIA]
                fig.add_trace(go.Scatterpolar(
                    r=r + [r[0]], theta=CRITERIA + [CRITERIA[0]],
                    fill='toself', name=label
                ))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,10])), showlegend=True, height=460)
        st.plotly_chart(fig, use_container_width=True)
    else:
        # Plotly ì„¤ì¹˜ê°€ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ ë§‰ëŒ€ í‰ê· ë§Œ
        st.bar_chart(avg)
    # ì ìˆ˜í‘œ (í•œê¸€ ì»¬ëŸ¼)
    st.dataframe(df, use_container_width=True)
else:
    st.caption("ì•„ì§ í‰ê°€ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

st.divider()

# ----------------------------- ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV) -----------------------------
st.subheader("ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report(records: List[Dict]) -> pd.DataFrame:
    # ì´ë¯¸ ë ˆì½”ë“œê°€ í•œê¸€ ì»¬ëŸ¼ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ CSV
    return pd.DataFrame(records) if records else pd.DataFrame(columns=["ì§ˆë¬¸","í•©ê³„"]+CRITERIA+["ê°•ì ","ê°ì ","ê°œì„ "])

rep = build_report(st.session_state.records)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=rep.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")
