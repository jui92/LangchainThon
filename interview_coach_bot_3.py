# -*- coding: utf-8 -*-
"""
Job Helper Bot (ë™ì  í¬ë¡¤ë§ + ìš°ëŒ€ ì‚¬í•­ê¹Œì§€ í™•ë³´)
- 1) ì±„ìš© ê³µê³  URL â†’ ì •ì œ(ì •ì  ìˆ˜ì§‘ â†’ í¬í„¸ ì •ë°€ íŒŒì„œ â†’ (ë¶€ì¡± ì‹œ) ë™ì  í¬ë¡¤ë§ìœ¼ë¡œ 'ë”ë³´ê¸°' í´ë¦­ â†’ JSON ìƒíƒœ íŒŒì‹± â†’ LLM ë³´ê°•)
- 2) íšŒì‚¬ ìš”ì•½(ì •ì œ ê²°ê³¼)
- 3) ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ (pdf/txt/md/docx) + ì¸ë±ì‹±
- 4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„± (ì„¸ì…˜ ìƒíƒœì— ì €ì¥ë˜ì–´ ì‚¬ë¼ì§€ì§€ ì•ŠìŒ)
- 5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ (ì´ë ¥ì„œ RAG ë°˜ì˜)
- 6) ì±„ì  & ì½”ì¹­ (ì´ì =ê¸°ì¤€ ë‹¤ì„¯ í•­ëª© í•©ê³„, ì¼ê´€ì„± ìœ ì§€)
- 7) íŒ”ë¡œì—… ì§ˆë¬¸ & ë‹µë³€
- 8) íŒ”ë¡œì—… í”¼ë“œë°±
"""

import os, re, io, json, time, random, urllib.parse
from typing import Optional, Dict, List, Tuple

import requests
import streamlit as st
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import html2text

# ---------------- OpenAI ----------------
try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

# ---------------- PDF/DOCX ----------------
try:
    import pypdf
except Exception:
    pypdf = None

# ---------------- í˜ì´ì§€ ì„¤ì • ----------------
st.set_page_config(page_title="Job Helper Bot", page_icon="ğŸ§­", layout="wide")
st.title("Job Helper Bot â€” íšŒì‚¬ ì •ì œ/ìì†Œì„œ/ì§ˆë¬¸/ì±„ì /íŒ”ë¡œì—… (ë™ì  í¬ë¡¤ë§ ì§€ì›)")

# ---------------- OpenAI í‚¤/ëª¨ë¸ ----------------
API_KEY = os.getenv("OPENAI_API_KEY") or (st.secrets.get("OPENAI_API_KEY", None) if hasattr(st, "secrets") else None)
if not API_KEY:
    API_KEY = st.text_input("OPENAI_API_KEY", type="password")
if not API_KEY:
    st.stop()
client = OpenAI(api_key=API_KEY)

with st.sidebar:
    st.subheader("ëª¨ë¸/ì˜µì…˜")
    CHAT_MODEL  = st.selectbox("ëŒ€í™”/ìƒì„± ëª¨ë¸", ["gpt-4o-mini", "gpt-4o"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)
    ENABLE_DYNAMIC = st.toggle("ë™ì  í¬ë¡¤ë§(Playwright) ì‚¬ìš©", value=True)
    st.caption("ì²« ì‹¤í–‰ ì „ 1íšŒ: `python -m playwright install chromium`")

# ---------------- ìœ í‹¸ ----------------
def normalize_url(u: str) -> Optional[str]:
    if not u: return None
    u = u.strip()
    if not re.match(r"^https?://", u): u = "https://" + u
    parts = urllib.parse.urlsplit(u)
    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))

def http_get(url: str, timeout: int = 12) -> Optional[requests.Response]:
    try:
        r = requests.get(
            url,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
                "Accept-Language": "ko, en;q=0.9",
            },
            timeout=timeout,
        )
        if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
            return r
    except Exception:
        pass
    return None

def html_to_text(html_str: str) -> str:
    conv = html2text.HTML2Text()
    conv.ignore_links = True
    conv.ignore_images = True
    conv.body_width = 0
    txt = conv.handle(html_str or "")
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return txt.strip()

# ---------------- ì •ì  ìˆ˜ì§‘ ----------------
def fetch_jina_text(url: str, timeout: int = 15) -> str:
    """Jina proxyë¡œ ê°„ë‹¨ í…ìŠ¤íŠ¸ ìˆ˜ì§‘(ì¼ë¶€ ì‚¬ì´íŠ¸ ì°¨ë‹¨ ê°€ëŠ¥)"""
    try:
        parts = urllib.parse.urlsplit(url)
        prox = f"https://r.jina.ai/http://{parts.netloc}{parts.path}"
        if parts.query: prox += f"?{parts.query}"
        r = http_get(prox, timeout=timeout)
        return r.text.strip() if r else ""
    except Exception:
        return ""

def fetch_web_text(url: str) -> str:
    r = http_get(url, timeout=12)
    if not r: return ""
    return html_to_text(r.text)

def fetch_bs4_text(url: str) -> Tuple[str, Optional[BeautifulSoup], Optional[str]]:
    r = http_get(url, timeout=12)
    if not r: return "", None, None
    soup = BeautifulSoup(r.text, "lxml")
    return soup.get_text(" ", strip=True)[:120000], soup, r.text

def fetch_all_text_static(url: str):
    url = normalize_url(url)
    if not url: return "", {"error":"invalid_url"}, None, None
    # 1) Jina
    jina = fetch_jina_text(url)
    if jina:
        _, soup, html = fetch_bs4_text(url)
        return jina, {"source":"jina","len":len(jina),"url":url}, soup, html
    # 2) ì§ì ‘ í…ìŠ¤íŠ¸ ë³€í™˜
    web = fetch_web_text(url)
    if web:
        _, soup, html = fetch_bs4_text(url)
        return web, {"source":"web","len":len(web),"url":url}, soup, html
    # 3) ìµœì†Œ ë³´ì¥
    bs, soup, html = fetch_bs4_text(url)
    return bs, {"source":"bs4","len":len(bs),"url":url}, soup, html

# ---------------- ë™ì  ìˆ˜ì§‘: 'ë”ë³´ê¸°' í´ë¦­ ê°•í™”íŒ ----------------
def fetch_dynamic_html(url: str, site_hint: str | None = None,
                       max_rounds: int = 5, wait_ms: int = 700) -> str:
    """Playwrightë¡œ ì ‘ì† â†’ 'ë”ë³´ê¸°/í¼ì¹˜ê¸°/ìƒì„¸' í›„ë³´ë¥¼ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ í´ë¦­ â†’ DOM ë³€í™” ì—†ìœ¼ë©´ ì¢…ë£Œ â†’ ìµœì¢… HTML ë°˜í™˜"""
    try:
        from playwright.sync_api import sync_playwright
    except Exception:
        return ""

    def _candidate_selectors(site: str | None):
        common = [
            'button:has-text("ë”ë³´ê¸°")',
            'button:has-text("ë” ë³´ê¸°")',
            'button:has-text("ì „ì²´ ë³´ê¸°")',
            'button:has-text("ìƒì„¸ ë³´ê¸°")',
            'button:has-text("í¼ì¹˜ê¸°")',
            '[role="button"]:has-text("ë”ë³´ê¸°")',
            '[data-testid*="more"]',
            '[data-cy*="more"]',
            '[aria-label*="ë”ë³´ê¸°"]',
            '.more, .btn-more, .see-more, .read-more, .expand, .toggle',
            '[class*="fold"] button, [class*="fold"] a',
            '[class*="collapsed"] button, [class*="collapsed"] a',
        ]
        site = (site or "").lower()
        if "wanted" in site:
            common += [
                'button:has-text("ìƒì„¸ìš”ê°• ë”ë³´ê¸°")',
                '[data-accordion-button]',
            ]
        if "saramin" in site:
            common += [
                '#btnReadMore, .btn_open, .btn_more, .recruit_more',
                '.wrap_jview .btn_toggle, .wrap_jview .btn_more',
            ]
        if "jobkorea" in site:
            common += [
                '.btnMore, .foldBtn, .lyBtnMore',
                '.tbDetail .btn, .tbSupport .btn',
            ]
        return list(dict.fromkeys(common))

    html = ""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
            ctx = browser.new_context(locale="ko-KR")
            page = ctx.new_page()
            page.set_default_timeout(15000)
            page.goto(url, wait_until="domcontentloaded")
            try:
                page.wait_for_load_state("networkidle", timeout=12000)
            except Exception:
                pass

            last_height = -1
            selectors = _candidate_selectors(site_hint)

            for _round in range(max_rounds):
                clicked_any = False

                # í…ìŠ¤íŠ¸ ê¸°ë°˜ í´ë¦­
                for text in ["ë”ë³´ê¸°", "ë” ë³´ê¸°", "ì „ì²´ ë³´ê¸°", "ìƒì„¸ ë³´ê¸°", "í¼ì¹˜ê¸°", "ì ‘ê¸°"]:
                    try:
                        loc = page.get_by_text(text, exact=False)
                        count = loc.count()
                        for i in range(min(count, 6)):
                            try:
                                el = loc.nth(i)
                                el.scroll_into_view_if_needed()
                                el.click()
                                page.wait_for_timeout(wait_ms)
                                clicked_any = True
                            except Exception:
                                continue
                    except Exception:
                        pass

                # ì…€ë ‰í„° ê¸°ë°˜ í´ë¦­
                for sel in selectors:
                    try:
                        loc = page.locator(sel)
                        count = loc.count()
                        if count == 0: 
                            continue
                        for i in range(min(count, 6)):
                            try:
                                el = loc.nth(i)
                                el.scroll_into_view_if_needed()
                                el.click()
                                page.wait_for_timeout(wait_ms)
                                clicked_any = True
                            except Exception:
                                continue
                    except Exception:
                        continue

                try:
                    cur_height = page.evaluate("()=>document.body.scrollHeight")
                except Exception:
                    cur_height = -1

                if not clicked_any and (cur_height == last_height):
                    break
                last_height = cur_height

            html = page.content()
            ctx.close()
            browser.close()
    except Exception:
        html = ""
    return html

# ---------------- JSON ìƒíƒœ(__NEXT_DATA__ ë“±) íŒŒì‹± ----------------
def extract_json_blobs_from_soup(soup: BeautifulSoup) -> list:
    blobs = []
    for s in soup.find_all("script"):
        text = (s.string or s.get_text() or "").strip()
        if not text:
            continue
        if text.startswith("{") or text.startswith("["):
            try:
                blobs.append(json.loads(text))
            except Exception:
                pass
    return blobs

def pick_sections_from_json_blobs(blobs: list) -> dict:
    out = {"responsibilities": [], "qualifications": [], "preferences": []}
    seen = {k:set() for k in out}

    def _push(k, v):
        t = re.sub(r"\s+"," ", str(v)).strip(" -â€¢Â·â–¶â–ªï¸").strip()
        if t and t not in seen[k]:
            seen[k].add(t); out[k].append(t[:180])

    pref_kw = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|Nice\s*to\s*have|ê°€ì‚°ì )", re.I)
    qual_kw = re.compile(r"(ìê²©|ìš”ê±´|qualification|requirement)", re.I)
    resp_kw = re.compile(r"(ì—…ë¬´|ë‹´ë‹¹|role|responsibilit)", re.I)

    def walk(o):
        if isinstance(o, dict):
            for k,v in o.items():
                kl = str(k).lower()
                if isinstance(v, list):
                    if pref_kw.search(kl):
                        for it in v: _push("preferences", it)
                    elif qual_kw.search(kl):
                        for it in v: _push("qualifications", it)
                    elif resp_kw.search(kl):
                        for it in v: _push("responsibilities", it)
                elif isinstance(v, str):
                    if pref_kw.search(kl):
                        for it in re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+", v): _push("preferences", it)
                    elif qual_kw.search(kl):
                        for it in re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+", v): _push("qualifications", it)
                    elif resp_kw.search(kl):
                        for it in re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+", v): _push("responsibilities", it)
                walk(v)
        elif isinstance(o, list):
            for it in o: walk(it)

    for b in blobs: walk(b)

    # ìê²©ìš”ê±´ì— ì„ì¸ ìš°ëŒ€ í‚¤ì›Œë“œ ì´ë™
    moved, remain = [], []
    pref_kw2 = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|Nice\s*to\s*have|ê°€ì‚°ì )", re.I)
    for q in out["qualifications"]:
        if pref_kw2.search(q): moved.append(q)
        else: remain.append(q)
    out["qualifications"] = remain
    out["preferences"] = (out["preferences"] + moved)[:12]
    for k in out: out[k] = out[k][:12]
    return out

# ---------------- í¬í„¸ ì •ë°€ íŒŒì„œ ----------------
RESP_HDR = re.compile(r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Role|Responsibilities?)", re.I)
QUAL_HDR = re.compile(r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements?|Qualifications?)", re.I)
PREF_HDR = re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|Preferred|Nice\s*to\s*have|Plus)", re.I)
PREF_KW  = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |Nice\s*to\s*have)", re.I)

def _clean_line(s: str) -> str:
    return re.sub(r"\s+"," ", s or "").strip(" -â€¢Â·â–¶â–ªï¸")[:180]

def _push_unique(bucket: List[str], text: str, seen: set):
    t = _clean_line(text)
    if t and t not in seen:
        seen.add(t); bucket.append(t)

def collect_after_heading(soup: BeautifulSoup, head_regex: re.Pattern, limit: int = 16) -> List[str]:
    out, seen = [], set()
    heads = [tag for tag in soup.find_all(re.compile("^h[1-4]$")) if head_regex.search(tag.get_text(" ", strip=True) or "")]
    for h in heads:
        sib = h.find_next_sibling()
        while sib and sib.name not in {"h1","h2","h3","h4"} and len(out) < limit:
            if sib.name in {"ul","ol"}:
                for li in sib.find_all("li", recursive=True):
                    _push_unique(out, li.get_text(" ", strip=True), seen)
                    if len(out) >= limit: break
            elif sib.name in {"p","div","section"}:
                txt = sib.get_text(" ", strip=True)
                if len(txt) > 4:
                    for l in re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+|\s{2,}", txt):
                        _push_unique(out, l, seen)
                        if len(out) >= limit: break
            sib = sib.find_next_sibling()
        if len(out) >= limit: break
    return out[:limit]

def parse_wanted(soup: BeautifulSoup) -> Dict[str, List[str]]:
    res  = collect_after_heading(soup, RESP_HDR, 16)
    qual = collect_after_heading(soup, QUAL_HDR, 16)
    pref = collect_after_heading(soup, PREF_HDR, 16)
    # ìê²©ìš”ê±´ì— ì„ì¸ ìš°ëŒ€ ì´ë™
    remain, moved = [], []
    for q in qual:
        (moved if PREF_KW.search(q) else remain).append(q)
    pref += moved
    return {"responsibilities": res[:12], "qualifications": remain[:12], "preferences": pref[:12]}

def parse_saramin(soup: BeautifulSoup) -> Dict[str, List[str]]:
    out = {"responsibilities": [], "qualifications": [], "preferences": []}
    out["responsibilities"] += collect_after_heading(soup, RESP_HDR, 16)
    out["qualifications"]   += collect_after_heading(soup, QUAL_HDR, 16)
    out["preferences"]      += collect_after_heading(soup, PREF_HDR, 16)
    # DL êµ¬ì¡° ë³´ì¡°
    for dl in soup.find_all("dl"):
        for dt in dl.find_all("dt", recursive=False):
            title = dt.get_text(" ", strip=True) or ""
            dd = dt.find_next_sibling("dd")
            if not dd: continue
            text = dd.get_text(" ", strip=True)
            if not text: continue
            lines = re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+|\s{2,}", text)
            if RESP_HDR.search(title):
                out["responsibilities"] += lines
            elif QUAL_HDR.search(title):
                out["qualifications"] += lines
            elif PREF_HDR.search(title) or PREF_KW.search(title):
                out["preferences"] += lines
    # ìê²©ìš”ê±´ì— ì„ì¸ ìš°ëŒ€ ì´ë™
    remain, moved = [], []
    for q in out["qualifications"]:
        (moved if PREF_KW.search(q) else remain).append(q)
    out["qualifications"] = remain
    out["preferences"] += moved

    # í´ë¦°
    for k in out:
        seen=set(); clean=[]
        for s in out[k]:
            s=_clean_line(s)
            if s and s not in seen:
                seen.add(s); clean.append(s)
        out[k]=clean[:12]
    return out

def parse_jobkorea(soup: BeautifulSoup) -> Dict[str, List[str]]:
    res  = collect_after_heading(soup, re.compile(r"(ìƒì„¸\s*ìš”ê°•|ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Responsibilities?)", re.I), 16)
    qual = collect_after_heading(soup, re.compile(r"(ì§€ì›\s*ìê²©|ìê²©\s*ìš”ê±´|Requirements?|Qualifications?)", re.I), 16)
    pref = collect_after_heading(soup, re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|Preferred|Plus|Nice\s*to\s*have)", re.I), 16)
    # ì´ë™
    remain, moved = [], []
    for q in qual:
        (moved if PREF_KW.search(q) else remain).append(q)
    return {"responsibilities": res[:12], "qualifications": remain[:12], "preferences": (pref+moved)[:12]}

def parse_portal_specific(url: str, soup: Optional[BeautifulSoup], raw_text: str) -> Dict[str, List[str]]:
    out = {"responsibilities":[], "qualifications":[], "preferences":[]}
    if not soup:
        # ìµœì†Œ ë¼ì¸ ìŠ¤ìº”
        lines = [ _clean_line(x) for x in (raw_text or "").split("\n") if x.strip() ]
        bucket = None
        for l in lines:
            if RESP_HDR.search(l): bucket="responsibilities"; continue
            if QUAL_HDR.search(l): bucket="qualifications"; continue
            if PREF_HDR.search(l) or PREF_KW.search(l): bucket="preferences"; continue
            if bucket: out[bucket].append(l)
        for k in out:
            seen=set(); clean=[]
            for s in out[k]:
                s=_clean_line(s)
                if s and s not in seen: seen.add(s); clean.append(s)
            out[k]=clean[:12]
        return out

    host = urllib.parse.urlsplit(normalize_url(url) or "").netloc.lower()
    if "wanted.co.kr" in host:   out = parse_wanted(soup)
    elif "saramin.co.kr" in host: out = parse_saramin(soup)
    elif "jobkorea.co.kr" in host: out = parse_jobkorea(soup)
    else:
        out["responsibilities"] = collect_after_heading(soup, RESP_HDR, 16)
        out["qualifications"]   = collect_after_heading(soup, QUAL_HDR, 16)
        out["preferences"]      = collect_after_heading(soup, PREF_HDR, 16)

    # ìê²©ìš”ê±´ ë‚´ ìš°ëŒ€ í‚¤ì›Œë“œ ë³´ì •
    remain, moved = [], []
    for q in out.get("qualifications", []):
        (moved if PREF_KW.search(q) else remain).append(q)
    out["qualifications"] = remain
    out["preferences"] = (out.get("preferences", []) + moved)[:12]

    # ì¤‘ë³µ ì •ë¦¬
    for k in out:
        seen=set(); clean=[]
        for s in out[k]:
            s=_clean_line(s)
            if s and s not in seen:
                seen.add(s); clean.append(s)
        out[k]=clean[:12]
    return out

# ---------------- ë©”íƒ€ ì¶”ì¶œ & LLM ì •ì œ ----------------
def extract_company_meta(soup: Optional[BeautifulSoup]) -> Dict[str,str]:
    meta = {"company_name":"","company_intro":"","job_title":""}
    if not soup: return meta
    cand = []
    og = soup.find("meta", {"property":"og:site_name"})
    if og and og.get("content"): cand.append(og["content"])
    app = soup.find("meta", {"name":"application-name"})
    if app and app.get("content"): cand.append(app["content"])
    if soup.title and soup.title.string: cand.append(soup.title.string)
    cand = [re.split(r"[\-\|\Â·\â€”]", c)[0].strip() for c in cand if c]
    cand = [c for c in cand if 2 <= len(c) <= 40]
    meta["company_name"] = cand[0] if cand else ""
    md = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
    if md and md.get("content"):
        meta["company_intro"] = re.sub(r"\s+"," ", md["content"]).strip()[:500]
    jt = ""
    ogt = soup.find("meta", {"property":"og:title"})
    if ogt and ogt.get("content"): jt = ogt["content"]
    if not jt:
        h1 = soup.find("h1")
        if h1 and h1.get_text(): jt = h1.get_text(strip=True)
    if not jt:
        h2 = soup.find("h2")
        if h2 and h2.get_text(): jt = h2.get_text(strip=True)
    meta["job_title"] = re.sub(r"\s+"," ", jt).strip()[:120]
    return meta

PROMPT_SYSTEM_STRUCT = ("ë„ˆëŠ” ì±„ìš© ê³µê³ ë¥¼ ê¹”ë”í•˜ê²Œ êµ¬ì¡°í™”í•˜ëŠ” ë³´ì¡°ì›ì´ë‹¤. "
                        "ì…ë ¥ í…ìŠ¤íŠ¸ëŠ” í¬í„¸ ê´‘ê³  ë¬¸êµ¬, UIì”ì¬, ë³µìˆ˜ ì§ë¬´ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆë‹¤. "
                        "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì¤‘ë³µì—†ì´ ì •ì œí•˜ë¼.")

def llm_structurize(raw_text: str, meta_hint: Dict[str,str], model: str) -> Dict:
    ctx = (raw_text or "").strip()
    if len(ctx) > 14000: ctx = ctx[:14000]
    user_msg = {"role": "user",
                "content": ("ë‹¤ìŒ ì±„ìš© ê³µê³  ì›ë¬¸ì„ êµ¬ì¡°í™”í•´ì¤˜.\n\n"
                            f"[íŒíŠ¸] íšŒì‚¬ëª… í›„ë³´: {meta_hint.get('company_name','')}\n"
                            f"[íŒíŠ¸] ì§ë¬´ëª… í›„ë³´: {meta_hint.get('job_title','')}\n"
                            "--- ì›ë¬¸ ì‹œì‘ ---\n"
                            f"{ctx}\n"
                            "--- ì›ë¬¸ ë ---\n\n"
                            "JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , í‚¤ëŠ” ë°˜ë“œì‹œ ì•„ë˜ë§Œ í¬í•¨:\n"
                            "{"
                            "\"company_name\": str, "
                            "\"company_intro\": str, "
                            "\"job_title\": str, "
                            "\"responsibilities\": [str], "
                            "\"qualifications\": [str], "
                            "\"preferences\": [str]"
                            "}\n"
                            "- 'ìš°ëŒ€ ì‚¬í•­(preferences)'ì€ ë¹„ì›Œë‘ì§€ ë§ê³ , ì›ë¬¸ì—ì„œ 'ìš°ëŒ€/ì„ í˜¸/preferred/plus/ê°€ì‚°ì ' ë“± í‘œì‹œê°€ ìˆëŠ” í•­ëª©ì„ ê·¸ëŒ€ë¡œ ë‹´ì•„ë¼.\n"
                            "- ë¶ˆë¦¿/ë§ˆì»¤/ì´ëª¨ì§€ ì œê±°, ë¬¸ì¥ ê°„ê²°í™”, ì¤‘ë³µ ì œê±°.")}

    try:
        resp = client.chat.completions.create(model=model, temperature=0.2,
                                              response_format={"type":"json_object"},
                                              messages=[{"role":"system","content":PROMPT_SYSTEM_STRUCT}, user_msg])
        data = json.loads(resp.choices[0].message.content)
    except Exception as e:
        data = {"company_name": meta_hint.get("company_name",""),
                "company_intro": meta_hint.get("company_intro","ì›ë¬¸ì´ ì •ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."),
                "job_title": meta_hint.get("job_title",""),
                "responsibilities": [],
                "qualifications": [],
                "preferences": [],
                "error": str(e)}

    # í´ë¦°
    for k in ["responsibilities","qualifications","preferences"]:
        arr = data.get(k, [])
        if not isinstance(arr, list): arr = []
        clean_list=[]; seen=set()
        for it in arr:
            t = re.sub(r"\s+"," ", str(it)).strip(" -â€¢Â·â–¶â–ªï¸").strip()
            if t and t not in seen:
                seen.add(t); clean_list.append(t[:180])
        data[k] = clean_list[:12]
    for k in ["company_name","company_intro","job_title"]:
        if k in data and isinstance(data[k], str):
            data[k] = re.sub(r"\s+"," ", data[k]).strip()

    # ìš°ëŒ€ ë³´ì •: ìê²©ìš”ê±´ì— ì„ì¸ ìš°ëŒ€ ì´ë™
    if len(data.get("preferences", [])) < 1:
        pref_kw = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|Nice\s*to\s*have|ê°€ì‚°ì )", re.I)
        remain, moved = [], []
        for q in data.get("qualifications", []):
            (moved if pref_kw.search(q) else remain).append(q)
        if moved:
            data["preferences"] = moved[:12]
            data["qualifications"] = remain[:12]
    return data

# ---------------- íŒŒì¼ ë¦¬ë” ----------------
def read_pdf(data: bytes) -> str:
    if pypdf is None: return ""
    try:
        reader = pypdf.PdfReader(io.BytesIO(data))
        return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
    except Exception:
        return ""

def read_docx(data: bytes) -> str:
    # docx2txt ìš°ì„ , ì‹¤íŒ¨ ì‹œ python-docx fallback
    try:
        import docx2txt, tempfile
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=True) as tmp:
            tmp.write(data); tmp.flush()
            return docx2txt.process(tmp.name) or ""
    except Exception:
        try:
            import docx as docxlib
            f = io.BytesIO(data)
            doc = docxlib.Document(f)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception:
            return ""

def read_file_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt",".md")):
        for enc in ("utf-8","cp949","euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        return read_pdf(data)
    elif name.endswith(".docx"):
        return read_docx(data)
    return ""

# ---------------- ì„ë² ë”©/RAG ----------------
def chunk(text: str, size: int = 600, overlap: int = 120) -> List[str]:
    t = re.sub(r"\s+"," ", text).strip()
    if not t: return []
    out, start = [], 0
    while start < len(t):
        end = min(len(t), start+size)
        out.append(t[start:end])
        if end == len(t): break
        start = max(0, end-overlap)
    return out

def embed_texts(texts: List[str], model_name: str) -> np.ndarray:
    if not texts: return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=model_name, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

def cosine_topk(matrix: np.ndarray, query_vec: np.ndarray, k: int = 4):
    if matrix.size == 0: return np.array([]), np.array([], dtype=int)
    qn = query_vec / (np.linalg.norm(query_vec, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_resume_chunks(query: str, k: int = 4):
    chs, embs = st.session_state.get("resume_chunks", []), st.session_state.get("resume_embeds", None)
    if not chs or embs is None: return []
    qv = embed_texts([query], EMBED_MODEL)
    scores, idxs = cosine_topk(embs, qv, k=k)
    return [(float(s), chs[int(i)]) for s, i in zip(scores, idxs)]

# ---------------- ë‰´ìŠ¤ (ì„ íƒ) ----------------
def _load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

def naver_search_news(company: str, display: int = 5) -> List[Dict]:
    cid, csec = _load_naver_keys()
    if not (cid and csec): return []
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    try:
        r = requests.get(url, headers=headers, params={"query": company, "display": display, "sort":"date"}, timeout=8)
        if r.status_code != 200: return []
        js=r.json()
        items=[]
        for it in js.get("items", []):
            title = re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt|&gt;", "", it.get("title","")).strip()
            items.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
        return items
    except Exception:
        return []

def google_news_rss(company: str, max_items: int = 5) -> List[Dict]:
    q = urllib.parse.quote(company)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    try:
        r = requests.get(url, timeout=8)
        if r.status_code != 200: return []
        soup = BeautifulSoup(r.text, "xml")
        out=[]
        for it in soup.find_all("item")[:max_items]:
            out.append({"title": (it.title.get_text() if it.title else "").strip(),
                        "link": (it.link.get_text() if it.link else "").strip(),
                        "pubDate": (it.pubDate.get_text() if it.pubDate else "").strip()})
        return out
    except Exception:
        return []

def fetch_latest_news(company: str, max_items: int = 5) -> List[Dict]:
    items = naver_search_news(company, display=max_items)
    if items: return items
    return google_news_rss(company, max_items=max_items)

# ---------------- ì§ˆë¬¸/ì´ˆì•ˆ/ì±„ì /íŒ”ë¡œì—… LLM ----------------
CRITERIA = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]

PROMPT_SYSTEM_Q = ("ë„ˆëŠ” ì±„ìš©ë‹´ë‹¹ìë‹¤. íšŒì‚¬/ì§ë¬´ ë§¥ë½ê³¼ ì±„ìš©ìš”ê±´, ê·¸ë¦¬ê³  ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½ì„ í•¨ê»˜ ê³ ë ¤í•´ "
                   "ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì˜ í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•œë‹¤. ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ê·œëª¨/ë¦¬ìŠ¤í¬/íŠ¸ë ˆì´ë“œì˜¤í”„ ë“±ì„ ì„ì–´ë¼.")
PROMPT_SYSTEM_DRAFT = ("ë„ˆëŠ” ë©´ì ‘ ë‹µë³€ ì½”ì¹˜ë‹¤. íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´ê³¼ ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½ì„ ê²°í•©í•´ "
                       "STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼) ê¸°ë°˜ ë‹µë³€ **ì´ˆì•ˆ**ì„ 8~12ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.")
PROMPT_SYSTEM_SCORE = ("ë„ˆëŠ” ë§¤ìš° ì—„ê²©í•œ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ë¼. "
                       "ê° ê¸°ì¤€ì€ 0~20 ì •ìˆ˜ì´ë©°, ì´ì ì€ í•©ê³„(ìµœëŒ€ 100)ì™€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•œë‹¤. "
                       "ê° ê¸°ì¤€ì— ëŒ€í•´ ì§§ê³  êµ¬ì²´ì ì¸ ì½”ë©˜íŠ¸ë¥¼ ì œê³µí•˜ë¼.")

def llm_generate_one_question_with_resume(clean: Dict, level: str, model: str) -> str:
    hits = retrieve_resume_chunks("í•µì‹¬ í”„ë¡œì íŠ¸ì™€ ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½", k=4)
    resume_snips = [t for _, t in hits]
    resume_context = "\n".join([f"- {s[:350]}" for s in resume_snips])[:1200]
    ctx = json.dumps(clean, ensure_ascii=False)
    user = {"role":"user",
            "content": (f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´]\n{ctx}\n\n"
                        f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_context}\n\n"
                        f"[ìš”ì²­]\n- ë‚œì´ë„/ì—°ì°¨: {level}\n- í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë§Œ í•œ ì¤„ë¡œ ì¶œë ¥")}
    try:
        r = client.chat.completions.create(model=model, temperature=0.8,
                                           messages=[{"role":"system","content":PROMPT_SYSTEM_Q}, user])
        q = r.choices[0].message.content.strip()
        q = re.sub(r"^\s*\d+[\).\s-]*","", q).strip()
        return q.split("\n")[0].strip()
    except Exception:
        return ""

def llm_draft_answer(clean: Dict, question: str, model: str) -> str:
    hits = retrieve_resume_chunks(question, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user = {"role":"user",
            "content": (f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
                        f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
                        f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
                        "ìœ„ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ STAR ê¸°ë°˜ í•œêµ­ì–´ ë‹µë³€ **ì´ˆì•ˆ**ì„ ì‘ì„±í•´ì¤˜.")}
    try:
        r = client.chat.completions.create(model=model, temperature=0.5,
                                           messages=[{"role":"system","content":PROMPT_SYSTEM_DRAFT}, user])
        return r.choices[0].message.content.strip()
    except Exception:
        return ""

def llm_score_and_coach_strict(clean: Dict, question: str, answer: str, model: str) -> Dict:
    hits = retrieve_resume_chunks(question + "\n" + answer[:800], k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user = {"role":"user",
            "content": (f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
                        f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
                        f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
                        f"[ì§€ì›ì ë‹µë³€]\n{answer}\n\n"
                        "ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ í•œêµ­ì–´ ì‘ë‹µ:\n"
                        "{"
                        "\"overall_score\": 0~100 ì •ìˆ˜,"
                        "\"criteria\": [{\"name\":\"ë¬¸ì œì •ì˜\",\"score\":0~20,\"comment\":\"...\"},"
                        "{\"name\":\"ë°ì´í„°/ì§€í‘œ\",\"score\":0~20,\"comment\":\"...\"},"
                        "{\"name\":\"ì‹¤í–‰ë ¥/ì£¼ë„ì„±\",\"score\":0~20,\"comment\":\"...\"},"
                        "{\"name\":\"í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜\",\"score\":0~20,\"comment\":\"...\"},"
                        "{\"name\":\"ê³ ê°ê°€ì¹˜\",\"score\":0~20,\"comment\":\"...\"}],"
                        "\"strengths\": [\"...\"],"
                        "\"risks\": [\"...\"],"
                        "\"improvements\": [\"...\"],"
                        "\"revised_answer\": \"STAR êµ¬ì¡°ë¡œ ê°„ê²°íˆ\""
                        "}")}

    try:
        r = client.chat.completions.create(model=model, temperature=0.2, response_format={"type":"json_object"},
                                           messages=[{"role":"system","content":PROMPT_SYSTEM_SCORE}, user])
        data = json.loads(r.choices[0].message.content)
    except Exception as e:
        return {"overall_score": 0,
                "criteria": [{"name": n, "score": 0, "comment": ""} for n in CRITERIA],
                "strengths": [], "risks": [], "improvements": [], "revised_answer": "",
                "error": str(e)}

    # ì •í•©ì„± ë³´ì •
    fixed=[]
    for name in CRITERIA:
        found=None
        for it in data.get("criteria", []):
            if str(it.get("name","")).strip()==name:
                found=it; break
        if not found: found={"name":name,"score":0,"comment":""}
        sc = int(found.get("score",0)); sc=max(0,min(20,sc))
        found["score"]=sc; found["comment"]=str(found.get("comment","")).strip()
        fixed.append(found)
    total=sum(x["score"] for x in fixed)
    data["criteria"]=fixed
    data["overall_score"]=total
    for k in ["strengths","risks","improvements"]:
        arr=data.get(k,[]); 
        if not isinstance(arr,list): arr=[]
        data[k]=[str(x).strip() for x in arr if str(x).strip()][:5]
    data["revised_answer"]=str(data.get("revised_answer","")).strip()
    return data

# ---------------- ì„¸ì…˜ ìƒíƒœ ----------------
def _init_state():
    defaults = {
        "clean_struct": None,
        "company_news": [],
        "resume_raw": "",
        "resume_chunks": [],
        "resume_embeds": None,
        "cover_letter": "",
        "current_question": "",
        "answer_text": "",
        "records": [],
        "last_result": None,
        "followups": [],
        "selected_followup": "",
        "followup_answer": "",
        "last_followup_result": None,
    }
    for k,v in defaults.items():
        if k not in st.session_state: st.session_state[k]=v
_init_state()

# =============================================================================
# 1) ì±„ìš© ê³µê³  URL â†’ ì •ì œ
# =============================================================================
st.header("1) ì±„ìš© ê³µê³  URL â†’ ì •ì œ")
url_input = st.text_input("ì±„ìš© ê³µê³  ìƒì„¸ URL", placeholder="ì›í‹°ë“œ/ì‚¬ëŒì¸/ì¡ì½”ë¦¬ì•„ ë“± ìƒì„¸ URLì„ ì…ë ¥")
if st.button("ì›ë¬¸ ìˆ˜ì§‘ â†’ ì •ì œ", type="primary"):
    if not url_input.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        url = url_input.strip()
        # 1) ì •ì  ìˆ˜ì§‘
        with st.spinner("ì •ì  ìˆ˜ì§‘ ì¤‘..."):
            raw, meta, soup, html_raw = fetch_all_text_static(url)
            hint = extract_company_meta(soup) if soup else {"company_name":"","company_intro":"","job_title":""}

        # 2) 1ì°¨ ì •ë°€ íŒŒì‹±
        site_struct = parse_portal_specific(url, soup, raw)
        ok_cnt = sum(len(site_struct.get(k, [])) for k in ["responsibilities","qualifications","preferences"])

        # 3) ë¶€ì¡±í•˜ë©´ ë™ì  í¬ë¡¤ë§ + JSON ìƒíƒœ íŒŒì‹± â†’ ì¬íŒŒì‹±
        if ENABLE_DYNAMIC and ok_cnt < 3:
            host = urllib.parse.urlsplit(url).netloc.lower()
            with st.spinner("ë™ì  ìˆ˜ì§‘(ìƒì„¸ ì •ë³´ ë”ë³´ê¸° í´ë¦­) ì¤‘..."):
                dyn_html = fetch_dynamic_html(url, site_hint=host, max_rounds=6, wait_ms=700)
            if dyn_html:
                soup_dyn = BeautifulSoup(dyn_html, "lxml")
                raw_dyn = html_to_text(dyn_html)
                # JSON ë¸”ë¡­ì—ì„œ ì„¹ì…˜ ë¨¼ì € ì‹œë„
                blobs = extract_json_blobs_from_soup(soup_dyn)
                from_json = pick_sections_from_json_blobs(blobs)
                # DOM ì •ë°€ íŒŒì„œë„ ë³‘í–‰
                site_struct_dyn = parse_portal_specific(url, soup_dyn, raw_dyn)
                # ë³‘í•©(í’ë¶€í•œ í•­ëª© ìš°ì„ )
                merged = {"responsibilities": [], "qualifications": [], "preferences": []}
                for key in merged:
                    cand = (from_json.get(key, []) or []) + (site_struct_dyn.get(key, []) or []) + (site_struct.get(key, []) or [])
                    seen=set(); tmp=[]
                    for s in cand:
                        s = re.sub(r"\s+"," ", s).strip(" -â€¢Â·â–¶â–ªï¸").strip()
                        if s and s not in seen:
                            seen.add(s); tmp.append(s[:180])
                    merged[key] = tmp[:12]
                site_struct = merged
                ok_cnt = sum(len(site_struct.get(k, [])) for k in ["responsibilities","qualifications","preferences"])

        # 4) ê·¸ë˜ë„ ë¹„ë©´ LLM ì •ì œ ë³´ì™„
        if ok_cnt < 3:
            with st.spinner("LLM ì •ì œ ë³´ì™„ ì¤‘..."):
                clean = llm_structurize(raw, hint, CHAT_MODEL)
            for k in ["responsibilities","qualifications","preferences"]:
                if site_struct.get(k):
                    clean[k] = site_struct[k]
        else:
            clean = {
                "company_name": hint.get("company_name",""),
                "company_intro": hint.get("company_intro",""),
                "job_title": hint.get("job_title",""),
                "responsibilities": site_struct.get("responsibilities",[]),
                "qualifications":   site_struct.get("qualifications",[]),
                "preferences":      site_struct.get("preferences",[]),
            }

        st.session_state.clean_struct = clean
        with st.spinner("ìµœì‹  ë‰´ìŠ¤ í™•ì¸ ì¤‘..."):
            cname = clean.get("company_name") or hint.get("company_name") or ""
            st.session_state.company_news = fetch_latest_news(cname, max_items=5) if cname else []
        st.success("ì •ì œ ì™„ë£Œ!")

# =============================================================================
# 2) íšŒì‚¬ ìš”ì•½ (ì •ì œ ê²°ê³¼)
# =============================================================================
st.header("2) íšŒì‚¬ ìš”ì•½ (ì •ì œ ê²°ê³¼)")
clean = st.session_state.clean_struct
if clean:
    st.markdown(f"**íšŒì‚¬ëª…:** {clean.get('company_name','-')}")
    st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ:** {clean.get('company_intro','-')}")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼(ì§ë¬´ëª…):** {clean.get('job_title','-')}")
    c1,c2,c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´**")
        for b in clean.get("responsibilities", []): st.markdown(f"- {b}")
    with c2:
        st.markdown("**ìê²© ìš”ê±´**")
        for b in clean.get("qualifications", []): st.markdown(f"- {b}")
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­**")
        prefs = clean.get("preferences", [])
        if prefs:
            for b in prefs: st.markdown(f"- {b}")
        else:
            st.caption("ìš°ëŒ€ ì‚¬í•­ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
else:
    st.info("ë¨¼ì € 'ì›ë¬¸ ìˆ˜ì§‘ â†’ ì •ì œ'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# =============================================================================
# 3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ
# =============================================================================
st.divider()
st.header("3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ")
uploads = st.file_uploader("ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ ê°€ëŠ¥", type=["pdf","txt","md","docx"], accept_multiple_files=True)

if st.button("ì´ë ¥ì„œ ì¸ë±ì‹±", type="secondary"):
    if not uploads:
        st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        all_text=[]
        for up in uploads:
            t = read_file_text(up)
            if t: all_text.append(t)
        resume_text = "\n\n".join(all_text)
        if not resume_text.strip():
            st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            chunks = chunk(resume_text, size=600, overlap=120)
            with st.spinner("ì´ë ¥ì„œ ë²¡í„°í™” ì¤‘..."):
                embeds = embed_texts(chunks, EMBED_MODEL)
            st.session_state.resume_raw = resume_text
            st.session_state.resume_chunks = chunks
            st.session_state.resume_embeds = embeds
            st.success(f"ì¸ë±ì‹± ì™„ë£Œ (ì²­í¬ {len(chunks)}ê°œ)")

# =============================================================================
# 4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„± (ì„¸ì…˜ì— ì €ì¥)
# =============================================================================
st.divider()
st.header("4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„±")
topic = st.text_input("íšŒì‚¬ ìš”ì²­ ì£¼ì œ(ì„ íƒ)", placeholder="ì˜ˆ: ì§ë¬´ ì§€ì›ë™ê¸° / í˜‘ì—… ê²½í—˜ / ë¬¸ì œí•´ê²° ì‚¬ë¡€ ë“±")

def build_cover_letter(clean_struct: Dict, resume_text: str, topic_hint: str, model: str) -> str:
    enrich = {"news": [n.get("title","") for n in st.session_state.company_news[:3]]}
    company = json.dumps({"clean":clean_struct, "extra":enrich}, ensure_ascii=False)
    resume_snippet = resume_text.strip()
    if len(resume_snippet) > 9000: resume_snippet = resume_snippet[:9000]
    system = ("ë„ˆëŠ” í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì „ë¬¸ê°€ë‹¤. ì±„ìš© ê³µê³ ì˜ íšŒì‚¬/ì§ë¬´ ìš”ê±´ê³¼ í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ì°¸ê³ í•´ "
              "íšŒì‚¬ íŠ¹í™” ìì†Œì„œë¥¼ ì‘ì„±í•œë‹¤. ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ì„íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´í™”í•œë‹¤.")
    req = f"íšŒì‚¬ ì¸¡ ìš”ì²­ ì£¼ì œëŠ” '{topic_hint.strip()}' ì´ë‹¤." if topic_hint.strip() else \
          "íŠ¹ì • ì£¼ì œ ìš”ì²­ì´ ì—†ìœ¼ë¯€ë¡œ, ì±„ìš© ê³µê³ ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì§€ì›ë™ê¸°ì™€ ì§ë¬´ì í•©ì„±ì„ ê°•ì¡°í•˜ë¼."
    user = (f"[íšŒì‚¬/ì§ë¬´ ìš”ì•½(JSON)]\n{company}\n\n"
            f"[í›„ë³´ì ì´ë ¥ì„œ(ìš”ì•½ ê°€ëŠ¥)]\n{resume_snippet}\n\n"
            f"[ì‘ì„± ì§€ì‹œ]\n- {req}\n"
            "- ë¶„ëŸ‰: 600~900ì\n"
            "- êµ¬ì„±: 1) ì§€ì› ë™ê¸° 2) ì§ë¬´ ê´€ë ¨ í•µì‹¬ ì—­ëŸ‰Â·ê²½í—˜ 3) ì„±ê³¼/ì§€í‘œ 4) ì…ì‚¬ í›„ ê¸°ì—¬ ë°©ì•ˆ 5) ë§ˆë¬´ë¦¬\n"
            "- ìì—°ìŠ¤ëŸ½ê³  ì§„ì •ì„± ìˆëŠ” 1ì¸ì¹­ ì„œìˆ . ë¶ˆí•„ìš”í•œ ë¯¸ì‚¬ì—¬êµ¬/ì¤‘ë³µ ë¬¸êµ¬ ì‚­ì œ.")
    try:
        r = client.chat.completions.create(model=model, temperature=0.4,
                                           messages=[{"role":"system","content":system},{"role":"user","content":user}])
        return r.choices[0].message.content.strip()
    except Exception as e:
        return f"(ìì†Œì„œ ìƒì„± ì‹¤íŒ¨: {e})"

if st.button("ìì†Œì„œ ìƒì„±", type="primary"):
    if not st.session_state.clean_struct:
        st.warning("ë¨¼ì € íšŒì‚¬ ì •ì œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    elif not st.session_state.resume_raw.strip():
        st.warning("ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ì‹±í•˜ì„¸ìš”.")
    else:
        with st.spinner("ìì†Œì„œ ìƒì„± ì¤‘..."):
            cover = build_cover_letter(st.session_state.clean_struct, st.session_state.resume_raw, topic, CHAT_MODEL)
        st.session_state.cover_letter = cover
        st.success("ìì†Œì„œ ìƒì„± ì™„ë£Œ!")

if st.session_state.cover_letter:
    st.subheader("ìì†Œì„œ (ìƒì„± ê²°ê³¼)")
    st.write(st.session_state.cover_letter)
    st.download_button("ìì†Œì„œ TXT ë‹¤ìš´ë¡œë“œ", data=st.session_state.cover_letter.encode("utf-8"),
                       file_name="cover_letter.txt", mime="text/plain")

# =============================================================================
# 5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ
# =============================================================================
st.divider()
st.header("5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ")
level = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"], index=0)

c_q1, c_q2 = st.columns(2)
with c_q1:
    if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", type="primary"):
        if not st.session_state.clean_struct:
            st.warning("ë¨¼ì € íšŒì‚¬ ì •ì œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        else:
            q = llm_generate_one_question_with_resume(st.session_state.clean_struct, level, CHAT_MODEL)
            if q:
                st.session_state.current_question = q
                st.session_state.answer_text = ""
                st.session_state.last_result = None
                st.session_state.followups = []
                st.session_state.selected_followup = ""
                st.session_state.followup_answer = ""
                st.success("ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
with c_q2:
    if st.button("RAGë¡œ ë‹µë³€ ì´ˆì•ˆ ìƒì„±", type="secondary"):
        if not st.session_state.current_question:
            st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        else:
            draft = llm_draft_answer(st.session_state.clean_struct, st.session_state.current_question, CHAT_MODEL)
            if draft:
                st.session_state.answer_text = draft
                st.success("ì´ˆì•ˆ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨")

st.text_area("ì§ˆë¬¸", value=st.session_state.current_question, height=100)
st.text_area("ë‚˜ì˜ ë‹µë³€(ì´ˆì•ˆì„ í¸ì§‘í•´ ì™„ì„±í•˜ì„¸ìš”)", height=200, key="answer_text")

# =============================================================================
# 6) ì±„ì  & ì½”ì¹­
# =============================================================================
st.divider()
st.header("6) ì±„ì  & ì½”ì¹­")
if st.button("ì±„ì  & ì½”ì¹­ ì‹¤í–‰", type="primary"):
    if not st.session_state.current_question:
        st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì±„ì /ì½”ì¹­ ì¤‘..."):
            res = llm_score_and_coach_strict(st.session_state.clean_struct,
                                             st.session_state.current_question,
                                             st.session_state.answer_text,
                                             CHAT_MODEL)
        st.session_state.last_result = res
        st.session_state.records.append({
            "question": st.session_state.current_question,
            "answer": st.session_state.answer_text,
            "overall": res.get("overall_score", 0),
            "criteria": res.get("criteria", []),
            "strengths": res.get("strengths", []),
            "risks": res.get("risks", []),
            "improvements": res.get("improvements", []),
            "revised_answer": res.get("revised_answer","")
        })
        st.success("ì±„ì /ì½”ì¹­ ì™„ë£Œ!")

# =============================================================================
# 7) íŒ”ë¡œì—… ì§ˆë¬¸ & ë‹µë³€
# =============================================================================
st.divider()
st.header("7) íŒ”ë¡œì—… ì§ˆë¬¸ & ë‹µë³€")
last = st.session_state.last_result
if last and not st.session_state.followups:
    try:
        ctx = json.dumps({"company": st.session_state.clean_struct,
                          "news": [n.get("title","") for n in st.session_state.company_news[:3]]}, ensure_ascii=False)
        msg = {"role":"user",
               "content":(f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´/ì´ìŠˆ]\n{ctx}\n\n"
                          f"[ì§€ì›ì ë‹µë³€]\n{st.session_state.answer_text}\n\n"
                          "ë©´ì ‘ê´€ ê´€ì ì—ì„œ íŒ”ë¡œì—… ì§ˆë¬¸ 3ê°œë¥¼ í•œ ì¤„ì”© í•œêµ­ì–´ë¡œ ì œì•ˆí•´ì¤˜. "
                          "ê¸°ì¡´ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê²Œ, ì§€í‘œ/ë¦¬ìŠ¤í¬/íŠ¸ë ˆì´ë“œì˜¤í”„/ì˜ì‚¬ê²°ì • ê·¼ê±°ë¥¼ ì„ì–´ì¤˜.")}
        r = client.chat.completions.create(model=CHAT_MODEL, temperature=0.7,
                                           messages=[{"role":"system","content":"ë©´ì ‘ íŒ”ë¡œì—… ìƒì„±ê¸°"}, msg])
        followups = [re.sub(r'^\s*\d+[\).\s-]*','', l).strip()
                     for l in r.choices[0].message.content.splitlines() if l.strip()]
        st.session_state.followups = followups[:3]
    except Exception:
        st.session_state.followups = []

if last:
    if st.session_state.followups:
        st.markdown("**íŒ”ë¡œì—… ì§ˆë¬¸ ì œì•ˆ**")
        for i, f in enumerate(st.session_state.followups, 1):
            st.markdown(f"- ({i}) {f}")
        st.selectbox("ì±„ì  ë°›ì„ íŒ”ë¡œì—… ì§ˆë¬¸ ì„ íƒ", st.session_state.followups, index=0, key="selected_followup")
        st.text_area("íŒ”ë¡œì—… ì§ˆë¬¸ì— ëŒ€í•œ ë‚˜ì˜ ë‹µë³€", height=160, key="followup_answer")
    else:
        st.caption("íŒ”ë¡œì—… ì§ˆë¬¸ì€ ë©”ì¸ ì§ˆë¬¸ ì±„ì  ì§í›„ ìë™ ì œì•ˆë©ë‹ˆë‹¤.")

# =============================================================================
# 8) íŒ”ë¡œì—… í”¼ë“œë°±
# =============================================================================
st.divider()
st.header("8) íŒ”ë¡œì—… í”¼ë“œë°±")
if st.button("íŒ”ë¡œì—… ì±„ì  & í”¼ë“œë°±", type="secondary"):
    fu_q   = st.session_state.get("selected_followup", "")
    fu_ans = st.session_state.get("followup_answer", "")
    if not fu_q:
        st.warning("íŒ”ë¡œì—… ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.")
    elif not fu_ans.strip():
        st.warning("íŒ”ë¡œì—… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.")
    else:
        with st.spinner("íŒ”ë¡œì—… ì±„ì  ì¤‘..."):
            res_fu = llm_score_and_coach_strict(st.session_state.clean_struct, fu_q, fu_ans, CHAT_MODEL)
        st.session_state.last_followup_result = res_fu
        st.success("íŒ”ë¡œì—… ì±„ì  ì™„ë£Œ!")

res_fu = st.session_state.last_followup_result
if res_fu:
    st.markdown("**íŒ”ë¡œì—… ê²°ê³¼**")
    st.metric("ì´ì (/100)", res_fu.get("overall_score", 0))
    for it in res_fu.get("criteria", []):
        st.markdown(f"- **{it['name']}**: {it['score']}/20 â€” {it.get('comment','')}")
    if res_fu.get("revised_answer",""):
        st.markdown("**íŒ”ë¡œì—… ìˆ˜ì •ë³¸ (STAR)**")
        st.write(res_fu["revised_answer"])
