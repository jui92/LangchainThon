# -*- coding: utf-8 -*-
################################################################################
# Job Helper Bot (Selenium-enabled, domain-tuned, NameError-safe)
# - ìì†Œì„œ ìƒì„± / ëª¨ì˜ ë©´ì ‘
# - Seleniumë¡œ 'ë”ë³´ê¸°/ìƒì„¸/ìš°ëŒ€'ë¥¼ í¼ì³ ìš°ëŒ€ì‚¬í•­ê¹Œì§€ ìˆ˜ì§‘ (ê°•ì œ ì‚¬ìš© ì˜µì…˜ í¬í•¨)
# - ì •ì  ìˆ˜ì§‘(requests/bs4, Jina proxy) í´ë°±
################################################################################

import os, re, json, urllib.parse, time, io, tempfile
from typing import Optional, Tuple, Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from bs4 import BeautifulSoup
import html2text
import streamlit as st
import numpy as np
import pandas as pd

# ==== OpenAI ====
try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

# ==== (ì„ íƒ) Selenium ====
SELENIUM_AVAILABLE = True
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException
    from webdriver_manager.chrome import ChromeDriverManager
except Exception:
    SELENIUM_AVAILABLE = False

# -----------------------------------------------------------------------------
# Streamlit ê¸°ë³¸ ì„¤ì •
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Job Helper Bot (Selenium)", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” Job Helper Bot (Selenium) : ìì†Œì„œ ìƒì„± / ëª¨ì˜ ë©´ì ‘")

# -----------------------------------------------------------------------------
# OpenAI í‚¤ ì…ë ¥/í™•ë³´
# -----------------------------------------------------------------------------
API_KEY = os.getenv("OPENAI_API_KEY") or (st.secrets.get("OPENAI_API_KEY", None) if hasattr(st, "secrets") else None)
if not API_KEY:
    API_KEY = st.text_input("OPENAI_API_KEY ì…ë ¥", type="password")
if not API_KEY:
    st.stop()
client = OpenAI(api_key=API_KEY)

# -----------------------------------------------------------------------------
# Sidebar ì˜µì…˜
# -----------------------------------------------------------------------------
with st.sidebar:
    st.subheader("ëª¨ë¸ & ì˜µì…˜")
    CHAT_MODEL = st.selectbox("ëŒ€í™”/ìƒì„± ëª¨ë¸", ["gpt-4o-mini","gpt-4o"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)
    ENABLE_COMPANY_ENRICH = st.checkbox("íšŒì‚¬ ë¹„ì „/ì¸ì¬ìƒ/ë‰´ìŠ¤ ìˆ˜ì§‘", value=True)
    ENABLE_SELENIUM = st.checkbox(
        "Selenium(ë™ì  ìˆ˜ì§‘) ì‚¬ìš©",
        value=True if SELENIUM_AVAILABLE else False,
        help="â€˜ë”ë³´ê¸°/ìƒì„¸/ìš°ëŒ€â€™ ë²„íŠ¼ì„ ìë™ í´ë¦­í•´ ì „ì²´ ë³¸ë¬¸ ìˆ˜ì§‘"
    )
    FORCE_SELENIUM = st.checkbox("Seleniumë§Œ ì‚¬ìš©(í´ë°± ê¸ˆì§€)", value=False)  # â˜… ì¶”ê°€
    SELENIUM_TIMEOUT = st.slider("Selenium ëŒ€ê¸°(ì´ˆ)", 4, 20, 8)
    MAX_FETCH_PARALLEL = st.slider("ë³‘ë ¬ ìˆ˜ì§‘ ì“°ë ˆë“œ", 2, 8, 4)

# -----------------------------------------------------------------------------
# HTTP ì„¸ì…˜ & html2text
# -----------------------------------------------------------------------------
def _http_session():
    sess = requests.Session()
    retry = Retry(total=2, backoff_factor=0.3,
                  status_forcelist=[429,500,502,503,504],
                  allowed_methods=["GET","HEAD"])
    adapter = HTTPAdapter(max_retries=retry, pool_connections=16, pool_maxsize=16)
    sess.mount("http://", adapter); sess.mount("https://", adapter)
    sess.headers.update({
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
        "Accept-Language":"ko, en;q=0.9"
    })
    return sess
HTTP = _http_session()

def _get_html2text():
    conv = html2text.HTML2Text()
    conv.ignore_links = True
    conv.ignore_images = True
    conv.body_width = 0
    return conv
HTML2TEXT = _get_html2text()

# -----------------------------------------------------------------------------
# ê³µí†µ ìœ í‹¸
# -----------------------------------------------------------------------------
def normalize_url(u: str) -> Optional[str]:
    if not u: return None
    u = u.strip()
    if not re.match(r"^https?://", u): u = "https://" + u
    parts = urllib.parse.urlsplit(u)
    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))

def clean_text(s: str, max_len: int = 16000) -> str:
    if not s: return ""
    s = re.sub(r"\r","", s)
    s = re.sub(r"\s+"," ", s).strip()
    return s[:max_len] if len(s) > max_len else s

def http_get(url: str, timeout: int = 8) -> Optional[requests.Response]:
    try:
        r = HTTP.get(url, timeout=timeout)
        # ì¼ë°˜ì ìœ¼ë¡œ HTMLë§Œ í—ˆìš©
        if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
            return r
    except Exception:
        pass
    return None

# -----------------------------------------------------------------------------
# (1) íšŒì‚¬ ë¹„ì „/ì¸ì¬ìƒ/ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ë“¤  â€» UIì—ì„œ í˜¸ì¶œë˜ë¯€ë¡œ ì—¬ê¸° â€˜ìœ„ìª½â€™ì— ì„ ì–¸
# -----------------------------------------------------------------------------
def fetch_company_pages(home_url: str) -> Dict[str, List[str]]:
    out = {"vision": [], "talent": []}
    base = normalize_url(home_url)
    if not base: return out
    paths = ["","/","/about","/company","/about-us","/mission","/vision","/values","/culture","/careers","/talent","/people"]
    urls, seen = [], set()
    for p in paths:
        u = (base.rstrip("/") + p) if p else base
        if u not in seen:
            seen.add(u); urls.append(u)

    texts_all=[]
    with ThreadPoolExecutor(max_workers=MAX_FETCH_PARALLEL) as ex:
        futs = {ex.submit(HTTP.get, u, 6): u for u in urls}
        for fu in as_completed(futs):
            r=None
            try: r = fu.result()
            except Exception: pass
            if not (r and r.status_code==200): continue
            soup = BeautifulSoup(r.text, "lxml")
            for tag in soup.find_all(["h1","h2","h3","h4","p","li"]):
                t = tag.get_text(" ", strip=True)
                if t and 6 <= len(t) <= 260:
                    texts_all.append(re.sub(r"\s+"," ", t))

    for t in texts_all:
        low = t.lower()
        if any(k in low for k in ["talent","ì¸ì¬ìƒ","ì¸ì¬","people we","who we hire"]):
            out["talent"].append(t)
        if any(k in low for k in ["ë¹„ì „","ë¯¸ì…˜","í•µì‹¬ê°€ì¹˜","ê°€ì¹˜","ì›ì¹™","mission","vision","values","principle"]):
            out["vision"].append(t)

    for k in out:
        out[k] = list(dict.fromkeys(x.strip() for x in out[k]))[:12]
    return out

def _load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

def naver_search_news(company: str, display: int = 5) -> List[Dict]:
    cid, csec = _load_naver_keys()
    if not (cid and csec): return []
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    try:
        r = HTTP.get(url, headers=headers, params={"query": company, "display": display, "sort":"date"}, timeout=6)
        if r.status_code != 200: return []
        js=r.json()
        items=[]
        for it in js.get("items", []):
            title = re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt;|&gt;", "", it.get("title","")).strip()
            items.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
        return items
    except Exception:
        return []

def google_news_rss(company: str, max_items: int = 5) -> List[Dict]:
    q = urllib.parse.quote(company)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    try:
        r = HTTP.get(url, timeout=6)
        if r.status_code != 200: return []
        soup = BeautifulSoup(r.text, "xml")
        out=[]
        for it in soup.find_all("item")[:max_items]:
            out.append({"title": (it.title.get_text() if it.title else "").strip(),
                        "link": (it.link.get_text() if it.link else "").strip(),
                        "pubDate": (it.pubDate.get_text() if it.pubDate else "").strip()})
        return out
    except Exception:
        return []

def fetch_latest_news(company: str, max_items: int = 5) -> List[Dict]:
    items = naver_search_news(company, display=max_items)
    return items if items else google_news_rss(company, max_items=max_items)

# -----------------------------------------------------------------------------
# (2) Selenium ë™ì  ìˆ˜ì§‘ (ì„ íƒ)
# -----------------------------------------------------------------------------
def _build_chrome(headless: bool = True):
    if not SELENIUM_AVAILABLE:
        raise RuntimeError("Selenium ë¯¸ì„¤ì¹˜")
    opts = ChromeOptions()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1440,2400")
    opts.add_argument("--lang=ko-KR")
    # í—¤ë“œë¦¬ìŠ¤ íƒì§€ íšŒí”¼ (best-effort)
    opts.add_argument("--disable-blink-features=AutomationControlled")
    try:
        driver = webdriver.Chrome(ChromeDriverManager().install(), options=opts)
    except WebDriverException:
        driver = webdriver.Chrome(options=opts)
    # webdriver íŠ¹ì„± ìˆ¨ê¹€ (ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œë§Œ)
    try:
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        })
    except Exception:
        pass
    return driver

def _click_by_text_candidates(driver, texts: List[str], timeout=4):
    for t in texts:
        try:
            xpath_exact = f"//*[normalize-space(text())='{t}']"
            xpath_contains = f"//*[contains(normalize-space(text()), '{t}')]"
            for xp in (xpath_exact, xpath_contains):
                els = driver.find_elements(By.XPATH, xp)
                for el in els[:8]:
                    try:
                        driver.execute_script("arguments[0].click();", el)
                        time.sleep(0.25)
                        return True
                    except Exception:
                        continue
        except Exception:
            continue
    return False

def _click_many(driver, css_list, limit_per_selector=8):
    for css in css_list:
        try:
            els = driver.find_elements(By.CSS_SELECTOR, css)
            for el in els[:limit_per_selector]:
                try:
                    driver.execute_script("arguments[0].click();", el)
                    time.sleep(0.2)
                except Exception:
                    continue
        except Exception:
            continue

def _expand_wanted(driver):
    # ì›í‹°ë“œ: ìƒì„¸ ë³¸ë¬¸/ì„¹ì…˜ í† ê¸€
    selectors = [
        "[data-qa='btn-read-more']",
        "[aria-expanded='false']",
        "[role='button']",
        "button[class*='Read'], a[class*='Read']",
        "button[class*='More'], a[class*='More']",
    ]
    _click_many(driver, selectors)
    _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","ê¸°ì—…ì •ë³´"], timeout=4)

def _expand_saramin(driver):
    # ì‚¬ëŒì¸: ì ‘í˜ í† ê¸€ê³¼ ë”ë³´ê¸°
    selectors = [
        ".btn_more", ".btnMore", ".btn-detail", ".btn_toggle",
        "[aria-expanded='false']", "[role='button']",
        "button[class*='more'], a[class*='more']"
    ]
    _click_many(driver, selectors)
    _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","ê¸°ì—…ì •ë³´","ìƒì„¸ì •ë³´"], timeout=4)

def _expand_jobkorea(driver):
    # ì¡ì½”ë¦¬ì•„: ë³¸ë¬¸ ì ‘í˜ í† ê¸€ / ë”ë³´ê¸°
    selectors = [
        ".btnFold", ".btnToggleRead", ".btn_more",
        "[aria-expanded='false']", "[role='button']",
        "button[class*='More'], a[class*='More']"
    ]
    _click_many(driver, selectors)
    _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","ê¸°ì—…ì •ë³´","ìƒì„¸ë³´ê¸°"], timeout=4)

def selenium_expand_then_get_html(url: str, timeout: int = 8) -> str:
    driver = _build_chrome(headless=True)
    try:
        driver.set_page_load_timeout(timeout)
        driver.get(url)
        try:
            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, "//*")))
        except TimeoutException:
            pass

        host = urllib.parse.urlsplit(url).netloc.lower()

        # ê³µí†µ í´ë¦­
        _click_by_text_candidates(driver, ["ë”ë³´ê¸°","ìƒì„¸ë³´ê¸°","ìì„¸íˆ ë³´ê¸°","ìì„¸íˆ","ì „ì²´ë³´ê¸°","í¼ì¹˜ê¸°","ëª¨ë‘ ë³´ê¸°","Read more","More"], timeout=timeout)
        _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","Requirements","Responsibilities","Preferred"], timeout=timeout)

        # ë„ë©”ì¸ ì „ìš© í™•ì¥
        if "wanted.co.kr" in host:
            _expand_wanted(driver)
        if "saramin.co.kr" in host or "saramin" in host:
            _expand_saramin(driver)
        if "jobkorea.co.kr" in host:
            _expand_jobkorea(driver)

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´(ì§€ì—° ë¡œë”© ë°©ì§€)
        for _ in range(6):
            try:
                driver.execute_script("window.scrollBy(0, 1000);"); time.sleep(0.25)
            except Exception:
                break

        return driver.page_source or ""
    except Exception:
        return ""
    finally:
        try: driver.quit()
        except Exception: pass

# -----------------------------------------------------------------------------
# (3) í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Selenium â†’ Jina â†’ requests â†’ BS4)
# -----------------------------------------------------------------------------
def html_to_text(html_str: str) -> str:
    txt = HTML2TEXT.handle(html_str or "")
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return clean_text(txt)

def fetch_all_text(url: str, use_selenium: bool, timeout: int = 8, force_selenium: bool = False) -> Tuple[str, Dict, Optional[str]]:
    """
    force_selenium=True ì´ë©´, Selenium ì‹¤íŒ¨ ì‹œ í´ë°±í•˜ì§€ ì•Šê³  ì¦‰ì‹œ ì‹¤íŒ¨ ë©”íƒ€ë¥¼ ë°˜í™˜.
    """
    url = normalize_url(url)
    if not url:
        return "", {"error":"invalid_url"}, None

    # a) Selenium ë¨¼ì €
    if use_selenium and SELENIUM_AVAILABLE:
        try:
            html_dyn = selenium_expand_then_get_html(url, timeout=timeout)
            if html_dyn and len(html_dyn) > 300:
                txt = html_to_text(html_dyn)
                return txt, {"source":"selenium","len":len(txt),"url_final":url}, html_dyn
            elif force_selenium:
                return "", {"source":"selenium_failed","len":0,"url_final":url}, None
        except Exception:
            if force_selenium:
                return "", {"source":"selenium_error","len":0,"url_final":url}, None
            # í´ë°± í—ˆìš©ì´ë©´ ê³„ì† ì§„í–‰

    # b) Jina proxy (content-typeì´ text/plainì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ í˜¸ì¶œ)
    try:
        parts = urllib.parse.urlsplit(url)
        prox = f"https://r.jina.ai/http://{parts.netloc}{parts.path}"
        if parts.query: prox += f"?{parts.query}"
        rj = HTTP.get(prox, timeout=timeout)  # content-type ì œí•œ ì—†ì´
        if rj and rj.status_code == 200 and rj.text:
            base_r = http_get(url, timeout=timeout)  # ì›ë¬¸ HTML (ê°€ëŠ¥í•˜ë©´)
            soup_html = base_r.text if base_r else None
            return clean_text(rj.text), {"source":"jina","len":len(rj.text),"url_final":url}, soup_html
    except Exception:
        pass

    # c) ì¼ë°˜ GET â†’ html2text
    r = http_get(url, timeout=timeout)
    if r:
        txt = html_to_text(r.text)
        return txt, {"source":"webbase","len":len(txt),"url_final":url}, r.text

    # d) BS4 fallback
    r2 = http_get(url, timeout=timeout)
    if r2:
        soup = BeautifulSoup(r2.text, "lxml")
        big=[]
        for sel in ("article","section","main","div","ul","ol"):
            for el in soup.select(sel):
                t = el.get_text(" ", strip=True)
                if t and len(t) > 300:
                    big.append(re.sub(r"\s+"," ", t))
        out = "\n\n".join(dict.fromkeys(big)) if big else soup.get_text(" ", strip=True)
        return clean_text(out), {"source":"bs4","len":len(out),"url_final":url}, r2.text

    return "", {"source":"none","len":0,"url_final":url}, None

# -----------------------------------------------------------------------------
# (4) ë©”íƒ€/ì •ì œ/ê·œì¹™ ê¸°ë°˜ ë³´ì™„
# -----------------------------------------------------------------------------
def extract_company_meta(soup_html: Optional[str]) -> Dict[str,str]:
    meta = {"company_name":"","company_intro":"","job_title":""}
    if not soup_html: return meta
    try:
        soup = BeautifulSoup(soup_html, "lxml")
        cand=[]
        og = soup.find("meta", {"property":"og:site_name"})
        if og and og.get("content"): cand.append(og["content"])
        app = soup.find("meta", {"name":"application-name"})
        if app and app.get("content"): cand.append(app["content"])
        if soup.title and soup.title.string: cand.append(soup.title.string)
        cand = [re.split(r"[\-\|\Â·\â€”]", c)[0].strip() for c in cand if c]
        cand = [c for c in cand if 2 <= len(c) <= 40]
        meta["company_name"] = cand[0] if cand else ""
        md = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
        if md and md.get("content"):
            meta["company_intro"] = re.sub(r"\s+"," ", md["content"]).strip()[:500]
        jt = ""
        ogt = soup.find("meta", {"property":"og:title"})
        if ogt and ogt.get("content"): jt = ogt["content"]
        if not jt:
            h1 = soup.find("h1")
            if h1 and h1.get_text(): jt = h1.get_text(strip=True)
        if not jt:
            h2 = soup.find("h2")
            if h2 and h2.get_text(): jt = h2.get_text(strip=True)
        meta["job_title"] = re.sub(r"\s+"," ", jt).strip()[:120]
    except Exception:
        pass
    return meta

def rule_based_sections(raw_text: str) -> dict:
    txt = clean_text(raw_text, 16000)
    lines = [re.sub(r"\s+"," ", l).strip(" -â€¢Â·â–¶â–ªï¸") for l in txt.split("\n") if l.strip()]
    hdr_resp = re.compile(r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Role|Responsibilities?)", re.I)
    hdr_qual = re.compile(r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements?|Qualifications?)", re.I)
    hdr_pref = re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|ì„ í˜¸|Preferred|Nice\s*to\s*have|Plus)", re.I)
    bucket = None
    out = {"responsibilities": [], "qualifications": [], "preferences": []}

    def push(line, b):
        if line and len(line) > 1 and line not in out[b]:
            out[b].append(line[:180])

    for l in lines:
        if hdr_resp.search(l): bucket="responsibilities"; continue
        if hdr_qual.search(l): bucket="qualifications"; continue
        if hdr_pref.search(l): bucket="preferences"; continue
        if bucket is None:
            if hdr_pref.search(l): bucket="preferences"
            elif any(k in l.lower() for k in ["java","python","spark","airflow","kafka","ml","sql"]):
                bucket="responsibilities"
            else:
                continue
        push(l, bucket)

    kw_pref = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
    remain_qual=[]
    for q in out["qualifications"]:
        (out["preferences"] if kw_pref.search(q) else remain_qual).append(q)
    out["qualifications"]=remain_qual

    for k in out:
        out[k] = list(dict.fromkeys([re.sub(r"\s+"," ", s).strip(" -â€¢Â·â–¶â–ªï¸").strip() for s in out[k]]))[:12]
    return out

PROMPT_SYSTEM_STRUCT = (
    "ë„ˆëŠ” ì±„ìš© ê³µê³ ë¥¼ ê¹”ë”í•˜ê²Œ êµ¬ì¡°í™”í•˜ëŠ” ë³´ì¡°ì›ì´ë‹¤. "
    "ì…ë ¥ í…ìŠ¤íŠ¸ëŠ” ì¡ë‹¤í•œ ê´‘ê³ /UIì”ì¬ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆë‹¤. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì¤‘ë³µ ì—†ì´ ì •ì œí•˜ë¼."
)

def llm_structurize(raw_text: str, meta_hint: Dict[str,str], model: str) -> Dict:
    ctx = clean_text(raw_text, 14000)
    user_msg = {"role":"user","content":(
        "ë‹¤ìŒ ì±„ìš© ê³µê³  ì›ë¬¸ì„ êµ¬ì¡°í™”í•´ì¤˜.\n\n"
        f"[íŒíŠ¸] íšŒì‚¬ëª… í›„ë³´: {meta_hint.get('company_name','')}\n"
        f"[íŒíŠ¸] ì§ë¬´ëª… í›„ë³´: {meta_hint.get('job_title','')}\n"
        "--- ì›ë¬¸ ì‹œì‘ ---\n"
        f"{ctx}\n"
        "--- ì›ë¬¸ ë ---\n\n"
        "JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , í‚¤ëŠ” ë°˜ë“œì‹œ ì•„ë˜ë§Œ í¬í•¨:\n"
        "{"
        "\"company_name\": str, "
        "\"company_intro\": str, "
        "\"job_title\": str, "
        "\"responsibilities\": [str], "
        "\"qualifications\": [str], "
        "\"preferences\": [str]"
        "}\n"
        "- 'ìš°ëŒ€ ì‚¬í•­(preferences)'ì€ í‘œì‹œê°€ ìˆëŠ” í•­ëª©ë§Œ í¬í•¨.\n"
        "- ë¶ˆë¦¿/ì´ëª¨ì§€ ì œê±°, ê°„ê²°í™”, ì¤‘ë³µ ì œê±°."
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2, max_tokens=900,
            response_format={"type":"json_object"},
            messages=[{"role":"system","content":PROMPT_SYSTEM_STRUCT}, user_msg]
        )
        data = json.loads(resp.choices[0].message.content)
    except Exception as e:
        data = {"company_name": meta_hint.get("company_name",""),
                "company_intro": meta_hint.get("company_intro","ì›ë¬¸ì´ ì •ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."),
                "job_title": meta_hint.get("job_title",""),
                "responsibilities": [], "qualifications": [], "preferences": [], "error": str(e)}

    for k in ["responsibilities","qualifications","preferences"]:
        arr = data.get(k, [])
        if not isinstance(arr, list): arr=[]
        clean=[]; seen=set()
        for it in arr:
            t = re.sub(r"\s+"," ", str(it)).strip(" -â€¢Â·â–¶â–ªï¸").strip()
            if t and t not in seen:
                seen.add(t); clean.append(t[:180])
        data[k] = clean[:12]

    if len(data.get("preferences", [])) < 1:
        rb = rule_based_sections(ctx)
        if rb.get("preferences"):
            data["preferences"] = list(dict.fromkeys(data.get("preferences", []) + rb["preferences"]))[:12]
        else:
            kw_pref = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
            remain=[]; moved=[]
            for q in data.get("qualifications", []):
                (moved if kw_pref.search(q) else remain).append(q)
            data["preferences"]=moved[:12]; data["qualifications"]=remain[:12]

    for k in ["company_name","company_intro","job_title"]:
        if isinstance(data.get(k), str):
            data[k] = re.sub(r"\s+"," ", data[k]).strip()
    return data

# -----------------------------------------------------------------------------
# (5) íŒŒì¼ ë¦¬ë” / ì„ë² ë”© / ê²€ìƒ‰ / LLM ì§ˆë¬¸Â·ì´ˆì•ˆÂ·ì±„ì 
# -----------------------------------------------------------------------------
try:
    import pypdf
except Exception:
    pypdf = None

def read_pdf(data: bytes) -> str:
    if pypdf is None: return ""
    try:
        reader = pypdf.PdfReader(io.BytesIO(data))
        return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
    except Exception:
        return ""

def read_docx(data: bytes) -> str:
    try:
        import docx2txt
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=True) as tmp:
            tmp.write(data); tmp.flush()
            return docx2txt.process(tmp.name) or ""
    except Exception:
        return ""

def read_file_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt",".md")):
        for enc in ("utf-8","cp949","euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        return read_pdf(data)
    elif name.endswith(".docx"):
        return read_docx(data)
    return ""

def chunk(text: str, size: int = 600, overlap: int = 120) -> List[str]:
    t = re.sub(r"\s+"," ", text or "").strip()
    if not t: return []
    out, start = [], 0
    L = len(t)
    while start < L:
        end = min(L, start+size)
        out.append(t[start:end])
        if end == L: break
        start = max(0, end-overlap)
    return out

def embed_texts(texts: List[str], model_name: str) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=model_name, input=texts)
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    return vecs

def l2_normalize(mat: np.ndarray) -> np.ndarray:
    if mat.size == 0: return mat
    n = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12
    return mat / n

def cosine_topk(matrix_n: np.ndarray, query_vec_n: np.ndarray, k: int = 4):
    if matrix_n.size == 0: return np.array([]), np.array([], dtype=int)
    sims = matrix_n @ query_vec_n.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_resume_chunks(query: str, chunks: List[str], embeds_norm: np.ndarray, k: int = 4):
    if not chunks or embeds_norm is None or embeds_norm.size == 0:
        return []
    qv = embed_texts([query], EMBED_MODEL)
    qv_n = l2_normalize(qv)
    scores, idxs = cosine_topk(embeds_norm, qv_n, k=k)
    return [(float(s), chunks[int(i)]) for s, i in zip(scores, idxs)]

PROMPT_SYSTEM_Q = (
    "ë„ˆëŠ” ì±„ìš©ë‹´ë‹¹ìë‹¤. íšŒì‚¬/ì§ë¬´ ë§¥ë½ê³¼ ì±„ìš©ìš”ê±´, ê·¸ë¦¬ê³  ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½ì„ í•¨ê»˜ ê³ ë ¤í•´ "
    "ë©´ì ‘ ì§ˆë¬¸ì„ í•œêµ­ì–´ë¡œ ìƒì„±í•œë‹¤. ì§ˆë¬¸ì€ ì„œë¡œ ê²¹ì¹˜ì§€ ì•Šê²Œ ë‹¤ì–‘í™”í•˜ê³ , ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ê·œëª¨/ë¦¬ìŠ¤í¬/íŠ¸ë ˆì´ë“œì˜¤í”„ë„ ì„ì–´ë¼."
)
PROMPT_SYSTEM_DRAFT = (
    "ë„ˆëŠ” ë©´ì ‘ ë‹µë³€ ì½”ì¹˜ë‹¤. íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´ê³¼ ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½ì„ ê²°í•©í•´ "
    "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì´ˆì•ˆì„ STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)ë¡œ 8~12ë¬¸ì¥, í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤. "
    "ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì§€í‘œ/ìˆ˜ì¹˜/ê¸°ê°„/ì„íŒ©íŠ¸ë¥¼ í¬í•¨í•˜ë¼."
)
PROMPT_SYSTEM_SCORE_STRICT = (
    "ë„ˆëŠ” ë§¤ìš° ì—„ê²©í•œ í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ë¼. "
    "ê° ê¸°ì¤€ì€ 0~20 ì •ìˆ˜ì´ë©°, ì´ì ì€ ê¸°ì¤€ í•©ê³„(ìµœëŒ€ 100)ì™€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•œë‹¤. "
    "ê³¼ì¥/ëª¨í˜¸í•¨/ê·¼ê±° ë¶€ì¬/ìˆ«ì ì—†ëŠ” ì£¼ì¥/ì±…ì„ íšŒí”¼ ë“±ì„ ê°•í•˜ê²Œ ê°ì í•˜ë¼."
)
CRITERIA = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]

def llm_generate_one_question_with_resume(clean: Dict, level: str, model: str,
                                          resume_chunks: List[str], resume_embeds_norm: np.ndarray) -> str:
    hits = retrieve_resume_chunks("í•µì‹¬ í”„ë¡œì íŠ¸ì™€ ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½", resume_chunks, resume_embeds_norm, k=4)
    resume_context = "\n".join([f"- {t[:350]}" for _, t in hits])[:1200]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´]\n{ctx}\n\n"
        f"[ì§€ì›ì ì´ë ¥ì„œ ìš”ì•½(ë°œì·Œ)]\n{resume_context}\n\n"
        f"[ìš”ì²­]\n- ë‚œì´ë„/ì—°ì°¨: {level}\n"
        f"- ì¤‘ë³µ/ìœ ì‚¬ë„ ì§€ì–‘, êµì§‘í•© ë˜ëŠ” ê³µë°±ì˜ì—­ ê²¨ëƒ¥\n"
        f"- í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë§Œ í•œ ì¤„ë¡œ ì¶œë ¥"
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.7, max_tokens=120,
            messages=[{"role":"system","content":PROMPT_SYSTEM_Q}, user_msg]
        )
        q = resp.choices[0].message.content.strip()
        q = re.sub(r"^\s*\d+[\).\s-]*","", q).split("\n")[0].strip()
        return q
    except Exception:
        return ""

def llm_draft_answer(clean: Dict, question: str, model: str,
                     resume_chunks: List[str], resume_embeds_norm: np.ndarray) -> str:
    hits = retrieve_resume_chunks(question, resume_chunks, resume_embeds_norm, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
        f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
        f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ STAR ê¸°ë°˜ í•œêµ­ì–´ ë‹µë³€ **ì´ˆì•ˆ**ì„ ì‘ì„±í•´ì¤˜."
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.5, max_tokens=700,
            messages=[{"role":"system","content":PROMPT_SYSTEM_DRAFT}, user_msg]
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        return ""

def llm_score_and_coach_strict(clean: Dict, question: str, answer: str, model: str,
                               resume_chunks: List[str], resume_embeds_norm: np.ndarray) -> Dict:
    hits = retrieve_resume_chunks(question + "\n" + answer[:800], resume_chunks, resume_embeds_norm, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
        f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
        f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
        f"[ì§€ì›ì ë‹µë³€]\n{answer}\n\n"
        "ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ í•œêµ­ì–´ ì‘ë‹µ:\n"
        "{"
        "\"overall_score\": 0~100 ì •ìˆ˜,"
        "\"criteria\": [{\"name\":\"ë¬¸ì œì •ì˜\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ë°ì´í„°/ì§€í‘œ\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ì‹¤í–‰ë ¥/ì£¼ë„ì„±\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ê³ ê°ê°€ì¹˜\",\"score\":0~20,\"comment\":\"...\"}],"
        "\"strengths\": [\"...\"],"
        "\"risks\": [\"...\"],"
        "\"improvements\": [\"...\",\"...\",\"...\"],"
        "\"revised_answer\": \"STAR êµ¬ì¡°ë¡œ ê°„ê²°íˆ\""
        "}"
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2, max_tokens=900,
            response_format={"type":"json_object"},
            messages=[{"role":"system","content":PROMPT_SYSTEM_SCORE_STRICT}, user_msg]
        )
        data = json.loads(resp.choices[0].message.content)
        fixed=[]
        for name in CRITERIA:
            found=None
            for it in data.get("criteria", []):
                if str(it.get("name","")).strip()==name:
                    found=it; break
            if not found: found={"name":name,"score":0,"comment":""}
            sc = int(found.get("score",0)); sc=max(0,min(20,sc))
            found["score"]=sc; found["comment"]=str(found.get("comment","")).strip()
            fixed.append(found)
        total=sum(x["score"] for x in fixed)
        data["criteria"]=fixed
        data["overall_score"]=total
        for k in ["strengths","risks","improvements"]:
            arr=data.get(k,[]); 
            if not isinstance(arr,list): arr=[]
            data[k]=[str(x).strip() for x in arr if str(x).strip()][:5]
        data["revised_answer"]=str(data.get("revised_answer","")).strip()
        return data
    except Exception as e:
        return {"overall_score": 0,
                "criteria": [{"name": n, "score": 0, "comment": ""} for n in CRITERIA],
                "strengths": [], "risks": [], "improvements": [], "revised_answer": "",
                "error": str(e)}

# -----------------------------------------------------------------------------
# (6) ì„¸ì…˜ ìƒíƒœ
# -----------------------------------------------------------------------------
def _init_state():
    defaults = {
        "clean_struct": None,
        "resume_raw": "",
        "resume_chunks": [],
        "resume_embeds": None,
        "resume_embeds_norm": None,
        "current_question": "",
        "answer_text": "",
        "records": [],
        "followups": [],
        "selected_followup": "",
        "followup_answer": "",
        "last_result": None,
        "last_followup_result": None,
        "company_home": "",
        "company_vision": [],
        "company_talent": [],
        "company_news": []
    }
    for k, v in defaults.items():
        if k not in st.session_state: st.session_state[k] = v
_init_state()

# -----------------------------------------------------------------------------
# (7) UI: 1) ì±„ìš© ê³µê³  URL ì…ë ¥ â†’ ì›ë¬¸ ìˆ˜ì§‘Â·ì •ì œ
# -----------------------------------------------------------------------------
st.header("1) ì±„ìš© ê³µê³  URL")
url = st.text_input("ì±„ìš© ê³µê³  ìƒì„¸ URL", placeholder="ì·¨ì—… í¬í„¸ ì‚¬ì´íŠ¸ì˜ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
st.text_input("íšŒì‚¬ ê³µì‹ í™ˆí˜ì´ì§€ URL (ì„ íƒ)", key="company_home", placeholder="íšŒì‚¬ ê³µì‹ í™ˆí˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”.")

if st.button("ì›ë¬¸ ìˆ˜ì§‘ â†’ ì •ì œ", type="primary"):
    if not url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ì›ë¬¸ ìˆ˜ì§‘ ì¤‘..."):
            raw, meta, soup_html = fetch_all_text(
                url.strip(),
                use_selenium=ENABLE_SELENIUM,
                timeout=SELENIUM_TIMEOUT,
                force_selenium=FORCE_SELENIUM,   # â˜… ê°•ì œ ì‚¬ìš© ì˜µì…˜ ì „ë‹¬
            )
            hint = extract_company_meta(soup_html)

        # ìˆ˜ì§‘ ì†ŒìŠ¤/ê¸¸ì´ í‘œì‹œ
        st.caption(f"ìˆ˜ì§‘ ì†ŒìŠ¤: {meta.get('source')} Â· ê¸¸ì´: {meta.get('len')}")

        if not raw:
            if meta.get("source") in ("selenium_failed","selenium_error"):
                st.error("Selenium ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (Chrome/í¬ë¡œë¯¸ì›€ ì„¤ì¹˜, íƒ€ì„ì•„ì›ƒ/ì…€ë ‰í„° í™•ì¸ í•„ìš”)")
            else:
                st.error("ì›ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¡œê·¸ì¸/ë™ì  ë Œë”ë§/ë´‡ ì°¨ë‹¨ ê°€ëŠ¥)")
        else:
            with st.spinner("LLMìœ¼ë¡œ ì •ì œ ì¤‘..."):
                clean = llm_structurize(raw, hint, CHAT_MODEL)

            # ê·œì¹™ ê¸°ë°˜ ìš°ëŒ€ì‚¬í•­ ë³´ì™„
            if not clean.get("preferences"):
                rb = rule_based_sections(raw)
                if rb.get("preferences"):
                    clean["preferences"] = rb["preferences"][:12]

            st.session_state.clean_struct = clean

            # ê°„ë‹¨ ì§„ë‹¨: ìš°ëŒ€ í‚¤ì›Œë“œ ë“±ì¥ëŸ‰
            try:
                kw_cnt = sum(1 for x in re.findall(r"[ê°€-í£A-Za-z]+", raw or "") if any(k in x for k in ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","Preferred"]))
                st.caption(f"ì§„ë‹¨: ì›ë¬¸ ë‚´ 'ìš°ëŒ€' í‚¤ì›Œë“œ ì¶”ì • ë“±ì¥ ìˆ˜ â‰ˆ {kw_cnt}")
            except Exception:
                pass

            # íšŒì‚¬ ë¹„ì „/ì¸ì¬ìƒ/ë‰´ìŠ¤
            if ENABLE_COMPANY_ENRICH:
                with st.spinner("íšŒì‚¬ ë¹„ì „/ì¸ì¬ìƒ/ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
                    vis, tal, news = [], [], []
                    tasks = []
                    with ThreadPoolExecutor(max_workers=3) as ex:
                        home = (st.session_state.company_home or "").strip()
                        if home:
                            tasks.append(("pages", ex.submit(fetch_company_pages, home)))
                        cname = clean.get("company_name") or hint.get("company_name") or ""
                        if cname:
                            tasks.append(("news", ex.submit(fetch_latest_news, cname, 5)))
                        for tag, fut in tasks:
                            try:
                                res = fut.result()
                                if tag=="pages":
                                    vis = res.get("vision", [])
                                    tal = res.get("talent", [])
                                else:
                                    news = res
                            except Exception:
                                pass
                    st.session_state.company_vision = vis
                    st.session_state.company_talent = tal
                    st.session_state.company_news = news
            else:
                st.session_state.company_vision = []
                st.session_state.company_talent = []
                st.session_state.company_news = []
            st.success("ì •ì œ ì™„ë£Œ!")

# -----------------------------------------------------------------------------
# (8) UI: 2) íšŒì‚¬ ìš”ì•½ ì„¹ì…˜
# -----------------------------------------------------------------------------
st.header("2) íšŒì‚¬ ìš”ì•½")
clean = st.session_state.clean_struct
if clean:
    st.markdown(f"**íšŒì‚¬ëª…:** {clean.get('company_name','-')}")
    st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½):** {clean.get('company_intro','-')}")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼(ì§ë¬´ëª…):** {clean.get('job_title','-')}")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´**")
        for b in clean.get("responsibilities", []): st.markdown(f"- {b}")
    with c2:
        st.markdown("**ìê²© ìš”ê±´**")
        for b in clean.get("qualifications", []): st.markdown(f"- {b}")
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­**")
        prefs = clean.get("preferences", [])
        if prefs:
            for b in prefs: st.markdown(f"- {b}")
        else:
            st.caption("ìš°ëŒ€ ì‚¬í•­ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
else:
    st.info("ë¨¼ì € URLì„ ì •ì œí•´ ì£¼ì„¸ìš”.")

# íšŒì‚¬ ë¹„ì „/ì¸ì¬ìƒ/ë‰´ìŠ¤ í‘œì‹œ
if ENABLE_COMPANY_ENRICH and (st.session_state.company_vision or st.session_state.company_talent or st.session_state.company_news):
    st.divider()
    st.subheader("íšŒì‚¬ ë¹„ì „/ì¸ì¬ìƒ & ìµœì‹  ì´ìŠˆ")
    colv, colt = st.columns(2)
    with colv:
        st.markdown("**ë¹„ì „/í•µì‹¬ê°€ì¹˜ (ìŠ¤í¬ë˜í•‘)**")
        for v in st.session_state.company_vision[:8]:
            st.markdown(f"- {v}")
        if not st.session_state.company_vision:
            st.caption("ë¹„ì „/í•µì‹¬ê°€ì¹˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    with colt:
        st.markdown("**ì¸ì¬ìƒ (ìŠ¤í¬ë˜í•‘)**")
        for t in st.session_state.company_talent[:8]:
            st.markdown(f"- {t}")
        if not st.session_state.company_talent:
            st.caption("ì¸ì¬ìƒ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    if st.session_state.company_news:
        st.markdown("**ìµœì‹  ë‰´ìŠ¤(ìƒìœ„ 3~5ê±´)**")
        for n in st.session_state.company_news[:5]:
            st.markdown(f"- [{n.get('title','(ì œëª© ì—†ìŒ)')}]({n.get('link','#')})")

st.divider()

# -----------------------------------------------------------------------------
# (9) UI: 3) ì´ë ¥ì„œ ì—…ë¡œë“œ/ì¸ë±ì‹±
# -----------------------------------------------------------------------------
st.header("3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ")
uploads = st.file_uploader("ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ ê°€ëŠ¥", type=["pdf","txt","md","docx"], accept_multiple_files=True)
_RESUME_CHUNK = 500; _RESUME_OVLP = 100

col_idx = st.columns(2)
with col_idx[0]:
    if st.button("ì´ë ¥ì„œ ì¸ë±ì‹±", type="secondary"):
        if not uploads:
            st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            all_text=[]
            for up in uploads:
                t = read_file_text(up)
                if t: all_text.append(t)
            resume_text = "\n\n".join(all_text)
            if not resume_text.strip():
                st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                chunks = chunk(resume_text, size=_RESUME_CHUNK, overlap=_RESUME_OVLP)
                with st.spinner("ì´ë ¥ì„œ ë²¡í„°í™” ì¤‘..."):
                    embeds = embed_texts(chunks, EMBED_MODEL)
                    embeds_norm = l2_normalize(embeds)
                st.session_state.resume_raw = resume_text
                st.session_state.resume_chunks = chunks
                st.session_state.resume_embeds = embeds
                st.session_state.resume_embeds_norm = embeds_norm
                st.success(f"ì¸ë±ì‹± ì™„ë£Œ (ì²­í¬ {len(chunks)}ê°œ)")

st.divider()

# -----------------------------------------------------------------------------
# (10) UI: 4) ìì†Œì„œ ìƒì„±
# -----------------------------------------------------------------------------
st.header("4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„±")
topic = st.text_input("íšŒì‚¬ ìš”ì²­ ì£¼ì œ(ì„ íƒ)", placeholder="ì˜ˆ: ì§ë¬´ ì§€ì›ë™ê¸° / í˜‘ì—… ê²½í—˜ / ë¬¸ì œí•´ê²° ì‚¬ë¡€ ë“±")

def build_cover_letter(clean_struct: Dict, resume_text: str, topic_hint: str, model: str) -> str:
    enrich = {"vision": st.session_state.company_vision[:6],
              "talent": st.session_state.company_talent[:6],
              "news": [n.get("title","") for n in st.session_state.company_news[:3]]}
    company = json.dumps({"clean":clean_struct, "extra":enrich}, ensure_ascii=False)
    resume_snippet = resume_text.strip()[:9000]
    system = ("ë„ˆëŠ” í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì „ë¬¸ê°€ë‹¤. ì±„ìš© ê³µê³ ì˜ íšŒì‚¬/ì§ë¬´ ìš”ê±´ê³¼ í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ì°¸ê³ í•´ "
              "íšŒì‚¬ íŠ¹í™” ìì†Œì„œë¥¼ ì‘ì„±í•œë‹¤. ê³¼ì¥/í—ˆìœ„ëŠ” ê¸ˆì§€í•˜ê³ , ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ì„íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´í™”í•œë‹¤. "
              "íšŒì‚¬ì˜ ë¹„ì „/ì¸ì¬ìƒ/ìµœê·¼ ì´ìŠˆê°€ ì œê³µë˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ë¼.")
    req = (f"íšŒì‚¬ ì¸¡ ìš”ì²­ ì£¼ì œëŠ” '{topic_hint.strip()}' ì´ë‹¤. ì´ ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•˜ë¼."
           if topic_hint and topic_hint.strip() else
           "íŠ¹ì • ì£¼ì œ ìš”ì²­ì´ ì—†ìœ¼ë¯€ë¡œ, ì±„ìš© ê³µê³ ì™€ ë¹„ì „/ì¸ì¬ìƒì„ ì¤‘ì‹¬ìœ¼ë¡œ ì§€ì›ë™ê¸°ì™€ ì§ë¬´ì í•©ì„±ì„ ê°•ì¡°í•˜ë¼.")
    user = (f"[íšŒì‚¬/ì§ë¬´ ìš”ì•½(JSON)]\n{company}\n\n"
            f"[í›„ë³´ì ì´ë ¥ì„œ(ìš”ì•½ ê°€ëŠ¥)]\n{resume_snippet}\n\n"
            f"[ì‘ì„± ì§€ì‹œ]\n- {req}\n"
            "- ë¶„ëŸ‰: 600~900ì\n"
            "- êµ¬ì„±: 1) ì§€ì› ë™ê¸° 2) ì§ë¬´ ê´€ë ¨ í•µì‹¬ ì—­ëŸ‰Â·ê²½í—˜ 3) ì„±ê³¼/ì§€í‘œ 4) ì…ì‚¬ í›„ ê¸°ì—¬ ë°©ì•ˆ 5) ë§ˆë¬´ë¦¬\n"
            "- ìì—°ìŠ¤ëŸ½ê³  ì§„ì •ì„± ìˆëŠ” 1ì¸ì¹­ ì„œìˆ . ë¬¸ì¥ê³¼ ë¬¸ë‹¨ ê°€ë…ì„±ì„ ìœ ì§€.\n"
            "- ë¶ˆí•„ìš”í•œ ë¯¸ì‚¬ì—¬êµ¬/ì¤‘ë³µ/ê´‘ê³  ë¬¸êµ¬ ì‚­ì œ.")
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.4, max_tokens=800,
            messages=[{"role":"system","content":system},{"role":"user","content":user}]
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(ìì†Œì„œ ìƒì„± ì‹¤íŒ¨: {e})"

if st.button("ìì†Œì„œ ìƒì„±", type="primary"):
    if not st.session_state.clean_struct:
        st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
    elif not st.session_state.resume_raw.strip():
        st.warning("ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ì´ë ¥ì„œ ì¸ë±ì‹±'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ìì†Œì„œ ìƒì„± ì¤‘..."):
            cover = build_cover_letter(st.session_state.clean_struct, st.session_state.resume_raw, topic, CHAT_MODEL)
        st.subheader("ìì†Œì„œ (ìƒì„± ê²°ê³¼)")
        st.write(cover)
        st.download_button("ìì†Œì„œ TXT ë‹¤ìš´ë¡œë“œ", data=cover.encode("utf-8"), file_name="cover_letter.txt", mime="text/plain")

st.divider()

# -----------------------------------------------------------------------------
# (11) UI: 5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ
# -----------------------------------------------------------------------------
st.header("5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ (RAG ê²°í•©)")
level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"], index=0)

cols_q = st.columns(2)
with cols_q[0]:
    if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", type="primary"):
        if not st.session_state.clean_struct:
            st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
        else:
            q = llm_generate_one_question_with_resume(
                st.session_state.clean_struct, level, CHAT_MODEL,
                st.session_state.resume_chunks, st.session_state.resume_embeds_norm
            )
            if q:
                st.session_state.current_question = q
                st.session_state.answer_text = ""
                st.session_state.last_result = None
                st.session_state.followups = []
                st.session_state.selected_followup = ""
                st.session_state.followup_answer = ""
                st.success("ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
with cols_q[1]:
    if st.button("RAGë¡œ ë‹µë³€ ì´ˆì•ˆ ìƒì„±", type="secondary"):
        if not st.session_state.current_question:
            st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        else:
            draft = llm_draft_answer(
                st.session_state.clean_struct, st.session_state.current_question, CHAT_MODEL,
                st.session_state.resume_chunks, st.session_state.resume_embeds_norm
            )
            if draft:
                st.session_state.answer_text = draft
                st.success("ì´ˆì•ˆ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨")

st.text_area("ì§ˆë¬¸", value=st.session_state.current_question, height=100)
st.text_area("ë‚˜ì˜ ë‹µë³€ (ì´ˆì•ˆì„ í¸ì§‘í•´ ì™„ì„±í•˜ì„¸ìš”)", height=200, key="answer_text")

# -----------------------------------------------------------------------------
# (12) UI: 6) ì±„ì  & ì½”ì¹­
# -----------------------------------------------------------------------------
st.header("6) ì±„ì  & ì½”ì¹­")
if st.button("ì±„ì  & ì½”ì¹­ ì‹¤í–‰", type="primary"):
    if not st.session_state.current_question:
        st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì±„ì /ì½”ì¹­ ì¤‘..."):
            res = llm_score_and_coach_strict(
                st.session_state.clean_struct,
                st.session_state.current_question,
                st.session_state.answer_text,
                CHAT_MODEL,
                st.session_state.resume_chunks,
                st.session_state.resume_embeds_norm
            )
        st.session_state.last_result = res
        st.session_state.records.append({
            "question": st.session_state.current_question,
            "answer": st.session_state.answer_text,
            "overall": res.get("overall_score", 0),
            "criteria": res.get("criteria", []),
            "strengths": res.get("strengths", []),
            "risks": res.get("risks", []),
            "improvements": res.get("improvements", []),
            "revised_answer": res.get("revised_answer","")
        })
        st.success("ì±„ì /ì½”ì¹­ ì™„ë£Œ!")

# -----------------------------------------------------------------------------
# (13) UI: 7) í”¼ë“œë°± & íŒ”ë¡œì—…
# -----------------------------------------------------------------------------
st.header("7) í”¼ë“œë°± ê²°ê³¼")
last = st.session_state.last_result
if last:
    left, right = st.columns([1,3])
    with left:
        st.metric("ì´ì (/100)", last.get("overall_score", 0))
    with right:
        st.markdown("**ê¸°ì¤€ë³„ ì ìˆ˜ & ì½”ë©˜íŠ¸**")
        for it in last.get("criteria", []):
            st.markdown(f"- **{it['name']}**: {it['score']}/20 â€” {it.get('comment','')}")
        if last.get("strengths"):
            st.markdown("**ê°•ì **")
            for s in last["strengths"]: st.markdown(f"- {s}")
        if last.get("risks"):
            st.markdown("**ê°ì  ìš”ì¸/ë¦¬ìŠ¤í¬**")
            for r in last["risks"]: st.markdown(f"- {r}")
        if last.get("improvements"):
            st.markdown("**ê°œì„  í¬ì¸íŠ¸**")
            for im in last["improvements"]: st.markdown(f"- {im}")
        if last.get("revised_answer"):
            st.markdown("**ìˆ˜ì •ë³¸ ë‹µë³€ (STAR)**")
            st.write(last["revised_answer"])
else:
    st.info("ì•„ì§ ì±„ì  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("íŒ”ë¡œì—… ì§ˆë¬¸ Â· ë‹µë³€ Â· í”¼ë“œë°±")
if last and not st.session_state.followups:
    try:
        ctx = json.dumps({"company": st.session_state.clean_struct,
                          "vision": st.session_state.company_vision[:6],
                          "talent": st.session_state.company_talent[:6],
                          "news": [n.get("title","") for n in st.session_state.company_news[:3]]}, ensure_ascii=False)
        msg = {"role":"user","content":(
            f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´/ë¹„ì „/ì´ìŠˆ]\n{ctx}\n\n"
            f"[ì§€ì›ì ë‹µë³€]\n{st.session_state.answer_text}\n\n"
            "ë©´ì ‘ê´€ ê´€ì ì—ì„œ íŒ”ë¡œì—… ì§ˆë¬¸ 3ê°œë¥¼ í•œ ì¤„ì”© í•œêµ­ì–´ë¡œ ì œì•ˆí•´ì¤˜. "
            "ê¸°ì¡´ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê²Œ, ì§€í‘œ/ë¦¬ìŠ¤í¬/íŠ¸ë ˆì´ë“œì˜¤í”„/ì˜ì‚¬ê²°ì • ê·¼ê±°ë¥¼ ì„ì–´ì¤˜."
        )}
        r = client.chat.completions.create(model=CHAT_MODEL, temperature=0.7, max_tokens=240,
                                           messages=[{"role":"system","content":"ë©´ì ‘ íŒ”ë¡œì—… ìƒì„±ê¸°"}, msg])
        followups = [re.sub(r'^\s*\d+[\).\s-]*','', l).strip()
                     for l in r.choices[0].message.content.splitlines() if l.strip()]
        st.session_state.followups = followups[:3]
    except Exception:
        st.session_state.followups = []

if last:
    if st.session_state.followups:
        st.markdown("**íŒ”ë¡œì—… ì§ˆë¬¸ ì œì•ˆ**")
        for i, f in enumerate(st.session_state.followups, 1):
            st.markdown(f"- ({i}) {f}")

        st.selectbox("ì±„ì  ë°›ì„ íŒ”ë¡œì—… ì§ˆë¬¸ ì„ íƒ", st.session_state.followups, index=0, key="selected_followup")
        st.text_area("íŒ”ë¡œì—… ì§ˆë¬¸ì— ëŒ€í•œ ë‚˜ì˜ ë‹µë³€", height=160, key="followup_answer")
        if st.button("íŒ”ë¡œì—… ì±„ì  & í”¼ë“œë°±", type="secondary"):
            fu_q   = st.session_state.get("selected_followup", "")
            fu_ans = st.session_state.get("followup_answer", "")
            if not fu_q:
                st.warning("íŒ”ë¡œì—… ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.")
            elif not fu_ans.strip():
                st.warning("íŒ”ë¡œì—… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.")
            else:
                with st.spinner("íŒ”ë¡œì—… ì±„ì  ì¤‘..."):
                    res_fu = llm_score_and_coach_strict(
                        st.session_state.clean_struct, fu_q, fu_ans, CHAT_MODEL,
                        st.session_state.resume_chunks, st.session_state.resume_embeds_norm
                    )
                st.markdown("**íŒ”ë¡œì—… ê²°ê³¼**")
                st.metric("ì´ì (/100)", res_fu.get("overall_score", 0))
                for it in res_fu.get("criteria", []):
                    st.markdown(f"- **{it['name']}**: {it['score']}/20 â€” {it.get('comment','')}")
                if res_fu.get("revised_answer",""):
                    st.markdown("**íŒ”ë¡œì—… ìˆ˜ì •ë³¸ (STAR)**")
                    st.write(res_fu["revised_answer"])
    else:
        st.caption("íŒ”ë¡œì—… ì§ˆë¬¸ì€ ë©”ì¸ ì§ˆë¬¸ ì±„ì  ì§í›„ ìë™ ì œì•ˆë©ë‹ˆë‹¤.")
