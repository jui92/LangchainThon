# -*- coding: utf-8 -*-
################################################################################
# Job Helper Bot (Selenium-ONLY + Speed-up + Follow-up restored)
# - ì±„ìš© í¬í„¸ URL + (ì„ íƒ) ê¸°ì—… í™ˆí˜ì´ì§€ URL + ìµœì‹  ë‰´ìŠ¤ â†’ ëª¨ì˜ë©´ì ‘/ìì†Œì„œ
# - Selenium ì „ìš© ìˆ˜ì§‘(ì›í‹°ë“œ Next.js __NEXT_DATA__ ë³‘í•©), ê·œì¹™ íŒŒì„œ ë³´ê°•
# - ì†ë„ ê°œì„ : Fast ëª¨ë“œ, ë™ì‹œ ì²˜ë¦¬(ThreadPoolExecutor), ìºì‹œ(st.cache_data)
# - íŒ”ë¡œì—… ì§ˆë¬¸ Â· ë‹µë³€ Â· í”¼ë“œë°± UI ë³µì›
################################################################################

import os, re, io, json, time, shutil, urllib.parse, tempfile, traceback
from typing import Optional, Tuple, Dict, List
from concurrent.futures import ThreadPoolExecutor

import streamlit as st
import numpy as np
import pandas as pd
import requests
import html2text
from bs4 import BeautifulSoup

# OpenAI
try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

# Selenium (Selenium Manager ì‚¬ìš©)
from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# (ì„ íƒ) ë¬¸ì„œ íŒŒì„œ
try:
    import pypdf
except Exception:
    pypdf = None
try:
    import docx2txt
except Exception:
    docx2txt = None

# -----------------------------------------------------------------------------
# App/Sidebar
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Job Helper Bot", page_icon="ğŸ¤–", layout="wide")
st.title("Job Helper Bot : ìì†Œì„œ ìƒì„± / ëª¨ì˜ ë©´ì ‘")

API_KEY = os.getenv("OPENAI_API_KEY") or (st.secrets.get("OPENAI_API_KEY", None) if hasattr(st, "secrets") else None)
if not API_KEY:
    API_KEY = st.text_input("OPENAI_API_KEY ì…ë ¥", type="password", help="OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
if not API_KEY:
    st.stop()
client = OpenAI(api_key=API_KEY)

with st.sidebar:
    st.subheader("ëª¨ë¸ / í¬ë¡¤ë§ ì˜µì…˜")
    CHAT_MODEL = st.selectbox("ëŒ€í™”/ìƒì„± ëª¨ë¸", ["gpt-4o-mini","gpt-4o"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)
    SELENIUM_TIMEOUT = st.slider("Selenium ëŒ€ê¸°(ì´ˆ)", 6, 30, 14)
    FAST_MODE = st.toggle("Fast ëª¨ë“œ(í™•ì¥ ìµœì†Œí™”, ë” ë¹ ë¦„)", value=True)
    st.caption("Fast ëª¨ë“œ: í´ë¦­/ìŠ¤í¬ë¡¤ ì‹œë„ íšŸìˆ˜ë¥¼ ì¤„ì—¬ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.")

# -----------------------------------------------------------------------------
# html2text
# -----------------------------------------------------------------------------
def _get_html2text():
    conv = html2text.HTML2Text()
    conv.ignore_links = True
    conv.ignore_images = True
    conv.body_width = 0
    return conv
HTML2TEXT = _get_html2text()

def html_to_text(html_str: str) -> str:
    txt = HTML2TEXT.handle(html_str or "")
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return re.sub(r"\s+", " ", txt).strip()

# -----------------------------------------------------------------------------
# Utils
# -----------------------------------------------------------------------------
def normalize_url(u: str) -> Optional[str]:
    if not u: return None
    u = u.strip()
    if not re.match(r"^https?://", u): u = "https://" + u
    parts = urllib.parse.urlsplit(u)
    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))

def clean_text(s: str, max_len: int = 16000) -> str:
    if not s: return ""
    s = re.sub(r"\r", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s[:max_len] if len(s) > max_len else s

# -----------------------------------------------------------------------------
# Selenium driver
# -----------------------------------------------------------------------------
def _pick_chrome_binary() -> Optional[str]:
    cands = [
        os.getenv("CHROME_BIN"), os.getenv("GOOGLE_CHROME_BIN"),
        shutil.which("chromium"), shutil.which("chromium-browser"),
        shutil.which("google-chrome"),
        "/usr/bin/chromium","/usr/bin/chromium-browser",
        "/usr/bin/google-chrome","/usr/bin/google-chrome-stable",
    ]
    for p in cands:
        if p and os.path.exists(p): return p
    return None

def _build_chrome(headless: bool = True):
    opts = ChromeOptions()
    if headless: opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1440,2400")
    opts.add_argument("--lang=ko-KR")
    binpath = _pick_chrome_binary()
    if binpath: opts.binary_location = binpath
    driver = webdriver.Chrome(options=opts)  # Selenium Manager ìë™
    return driver

# -----------------------------------------------------------------------------
# Domain expand helpers
# -----------------------------------------------------------------------------
def _click_by_text_candidates(driver, texts: List[str], per=12):
    for t in texts:
        try:
            xp1 = f"//*[normalize-space(text())='{t}']"
            xp2 = f"//*[contains(normalize-space(text()), '{t}')]"
            for xp in (xp1, xp2):
                els = driver.find_elements(By.XPATH, xp)
                for el in els[:per]:
                    try:
                        driver.execute_script("arguments[0].click();", el)
                        time.sleep(0.15 if FAST_MODE else 0.25)
                    except Exception:
                        continue
        except Exception:
            continue

def _click_many_css(driver, selectors: List[str], per=12):
    for sel in selectors:
        try:
            els = driver.find_elements(By.CSS_SELECTOR, sel)
            for el in els[:per]:
                try:
                    driver.execute_script("arguments[0].click();", el)
                    time.sleep(0.12 if FAST_MODE else 0.2)
                except Exception:
                    continue
        except Exception:
            continue

def _expand_wanted(driver):
    sel = [
        "[data-qa='btn-read-more']","[data-qa='job-header__more']",
        "button[aria-expanded='false']","[role='button'][class*='More']",
        "div[aria-expanded='false']",
    ]
    _click_many_css(driver, sel, per=(5 if FAST_MODE else 12))
    _click_by_text_candidates(driver, [
        "ë”ë³´ê¸°","ì „ì²´ë³´ê¸°","ìì„¸íˆ","ìƒì„¸ë³´ê¸°","ëª¨ë‘ ë³´ê¸°",
        "ì£¼ìš”ì—…ë¬´","ìê²©ìš”ê±´","ìš°ëŒ€ì‚¬í•­","ê¸°ì—…/íŒ€ ì†Œê°œ","í˜œíƒ ë° ë³µì§€",
        "ë‚˜ì¤‘ì— í•˜ê¸°","ë‹«ê¸°","í™•ì¸"
    ])

def _expand_saramin(driver):
    sel = [".btn_more",".btnMore",".btn-detail",".btn_toggle",
           "[aria-expanded='false']","[role='button']","button[class*='more'], a[class*='more']"]
    _click_many_css(driver, sel, per=(4 if FAST_MODE else 12))
    _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","ê¸°ì—…ì •ë³´","ìƒì„¸ì •ë³´"])

def _expand_jobkorea(driver):
    sel = [".btnFold",".btnToggleRead",".btn_more",
           "[aria-expanded='false']","[role='button']","button[class*='More'], a[class*='More']"]
    _click_many_css(driver, sel, per=(4 if FAST_MODE else 12))
    _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","ê¸°ì—…ì •ë³´","ìƒì„¸ë³´ê¸°"])

# -----------------------------------------------------------------------------
# Wanted __NEXT_DATA__ â†’ text
# -----------------------------------------------------------------------------
def extract_wanted_from_next_html(html: str) -> str:
    try:
        soup = BeautifulSoup(html or "", "html.parser")
        tag = soup.select_one("script#__NEXT_DATA__")
        if not tag: return ""
        raw = (tag.string or tag.text or "").strip()
        data = json.loads(raw)
    except Exception:
        return ""
    key_whitelist = [
        "job","position","title","desc","description",
        "responsibilit","duty","role","skill","stack",
        "require","qualification",
        "prefer","plus","nice",
        "benefit","welfare","perk"
    ]
    def _safe(x): return re.sub(r"\s+"," ", (x or "")).strip()
    def _walk(d, out):
        if isinstance(d, dict):
            for k, v in d.items():
                if any(t in str(k).lower() for t in key_whitelist):
                    if isinstance(v, str): out.append(v)
                    elif isinstance(v, list):
                        for it in v:
                            if isinstance(it, str): out.append(it)
                            elif isinstance(it, dict):
                                for _, subv in it.items():
                                    if isinstance(subv, str): out.append(subv)
                    elif isinstance(v, dict):
                        for _, subv in v.items():
                            if isinstance(subv, str): out.append(subv)
                _walk(v, out)
        elif isinstance(d, list):
            for it in d: _walk(it, out)
    bucket=[]; _walk(data, bucket)
    seen=set(); lines=[]
    for t in bucket:
        s=_safe(t)
        if len(s)>2 and s not in seen:
            seen.add(s); lines.append(s)
    return "\n".join(lines[:600])

# -----------------------------------------------------------------------------
# Selenium fetch (DOM + NEXT_DATA)
# -----------------------------------------------------------------------------
def selenium_get_html(url: str, timeout: int = 14) -> str:
    driver = _build_chrome(headless=True)
    try:
        driver.set_page_load_timeout(timeout)
        driver.get(url)
        try:
            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, "//*")))
        except TimeoutException:
            pass

        host = urllib.parse.urlsplit(url).netloc.lower()
        # light expand first
        _click_by_text_candidates(driver, ["ë”ë³´ê¸°","ìƒì„¸ë³´ê¸°","ìì„¸íˆ ë³´ê¸°","ì „ì²´ë³´ê¸°","Read more","More"], per=(3 if FAST_MODE else 8))
        _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","Requirements","Responsibilities","Preferred"], per=(3 if FAST_MODE else 8))

        if "wanted.co.kr" in host: _expand_wanted(driver)
        if "saramin" in host:     _expand_saramin(driver)
        if "jobkorea" in host:    _expand_jobkorea(driver)

        # scrolls
        loops = 3 if FAST_MODE else 7
        for _ in range(loops):
            try:
                driver.execute_script("window.scrollBy(0, 1200);"); time.sleep(0.15 if FAST_MODE else 0.25)
            except Exception:
                break

        html = driver.page_source or ""
        # Wanted extra
        if "wanted.co.kr" in host:
            try:
                txt_next = extract_wanted_from_next_html(html)
                if txt_next:
                    html += "\n<!-- TEXT_MERGE_SPLIT -->\n" + "\n".join([f"<p>{line}</p>" for line in txt_next.split("\n")])
            except Exception:
                pass
        return html
    finally:
        try: driver.quit()
        except Exception: pass

def fetch_all_text_selenium(url: str, timeout: int = 14) -> Tuple[str, Dict, Optional[str]]:
    url_n = normalize_url(url)
    if not url_n: return "", {"error":"invalid_url"}, None
    try:
        html = selenium_get_html(url_n, timeout=timeout)
    except Exception as e:
        st.error(f"Selenium ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.code("".join(traceback.format_exc()))
        return "", {"source":"selenium_error","len":0,"url_final":url_n}, None
    if not html or len(html) < 200:
        return "", {"source":"selenium_failed","len":0,"url_final":url_n}, None
    txt = html_to_text(html)
    return txt, {"source":"selenium","len":len(txt),"url_final":url_n}, html

# -----------------------------------------------------------------------------
# Meta & rule-based sections
# -----------------------------------------------------------------------------
def extract_company_meta_from_html(html: Optional[str]) -> Dict[str, str]:
    meta = {"company_name": "", "company_intro": "", "job_title": ""}
    if not html: return meta
    try:
        soup = BeautifulSoup(html, "html.parser")
        cand=[]
        og = soup.find("meta", {"property":"og:site_name"})
        if og and og.get("content"): cand.append(og["content"])
        app = soup.find("meta", {"name":"application-name"})
        if app and app.get("content"): cand.append(app["content"])
        if soup.title and soup.title.string: cand.append(soup.title.string)
        cand = [re.split(r"[\-\|\Â·\â€”]", c)[0].strip() for c in cand if c]
        cand = [c for c in cand if 2 <= len(c) <= 40]
        meta["company_name"] = cand[0] if cand else ""
        md = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
        if md and md.get("content"):
            meta["company_intro"] = re.sub(r"\s+"," ", md["content"]).strip()[:500]
        jt=""
        ogt = soup.find("meta", {"property":"og:title"})
        if ogt and ogt.get("content"): jt = ogt["content"]
        if not jt:
            h1 = soup.find("h1")
            if h1 and h1.get_text(): jt = h1.get_text(strip=True)
        if not jt:
            h2 = soup.find("h2")
            if h2 and h2.get_text(): jt = h2.get_text(strip=True)
        meta["job_title"] = re.sub(r"\s+"," ", jt).strip()[:120]
    except Exception:
        pass
    return meta

def rule_based_sections(raw_text: str) -> dict:
    txt = clean_text(raw_text, 16000)
    lines = [re.sub(r"\s+"," ", l).strip(" -â€¢Â·â–¶â–ªï¸") for l in txt.split("\n") if l.strip()]

    hdr_resp = re.compile(r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Role|Responsibilities?)", re.I)
    hdr_qual = re.compile(r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements?|Qualifications?)", re.I)
    hdr_pref = re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|ì„ í˜¸|Preferred|Nice\s*to\s*have|Plus)", re.I)
    out = {"responsibilities": [], "qualifications": [], "preferences": []}
    bucket=None

    def push(line,b):
        if line and len(line)>1 and line not in out[b]:
            out[b].append(line[:180])

    for l in lines:
        if hdr_resp.search(l): bucket="responsibilities"; continue
        if hdr_qual.search(l): bucket="qualifications"; continue
        if hdr_pref.search(l): bucket="preferences"; continue
        if bucket is None:
            if hdr_pref.search(l): bucket="preferences"
            elif any(k in l.lower() for k in ["java","python","spring","kafka","ml","sql"]):
                bucket="responsibilities"
            else: continue
        push(l,bucket)

    kw_pref = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
    remain=[]
    for q in out["qualifications"]:
        (out["preferences"] if kw_pref.search(q) else remain).append(q)
    out["qualifications"]=remain

    for k in out:
        seen=set(); clean=[]
        for s in out[k]:
            s=re.sub(r"\s+"," ", s).strip(" -â€¢Â·â–¶â–ªï¸").strip()
            if s and s not in seen:
                seen.add(s); clean.append(s)
        out[k]=clean[:14]
    return out

# -----------------------------------------------------------------------------
# LLM structure / Q&A / scoring
# -----------------------------------------------------------------------------
PROMPT_SYSTEM_STRUCT = ("ë„ˆëŠ” ì±„ìš© ê³µê³ ë¥¼ ê¹”ë”í•˜ê²Œ êµ¬ì¡°í™”í•˜ëŠ” ë³´ì¡°ì›ì´ë‹¤. "
                        "ì…ë ¥ í…ìŠ¤íŠ¸ëŠ” ê´‘ê³ /UXì”ì¬/ë³µìˆ˜ ì§ë¬´ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆë‹¤. "
                        "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì¤‘ë³µì—†ì´ ì •ì œí•˜ë¼.")

def llm_structurize(raw_text: str, meta_hint: Dict[str, str], model: str) -> Dict:
    ctx = clean_text(raw_text, 14000)
    user_msg = {"role": "user", "content": (
        "ë‹¤ìŒ ì±„ìš© ê³µê³  ì›ë¬¸ì„ êµ¬ì¡°í™”í•´ì¤˜.\n\n"
        f"[íŒíŠ¸] íšŒì‚¬ëª… í›„ë³´: {meta_hint.get('company_name','')}\n"
        f"[íŒíŠ¸] ì§ë¬´ëª… í›„ë³´: {meta_hint.get('job_title','')}\n"
        "--- ì›ë¬¸ ì‹œì‘ ---\n"
        f"{ctx}\n"
        "--- ì›ë¬¸ ë ---\n\n"
        "JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , í‚¤ëŠ” ë°˜ë“œì‹œ ì•„ë˜ë§Œ í¬í•¨:\n"
        "{"
        "\"company_name\": str, "
        "\"company_intro\": str, "
        "\"job_title\": str, "
        "\"responsibilities\": [str], "
        "\"qualifications\": [str], "
        "\"preferences\": [str]"
        "}\n"
        "- 'ìš°ëŒ€ ì‚¬í•­(preferences)'ì€ ì›ë¬¸ í‘œê¸°ê°€ ìˆëŠ” í•­ëª©ë§Œ ë‹´ë˜, ë¹„ì›Œë‘ì§€ ì•Šë„ë¡ ê·œì¹™ íŒŒì„œë¡œ ë³´ê°•.\n"
        "- ë¶ˆë¦¿/ì´ëª¨ì§€ ì œê±°, ë¬¸ì¥ ê°„ê²°í™”, ì¤‘ë³µ ì œê±°."
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2, max_tokens=900,
            response_format={"type": "json_object"},
            messages=[{"role":"system","content":PROMPT_SYSTEM_STRUCT}, user_msg]
        )
        data = json.loads(resp.choices[0].message.content)
    except Exception as e:
        data = {"company_name": meta_hint.get("company_name",""),
                "company_intro": meta_hint.get("company_intro","ì›ë¬¸ ì •ì œ ì‹¤íŒ¨"),
                "job_title": meta_hint.get("job_title",""),
                "responsibilities": [], "qualifications": [], "preferences": [], "error": str(e)}

    for k in ["responsibilities","qualifications","preferences"]:
        arr = data.get(k, []); 
        if not isinstance(arr, list): arr=[]
        clean_list=[]; seen=set()
        for it in arr:
            t = re.sub(r"\s+"," ", str(it)).strip(" -â€¢Â·â–¶â–ªï¸").strip()
            if t and t not in seen:
                seen.add(t); clean_list.append(t[:180])
        data[k] = clean_list[:14]

    if len(data.get("preferences", [])) < 1:
        rb = rule_based_sections(ctx)
        if rb.get("preferences"):
            merged = data.get("preferences", []) + rb["preferences"]
            data["preferences"] = list(dict.fromkeys(merged))[:14]
        else:
            kw_pref = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
            remain=[]; moved=[]
            for q in data.get("qualifications", []):
                (moved if kw_pref.search(q) else remain).append(q)
            data["preferences"]=moved[:14]; data["qualifications"]=remain[:14]

    for k in ["company_name","company_intro","job_title"]:
        if isinstance(data.get(k), str): data[k]=re.sub(r"\s+"," ", data[k]).strip()
    return data

def chunk(text: str, size: int = 600, overlap: int = 120) -> List[str]:
    t = re.sub(r"\s+"," ", text or "").strip()
    if not t: return []
    out, start = [], 0; L=len(t)
    while start < L:
        end = min(L, start+size); out.append(t[start:end])
        if end==L: break
        start = max(0, end-overlap)
    return out

def embed_texts(texts: List[str], model_name: str) -> np.ndarray:
    if not texts: return np.zeros((0,1536), dtype=np.float32)
    resp = client.embeddings.create(model=model_name, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

def cosine_topk(matrix: np.ndarray, query_vec: np.ndarray, k: int = 4):
    if matrix.size==0: return np.array([]), np.array([], dtype=int)
    qn = query_vec / (np.linalg.norm(query_vec, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T; sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_resume_chunks(query: str, chunks: List[str], embeds: np.ndarray, k: int = 4):
    if not chunks or embeds is None or embeds.size==0: return []
    qv = embed_texts([query], EMBED_MODEL)
    scores, idxs = cosine_topk(embeds, qv, k=k)
    return [(float(s), chunks[int(i)]) for s, i in zip(scores, idxs)]

PROMPT_SYSTEM_Q = ("ë„ˆëŠ” ì±„ìš©ë‹´ë‹¹ìë‹¤. íšŒì‚¬/ì§ë¬´ ë§¥ë½ê³¼ ì±„ìš©ìš”ê±´, ì§€ì›ì ì´ë ¥ì„œë¥¼ í•¨ê»˜ ê³ ë ¤í•´ "
                   "ì„œë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” ê³ í’ˆì§ˆ í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“ ë‹¤. ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ê·œëª¨/ë¦¬ìŠ¤í¬/íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì„ì–´ë¼.")
PROMPT_SYSTEM_DRAFT = ("ë„ˆëŠ” ë©´ì ‘ ë‹µë³€ ì½”ì¹˜ë‹¤. íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´ê³¼ ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½ì„ ê²°í•©í•´ "
                       "STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)ë¡œ 8~12ë¬¸ì¥ ë‹µë³€ **ì´ˆì•ˆ**ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤. "
                       "ê°€ëŠ¥í•˜ë©´ ì§€í‘œ/ìˆ˜ì¹˜/ê¸°ê°„/ì„íŒ©íŠ¸ë¥¼ í¬í•¨í•˜ë¼.")
PROMPT_SYSTEM_SCORE_STRICT = ("ë„ˆëŠ” ë§¤ìš° ì—„ê²©í•œ í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ë¼. "
                              "ê° ê¸°ì¤€ì€ 0~20 ì •ìˆ˜ì´ë©° ì´ì ì€ í•©ê³„(100)ì™€ ì¼ì¹˜í•´ì•¼ í•œë‹¤.")
CRITERIA = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]

def llm_generate_one_question_with_resume(clean: Dict, level: str, model: str,
                                          resume_chunks: List[str], resume_embeds: np.ndarray) -> str:
    hits = retrieve_resume_chunks("í•µì‹¬ í”„ë¡œì íŠ¸ì™€ ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½", resume_chunks, resume_embeds, k=4)
    resume_context = "\n".join([f"- {t[:350]}" for _, t in hits])[:1200]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´]\n{ctx}\n\n"
        f"[ì§€ì›ì ì´ë ¥ì„œ ìš”ì•½(ë°œì·Œ)]\n{resume_context}\n\n"
        f"[ìš”ì²­]\n- ë‚œì´ë„/ì—°ì°¨: {level}\n- ì¤‘ë³µ/ìœ ì‚¬ë„ ì§€ì–‘\n- í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë§Œ í•œ ì¤„ë¡œ ì¶œë ¥")}
    try:
        r = client.chat.completions.create(model=model, temperature=0.8, max_tokens=120,
                                           messages=[{"role":"system","content":PROMPT_SYSTEM_Q}, user_msg])
        q = r.choices[0].message.content.strip()
        q = re.sub(r"^\s*\d+[\).\s-]*","", q).split("\n")[0].strip()
        return q
    except Exception:
        return ""

def llm_draft_answer(clean: Dict, question: str, model: str,
                     resume_chunks: List[str], resume_embeds: np.ndarray) -> str:
    hits = retrieve_resume_chunks(question, resume_chunks, resume_embeds, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ STAR ê¸°ë°˜ í•œêµ­ì–´ ë‹µë³€ **ì´ˆì•ˆ**ì„ ì‘ì„±í•´ì¤˜.")}
    try:
        r = client.chat.completions.create(model=model, temperature=0.5, max_tokens=700,
                                           messages=[{"role":"system","content":PROMPT_SYSTEM_DRAFT}, user_msg])
        return r.choices[0].message.content.strip()
    except Exception:
        return ""

def llm_score_and_coach_strict(clean: Dict, question: str, answer: str, model: str,
                               resume_chunks: List[str], resume_embeds: np.ndarray) -> Dict:
    hits = retrieve_resume_chunks(question + "\n" + answer[:800], resume_chunks, resume_embeds, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
        f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n[ì§€ì›ì ë‹µë³€]\n{answer}\n\n"
        "ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ í•œêµ­ì–´ ì‘ë‹µ:\n"
        "{"
        "\"overall_score\": 0~100 ì •ìˆ˜,"
        "\"criteria\": [{\"name\":\"ë¬¸ì œì •ì˜\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ë°ì´í„°/ì§€í‘œ\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ì‹¤í–‰ë ¥/ì£¼ë„ì„±\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ê³ ê°ê°€ì¹˜\",\"score\":0~20,\"comment\":\"...\"}],"
        "\"strengths\": [\"...\"],\"risks\": [\"...\"],\"improvements\": [\"...\",\"...\",\"...\"],"
        "\"revised_answer\": \"STAR êµ¬ì¡°ë¡œ ê°„ê²°íˆ\""
        "}"
    )}
    try:
        r = client.chat.completions.create(model=model, temperature=0.2, max_tokens=900,
                                           response_format={"type":"json_object"},
                                           messages=[{"role":"system","content":PROMPT_SYSTEM_SCORE_STRICT}, user_msg])
        data = json.loads(r.choices[0].message.content)
        fixed=[]
        for name in CRITERIA:
            found=None
            for it in data.get("criteria", []):
                if str(it.get("name","")).strip()==name: found=it; break
            if not found: found={"name":name,"score":0,"comment":""}
            sc=int(found.get("score",0)); sc=max(0,min(20,sc))
            found["score"]=sc; found["comment"]=str(found.get("comment","")).strip()
            fixed.append(found)
        total=sum(x["score"] for x in fixed)
        data["criteria"]=fixed; data["overall_score"]=total
        for k in ["strengths","risks","improvements"]:
            arr=data.get(k,[]); 
            if not isinstance(arr,list): arr=[]
            data[k]=[str(x).strip() for x in arr if str(x).strip()][:5]
        data["revised_answer"]=str(data.get("revised_answer","")).strip()
        return data
    except Exception as e:
        return {"overall_score":0,"criteria":[{"name":n,"score":0,"comment":""} for n in CRITERIA],
                "strengths": [],"risks": [],"improvements": [],"revised_answer":"", "error":str(e)}

# -----------------------------------------------------------------------------
# Company pages / news (ìºì‹œ + ë™ì‹œ ì²˜ë¦¬)
# -----------------------------------------------------------------------------
@st.cache_data(show_spinner=False, ttl=3600)
def _http_get(url: str, timeout: int = 8) -> str:
    try:
        r = requests.get(url, timeout=timeout, headers={
            "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
            "Accept-Language":"ko, en;q=0.9"
        })
        if r.status_code==200: return r.text
    except Exception:
        pass
    return ""

@st.cache_data(show_spinner=False, ttl=3600)
def fetch_company_pages(home_url: str) -> Dict[str, List[str]]:
    out = {"vision": [], "talent": []}
    base = normalize_url(home_url or "")
    if not base: return out
    paths = ["","/","/about","/company","/about-us","/mission","/vision","/values","/culture","/careers","/talent","/people"]
    seen=set()
    for p in paths:
        url = (base.rstrip("/") + p) if p else base
        if url in seen: continue
        seen.add(url)
        html = _http_get(url, timeout=6)
        if not html: continue
        soup = BeautifulSoup(html, "lxml")
        texts=[]
        for tag in soup.find_all(["h1","h2","h3","h4","p","li"]):
            t = tag.get_text(" ", strip=True)
            if not t: continue
            t = re.sub(r"\s+"," ", t)
            if 6 <= len(t) <= 260: texts.append(t)
        for t in texts:
            low=t.lower()
            if any(k in low for k in ["talent","ì¸ì¬ìƒ","who we hire","people we"]):
                out["talent"].append(t)
            if any(k in low for k in ["ë¹„ì „","ë¯¸ì…˜","í•µì‹¬ê°€ì¹˜","ê°€ì¹˜","ì›ì¹™","mission","vision","values","principle"]):
                out["vision"].append(t)
    for k in out:
        uniq=[]; s=set()
        for x in out[k]:
            x=x.strip()
            if x and x not in s:
                s.add(x); uniq.append(x[:200])
        out[k]=uniq[:12]
    return out

@st.cache_data(show_spinner=False, ttl=1200)
def google_news_rss(company: str, max_items: int = 5) -> List[Dict]:
    if not company: return []
    q = urllib.parse.quote(company)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    try:
        r = requests.get(url, timeout=6)
        if r.status_code != 200: return []
        soup = BeautifulSoup(r.text, "xml")
        out=[]
        for it in soup.find_all("item")[:max_items]:
            out.append({"title": (it.title.get_text() if it.title else "").strip(),
                        "link": (it.link.get_text() if it.link else "").strip(),
                        "pubDate": (it.pubDate.get_text() if it.pubDate else "").strip()})
        return out
    except Exception:
        return []

# -----------------------------------------------------------------------------
# State
# -----------------------------------------------------------------------------
def _init_state():
    defaults = {
        "clean_struct": None,
        "resume_raw": "",
        "resume_chunks": [],
        "resume_embeds": None,
        "current_question": "",
        "answer_text": "",
        "last_result": None,
        "followups": [],
        "selected_followup": "",
        "followup_answer": "",
        "company_home": "",
        "company_vision": [],
        "company_talent": [],
        "company_news": [],
        "last_html": None,
    }
    for k,v in defaults.items():
        if k not in st.session_state: st.session_state[k]=v
_init_state()

# -----------------------------------------------------------------------------
# UI 1) ì±„ìš© ê³µê³  URL + (ì„ íƒ) ê¸°ì—… í™ˆí˜ì´ì§€ URL
# -----------------------------------------------------------------------------
st.header("1) ì±„ìš© ê³µê³  URL (Selenium ì „ìš©)")
job_url = st.text_input("ì±„ìš© ê³µê³  ìƒì„¸ URL", placeholder="ì›í‹°ë“œ/ì‚¬ëŒì¸/ì¡ì½”ë¦¬ì•„/ê¸°ì—… ì±„ìš© í˜ì´ì§€ URL")
st.text_input("íšŒì‚¬ ê³µì‹ í™ˆí˜ì´ì§€ URL (ì„ íƒ)", key="company_home", placeholder="íšŒì‚¬ ê³µì‹ í™ˆí˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”.")

if st.button("ì›ë¬¸ ìˆ˜ì§‘ â†’ ì •ì œ (Selenium ONLY)", type="primary"):
    if not job_url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("Seleniumìœ¼ë¡œ ì›ë¬¸ ìˆ˜ì§‘ ì¤‘..."):
            raw, meta, html = fetch_all_text_selenium(job_url.strip(), timeout=SELENIUM_TIMEOUT)
            hint = extract_company_meta_from_html(html)
            st.session_state.last_html = html

        st.caption(f"ìˆ˜ì§‘ ì†ŒìŠ¤: {meta.get('source')} Â· í…ìŠ¤íŠ¸ ê¸¸ì´: {meta.get('len')}")
        if not raw:
            st.error("ìˆ˜ì§‘ ì‹¤íŒ¨(ë¡œê·¸ì¸/ë™ì  ë Œë”ë§/ë´‡ ì°¨ë‹¨ ê°€ëŠ¥).")
        else:
            # ë™ì‹œ ì²˜ë¦¬: LLM ì •ì œ + (í™ˆí˜ì´ì§€/ë‰´ìŠ¤)
            with st.spinner("ì •ì œ ë° ë¶€ê°€ì •ë³´ ìˆ˜ì§‘ ì¤‘..."):
                tasks=[]
                with ThreadPoolExecutor(max_workers=3) as ex:
                    tasks.append(("clean", ex.submit(llm_structurize, raw, hint, CHAT_MODEL)))
                    if st.session_state.company_home.strip():
                        tasks.append(("pages", ex.submit(fetch_company_pages, st.session_state.company_home.strip())))
                    cname = hint.get("company_name","")
                    tasks.append(("news", ex.submit(google_news_rss, cname, 5)))

                    clean=None; vis=[]; tal=[]; news=[]
                    for name, fut in tasks:
                        try:
                            res = fut.result()
                            if name=="clean": clean=res
                            elif name=="pages":
                                vis = res.get("vision", []); tal = res.get("talent", [])
                            elif name=="news": news = res or []
                        except Exception:
                            continue
                # ê·œì¹™ íŒŒì„œ ë³´ê°•
                if clean and not clean.get("preferences"):
                    rb = rule_based_sections(raw)
                    if rb.get("preferences"): clean["preferences"]=rb["preferences"]
                st.session_state.clean_struct = clean
                st.session_state.company_vision = vis
                st.session_state.company_talent = tal
                st.session_state.company_news = news
            st.success("ì •ì œ ì™„ë£Œ!")

# -----------------------------------------------------------------------------
# UI 2) íšŒì‚¬ ìš”ì•½
# -----------------------------------------------------------------------------
st.header("2) íšŒì‚¬ ìš”ì•½")
clean = st.session_state.clean_struct
if clean:
    st.markdown(f"**íšŒì‚¬ëª…:** {clean.get('company_name','-')}")
    st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½):** {clean.get('company_intro','-')}")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼(ì§ë¬´ëª…):** {clean.get('job_title','-')}")
    c1,c2,c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´**");   [st.markdown(f"- {b}") for b in clean.get("responsibilities", [])]
    with c2:
        st.markdown("**ìê²© ìš”ê±´**");   [st.markdown(f"- {b}") for b in clean.get("qualifications", [])]
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­**")
        prefs = clean.get("preferences", [])
        if prefs: [st.markdown(f"- {b}") for b in prefs]
        else: st.caption("ìš°ëŒ€ ì‚¬í•­ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
else:
    st.info("ë¨¼ì € URLì„ ì •ì œí•´ ì£¼ì„¸ìš”.")

# ë¹„ì „/ì¸ì¬ìƒ/ë‰´ìŠ¤
if st.session_state.company_vision or st.session_state.company_talent or st.session_state.company_news:
    st.divider()
    st.subheader("íšŒì‚¬ ë¹„ì „/ì¸ì¬ìƒ & ìµœì‹  ì´ìŠˆ")
    vcol, tcol = st.columns(2)
    with vcol:
        st.markdown("**ë¹„ì „/í•µì‹¬ê°€ì¹˜**")
        for v in st.session_state.company_vision[:8]: st.markdown(f"- {v}")
        if not st.session_state.company_vision: st.caption("ë¹„ì „/í•µì‹¬ê°€ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    with tcol:
        st.markdown("**ì¸ì¬ìƒ**")
        for t in st.session_state.company_talent[:8]: st.markdown(f"- {t}")
        if not st.session_state.company_talent: st.caption("ì¸ì¬ìƒ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    if st.session_state.company_news:
        st.markdown("**ìµœì‹  ë‰´ìŠ¤(ìƒìœ„ 3~5)**")
        for n in st.session_state.company_news[:5]:
            st.markdown(f"- [{n.get('title','(ì œëª© ì—†ìŒ)')}]({n.get('link','#')})")

st.divider()

# -----------------------------------------------------------------------------
# UI 3) ì´ë ¥ì„œ ì—…ë¡œë“œ/ì¸ë±ì‹±
# -----------------------------------------------------------------------------
st.header("3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ")
uploads = st.file_uploader("ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ ê°€ëŠ¥", type=["pdf","txt","md","docx"], accept_multiple_files=True)
_RESUME_CHUNK=500; _RESUME_OVLP=100

def read_pdf(data: bytes) -> str:
    if pypdf is None: return ""
    try:
        reader = pypdf.PdfReader(io.BytesIO(data))
        return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
    except Exception: return ""

def read_docx_file(data: bytes) -> str:
    if docx2txt is None: return ""
    try:
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=True) as tmp:
            tmp.write(data); tmp.flush()
            return docx2txt.process(tmp.name) or ""
    except Exception: return ""

def read_file_text(uploaded) -> str:
    name = uploaded.name.lower(); data = uploaded.read()
    if name.endswith((".txt",".md")):
        for enc in ("utf-8","cp949","euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    if name.endswith(".pdf"):  return read_pdf(data)
    if name.endswith(".docx"): return read_docx_file(data)
    return ""

if st.button("ì´ë ¥ì„œ ì¸ë±ì‹±", type="secondary"):
    if not uploads: st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        all_text=[]
        for up in uploads:
            t = read_file_text(up)
            if t: all_text.append(t)
        resume_text = "\n\n".join(all_text)
        if not resume_text.strip(): st.error("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
        else:
            chunks = chunk(resume_text, size=_RESUME_CHUNK, overlap=_RESUME_OVLP)
            with st.spinner("ì´ë ¥ì„œ ë²¡í„°í™” ì¤‘..."):
                embeds = embed_texts(chunks, EMBED_MODEL)
            st.session_state.resume_raw = resume_text
            st.session_state.resume_chunks = chunks
            st.session_state.resume_embeds = embeds
            st.success(f"ì¸ë±ì‹± ì™„ë£Œ (ì²­í¬ {len(chunks)}ê°œ)")

st.divider()

# -----------------------------------------------------------------------------
# UI 4) ìì†Œì„œ ìƒì„±
# -----------------------------------------------------------------------------
st.header("4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„±")
topic = st.text_input("íšŒì‚¬ ìš”ì²­ ì£¼ì œ(ì„ íƒ)", placeholder="ì˜ˆ: ì§€ì›ë™ê¸° / í˜‘ì—… ê²½í—˜ / ë¬¸ì œí•´ê²° ì‚¬ë¡€ ë“±")

def build_cover_letter(clean_struct: Dict, resume_text: str, topic_hint: str, model: str) -> str:
    enrich = {"vision": st.session_state.company_vision[:6],
              "talent": st.session_state.company_talent[:6],
              "news": [n.get("title","") for n in st.session_state.company_news[:3]]}
    company = json.dumps({"clean":clean_struct, "extra":enrich}, ensure_ascii=False)
    resume_snip = resume_text.strip()[:9000]
    system = ("ë„ˆëŠ” í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì „ë¬¸ê°€ë‹¤. íšŒì‚¬/ì§ë¬´ ìš”ê±´ê³¼ í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ì°¸ê³ í•´ "
              "íšŒì‚¬ íŠ¹í™” ìì†Œì„œë¥¼ ì‘ì„±í•œë‹¤. ê³¼ì¥/í—ˆìœ„ ê¸ˆì§€, ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ì„íŒ©íŠ¸ ì¤‘ì‹¬.")
    req = (f"íšŒì‚¬ ì¸¡ ìš”ì²­ ì£¼ì œëŠ” '{topic_hint.strip()}' ì´ë‹¤. ì´ ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ ." 
           if topic_hint and topic_hint.strip()
           else "íŠ¹ì • ì£¼ì œê°€ ì—†ìœ¼ë¯€ë¡œ ì±„ìš©ìš”ê±´, ë¹„ì „/ì¸ì¬ìƒê³¼ì˜ ì •í•©ì„±ì„ ê°•ì¡°.")
    user = (f"[íšŒì‚¬/ì§ë¬´ ìš”ì•½(JSON)]\n{company}\n\n[í›„ë³´ì ì´ë ¥ì„œ]\n{resume_snip}\n\n"
            f"[ì‘ì„± ì§€ì‹œ]\n- {req}\n- ë¶„ëŸ‰ 600~900ì\n"
            "- êµ¬ì„±: ì§€ì›ë™ê¸°â†’ì—­ëŸ‰/ê²½í—˜â†’ì„±ê³¼/ì§€í‘œâ†’ì…ì‚¬ í›„ ê¸°ì—¬â†’ë§ˆë¬´ë¦¬\n"
            "- ì¤‘ë³µ/ë¯¸ì‚¬ì—¬êµ¬ ì œê±°, ìì—°ìŠ¤ëŸ¬ìš´ 1ì¸ì¹­.")
    try:
        r = client.chat.completions.create(model=model, temperature=0.4, max_tokens=800,
                                           messages=[{"role":"system","content":system},{"role":"user","content":user}])
        return r.choices[0].message.content.strip()
    except Exception as e:
        return f"(ìì†Œì„œ ìƒì„± ì‹¤íŒ¨: {e})"

if st.button("ìì†Œì„œ ìƒì„±", type="primary"):
    if not st.session_state.clean_struct:
        st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
    elif not st.session_state.resume_raw.strip():
        st.warning("ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ì´ë ¥ì„œ ì¸ë±ì‹±'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ìì†Œì„œ ìƒì„± ì¤‘..."):
            cover = build_cover_letter(st.session_state.clean_struct, st.session_state.resume_raw, topic, CHAT_MODEL)
        st.subheader("ìì†Œì„œ (ìƒì„± ê²°ê³¼)"); st.write(cover)
        st.download_button("ìì†Œì„œ TXT ë‹¤ìš´ë¡œë“œ", data=cover.encode("utf-8"),
                           file_name="cover_letter.txt", mime="text/plain")

st.divider()

# -----------------------------------------------------------------------------
# UI 5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ
# -----------------------------------------------------------------------------
st.header("5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ (RAG ê²°í•©)")
level = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"], index=0)

c1, c2 = st.columns(2)
with c1:
    if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", type="primary"):
        if not st.session_state.clean_struct:
            st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
        else:
            q = llm_generate_one_question_with_resume(
                st.session_state.clean_struct, level, CHAT_MODEL,
                st.session_state.resume_chunks, st.session_state.resume_embeds
            )
            if q:
                st.session_state.current_question = q
                st.session_state.answer_text = ""
                st.session_state.last_result = None
                st.session_state.followups = []
                st.session_state.selected_followup = ""
                st.session_state.followup_answer = ""
                st.success("ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
with c2:
    if st.button("RAGë¡œ ë‹µë³€ ì´ˆì•ˆ ìƒì„±", type="secondary"):
        if not st.session_state.current_question:
            st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        else:
            draft = llm_draft_answer(
                st.session_state.clean_struct, st.session_state.current_question, CHAT_MODEL,
                st.session_state.resume_chunks, st.session_state.resume_embeds
            )
            if draft:
                st.session_state.answer_text = draft
                st.success("ì´ˆì•ˆ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨")

st.text_area("ì§ˆë¬¸", value=st.session_state.current_question, height=100)
st.text_area("ë‚˜ì˜ ë‹µë³€ (ì´ˆì•ˆì„ í¸ì§‘í•´ ì™„ì„±)", key="answer_text", height=200)

# -----------------------------------------------------------------------------
# UI 6) ì±„ì  & ì½”ì¹­
# -----------------------------------------------------------------------------
st.header("6) ì±„ì  & ì½”ì¹­")
if st.button("ì±„ì  & ì½”ì¹­ ì‹¤í–‰", type="primary"):
    if not st.session_state.current_question:
        st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì±„ì /ì½”ì¹­ ì¤‘..."):
            res = llm_score_and_coach_strict(
                st.session_state.clean_struct,
                st.session_state.current_question,
                st.session_state.answer_text,
                CHAT_MODEL,
                st.session_state.resume_chunks,
                st.session_state.resume_embeds
            )
        st.session_state.last_result = res
        st.success("ì±„ì /ì½”ì¹­ ì™„ë£Œ!")

st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")
last = st.session_state.last_result
if last:
    st.metric("ì´ì (/100)", last.get("overall_score", 0))
    st.markdown("**ê¸°ì¤€ë³„ ì ìˆ˜ & ì½”ë©˜íŠ¸**")
    for it in last.get("criteria", []):
        st.markdown(f"- **{it['name']}**: {it['score']}/20 â€” {it.get('comment','')}")
    if last.get("strengths"):
        st.markdown("**ê°•ì **"); [st.markdown(f"- {s}") for s in last["strengths"]]
    if last.get("risks"):
        st.markdown("**ê°ì  ìš”ì¸/ë¦¬ìŠ¤í¬**"); [st.markdown(f"- {r}") for r in last["risks"]]
    if last.get("improvements"):
        st.markdown("**ê°œì„  í¬ì¸íŠ¸**"); [st.markdown(f"- {im}") for im in last["improvements"]]
    if last.get("revised_answer"):
        st.markdown("**ìˆ˜ì •ë³¸ ë‹µë³€(STAR)**"); st.write(last["revised_answer"])
else:
    st.info("ì•„ì§ ì±„ì  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()

# -----------------------------------------------------------------------------
# UI 7) íŒ”ë¡œì—… ì§ˆë¬¸ Â· ë‹µë³€ Â· í”¼ë“œë°± (ë³µì›)
# -----------------------------------------------------------------------------
st.subheader("íŒ”ë¡œì—… ì§ˆë¬¸ Â· ë‹µë³€ Â· í”¼ë“œë°±")

# ë©”ì¸ í”¼ë“œë°± ì§í›„ ìë™ ì œì•ˆ
if last and not st.session_state.followups:
    try:
        ctx = json.dumps({"company": st.session_state.clean_struct,
                          "vision": st.session_state.company_vision[:6],
                          "talent": st.session_state.company_talent[:6],
                          "news": [n.get("title","") for n in st.session_state.company_news[:3]]},
                         ensure_ascii=False)
        msg = {"role":"user","content":(
            f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´/ë¹„ì „/ì´ìŠˆ]\n{ctx}\n\n[ì§€ì›ì ë‹µë³€]\n{st.session_state.answer_text}\n\n"
            "ë©´ì ‘ê´€ ê´€ì ì—ì„œ íŒ”ë¡œì—… ì§ˆë¬¸ 3ê°œë¥¼ í•œ ì¤„ì”© í•œêµ­ì–´ë¡œ ì œì•ˆí•´ì¤˜. "
            "ê¸°ì¡´ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê²Œ, ì§€í‘œ/ë¦¬ìŠ¤í¬/íŠ¸ë ˆì´ë“œì˜¤í”„/ì˜ì‚¬ê²°ì • ê·¼ê±°ë¥¼ ì„ì–´ì¤˜.")}
        r = client.chat.completions.create(model=CHAT_MODEL, temperature=0.7,
                                           messages=[{"role":"system","content":"ë©´ì ‘ íŒ”ë¡œì—… ìƒì„±ê¸°"}, msg])
        followups = [re.sub(r'^\s*\d+[\).\s-]*','', l).strip()
                     for l in r.choices[0].message.content.splitlines() if l.strip()]
        st.session_state.followups = followups[:3]
    except Exception:
        st.session_state.followups = []

if last:
    if st.session_state.followups:
        st.markdown("**íŒ”ë¡œì—… ì§ˆë¬¸ ì œì•ˆ**")
        for i, f in enumerate(st.session_state.followups, 1): st.markdown(f"- ({i}) {f}")
        st.selectbox("ì±„ì  ë°›ì„ íŒ”ë¡œì—… ì§ˆë¬¸ ì„ íƒ", st.session_state.followups, index=0, key="selected_followup")
        st.text_area("íŒ”ë¡œì—… ì§ˆë¬¸ì— ëŒ€í•œ ë‚˜ì˜ ë‹µë³€", key="followup_answer", height=160)
        if st.button("íŒ”ë¡œì—… ì±„ì  & í”¼ë“œë°±", type="secondary"):
            fu_q = st.session_state.get("selected_followup",""); fu_ans = st.session_state.get("followup_answer","")
            if not fu_q: st.warning("íŒ”ë¡œì—… ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.")
            elif not fu_ans.strip(): st.warning("íŒ”ë¡œì—… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.")
            else:
                with st.spinner("íŒ”ë¡œì—… ì±„ì  ì¤‘..."):
                    res_fu = llm_score_and_coach_strict(
                        st.session_state.clean_struct, fu_q, fu_ans, CHAT_MODEL,
                        st.session_state.resume_chunks, st.session_state.resume_embeds
                    )
                st.markdown("**íŒ”ë¡œì—… ê²°ê³¼**")
                st.metric("ì´ì (/100)", res_fu.get("overall_score", 0))
                for it in res_fu.get("criteria", []):
                    st.markdown(f"- **{it['name']}**: {it['score']}/20 â€” {it.get('comment','')}")
                if res_fu.get("revised_answer",""):
                    st.markdown("**íŒ”ë¡œì—… ìˆ˜ì •ë³¸(STAR)**"); st.write(res_fu["revised_answer"])
    else:
        st.caption("íŒ”ë¡œì—… ì§ˆë¬¸ì€ ë©”ì¸ ì§ˆë¬¸ ì±„ì  ì§í›„ ìë™ ì œì•ˆë©ë‹ˆë‹¤.")
