# -*- coding: utf-8 -*-
################################################################################
# Job Helper Bot (Selenium-ONLY, Selenium Manager, no-lxml, Wanted fixed)
# - ìì†Œì„œ/ì§ˆë¬¸/ì±„ì 
# - Seleniumë§Œ ì‚¬ìš© (í´ë°± ì—†ìŒ)
# - lxml ë¯¸ì‚¬ìš© (BeautifulSoup: html.parser)
# - ì›í‹°ë“œ(Next.js) __NEXT_DATA__ë¥¼ HTMLì—ì„œ ì§ì ‘ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ ë³‘í•©
################################################################################

import os, re, json, urllib.parse, time, io, tempfile, shutil, traceback
from typing import Optional, Tuple, Dict, List

import streamlit as st
import numpy as np
import pandas as pd
import html2text
from bs4 import BeautifulSoup

# OpenAI (>=1.43)
try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

# Selenium (ë“œë¼ì´ë²„ëŠ” Selenium Managerê°€ ìë™ ë‹¤ìš´ë¡œë“œ)
from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# (ì„ íƒ) ë¬¸ì„œ íŒŒì„œ
try:
    import pypdf
except Exception:
    pypdf = None
try:
    import docx2txt
except Exception:
    docx2txt = None

# -----------------------------------------------------------------------------
# Streamlit ê¸°ë³¸
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Job Helper Bot (Selenium-ONLY)", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” Job Helper Bot (Selenium-ONLY) : ìì†Œì„œ ìƒì„± / ëª¨ì˜ ë©´ì ‘")

# -----------------------------------------------------------------------------
# OpenAI í‚¤
# -----------------------------------------------------------------------------
API_KEY = os.getenv("OPENAI_API_KEY") or (st.secrets.get("OPENAI_API_KEY", None) if hasattr(st, "secrets") else None)
if not API_KEY:
    API_KEY = st.text_input("OPENAI_API_KEY ì…ë ¥", type="password", help="OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
if not API_KEY:
    st.stop()
client = OpenAI(api_key=API_KEY)

# -----------------------------------------------------------------------------
# ì‚¬ì´ë“œë°”
# -----------------------------------------------------------------------------
with st.sidebar:
    st.subheader("ëª¨ë¸ & ì˜µì…˜")
    CHAT_MODEL = st.selectbox("ëŒ€í™”/ìƒì„± ëª¨ë¸", ["gpt-4o-mini", "gpt-4o"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small", "text-embedding-3-large"], index=0)
    SELENIUM_TIMEOUT = st.slider("Selenium ëŒ€ê¸°(ì´ˆ)", 6, 30, 14)

# -----------------------------------------------------------------------------
# html2text
# -----------------------------------------------------------------------------
def _get_html2text():
    conv = html2text.HTML2Text()
    conv.ignore_links = True
    conv.ignore_images = True
    conv.body_width = 0
    return conv
HTML2TEXT = _get_html2text()

# -----------------------------------------------------------------------------
# ìœ í‹¸
# -----------------------------------------------------------------------------
def normalize_url(u: str) -> Optional[str]:
    if not u: return None
    u = u.strip()
    if not re.match(r"^https?://", u): u = "https://" + u
    parts = urllib.parse.urlsplit(u)
    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))

def clean_text(s: str, max_len: int = 16000) -> str:
    if not s: return ""
    s = re.sub(r"\r", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s[:max_len] if len(s) > max_len else s

def html_to_text(html_str: str) -> str:
    txt = HTML2TEXT.handle(html_str or "")
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return clean_text(txt)

# -----------------------------------------------------------------------------
# Selenium ë¹Œë”
# -----------------------------------------------------------------------------
def _pick_chrome_binary() -> Optional[str]:
    cands = [
        os.getenv("CHROME_BIN"),
        os.getenv("GOOGLE_CHROME_BIN"),
        shutil.which("chromium"),
        shutil.which("chromium-browser"),
        shutil.which("google-chrome"),
        "/usr/bin/chromium",
        "/usr/bin/chromium-browser",
        "/usr/bin/google-chrome",
        "/usr/bin/google-chrome-stable",
    ]
    for p in cands:
        if p and os.path.exists(p):
            return p
    return None

def _build_chrome(headless: bool = True):
    opts = ChromeOptions()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1440,2400")
    opts.add_argument("--lang=ko-KR")
    # ë°”ì´ë„ˆë¦¬ ê²½ë¡œ ì§€ì •(ìˆìœ¼ë©´)
    binpath = _pick_chrome_binary()
    if binpath:
        opts.binary_location = binpath
    # Selenium Manager ì‚¬ìš© (ë“œë¼ì´ë²„ ìë™ê´€ë¦¬)
    driver = webdriver.Chrome(options=opts)
    return driver

# -----------------------------------------------------------------------------
# ê³µí†µ í´ë¦­ ìœ í‹¸
# -----------------------------------------------------------------------------
def _click_by_text_candidates(driver, texts: List[str]):
    for t in texts:
        try:
            xpath_exact = f"//*[normalize-space(text())='{t}']"
            xpath_contains = f"//*[contains(normalize-space(text()), '{t}')]"
            for xp in (xpath_exact, xpath_contains):
                els = driver.find_elements(By.XPATH, xp)
                for el in els[:12]:
                    try:
                        driver.execute_script("arguments[0].click();", el)
                        time.sleep(0.25)
                    except Exception:
                        continue
        except Exception:
            continue

def _click_many(driver, css_list, limit_per_selector=12):
    for css in css_list:
        try:
            els = driver.find_elements(By.CSS_SELECTOR, css)
            for el in els[:limit_per_selector]:
                try:
                    driver.execute_script("arguments[0].click();", el)
                    time.sleep(0.2)
                except Exception:
                    continue
        except Exception:
            continue

# -----------------------------------------------------------------------------
# ë„ë©”ì¸ë³„ í™•ì¥ í´ë¦­ (Wanted ê°•í™”)
# -----------------------------------------------------------------------------
def _expand_wanted(driver):
    selectors = [
        "[data-qa='btn-read-more']",
        "[data-qa='job-header__more']",
        "button[aria-expanded='false']",
        "[role='button'][class*='More']",
        "div[aria-expanded='false']",
    ]
    _click_many(driver, selectors, limit_per_selector=12)
    _click_by_text_candidates(driver, [
        "ë”ë³´ê¸°","ì „ì²´ë³´ê¸°","ìì„¸íˆ","ìƒì„¸ë³´ê¸°","ëª¨ë‘ ë³´ê¸°",
        "ì£¼ìš”ì—…ë¬´","ìê²©ìš”ê±´","ìš°ëŒ€ì‚¬í•­","ê¸°ì—…/íŒ€ ì†Œê°œ","í˜œíƒ ë° ë³µì§€",
        "ë‚˜ì¤‘ì— í•˜ê¸°","ë‹«ê¸°","í™•ì¸"
    ])
    for _ in range(10):
        try:
            driver.execute_script("window.scrollBy(0, 1200);"); time.sleep(0.2)
        except Exception:
            break

def _expand_saramin(driver):
    selectors = [
        ".btn_more", ".btnMore", ".btn-detail", ".btn_toggle",
        "[aria-expanded='false']", "[role='button']",
        "button[class*='more'], a[class*='more']"
    ]
    _click_many(driver, selectors)
    _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","ê¸°ì—…ì •ë³´","ìƒì„¸ì •ë³´"])

def _expand_jobkorea(driver):
    selectors = [
        ".btnFold", ".btnToggleRead", ".btn_more",
        "[aria-expanded='false']", "[role='button']",
        "button[class*='More'], a[class*='More']"
    ]
    _click_many(driver, selectors)
    _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","ê¸°ì—…ì •ë³´","ìƒì„¸ë³´ê¸°"])

# -----------------------------------------------------------------------------
# Wanted: __NEXT_DATA__ë¥¼ HTMLì—ì„œ ì§ì ‘ íŒŒì‹± â†’ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
# -----------------------------------------------------------------------------
def extract_wanted_from_next_html(html: str) -> str:
    try:
        soup = BeautifulSoup(html or "", "html.parser")
        tag = soup.select_one("script#__NEXT_DATA__")
        if not tag:
            return ""
        raw = (tag.string or tag.text or "").strip()
        data = json.loads(raw)
    except Exception:
        return ""

    key_whitelist = [
        "job","position","title","desc","description",
        "responsibilit","duty","role",
        "require","qualification",
        "prefer","plus","nice",
        "benefit","welfare","perk"
    ]

    def _safe_text(x): return re.sub(r"\s+", " ", (x or "")).strip()

    def _walk(d, out):
        if isinstance(d, dict):
            for k, v in d.items():
                kl = str(k).lower()
                if any(t in kl for t in key_whitelist):
                    if isinstance(v, str):
                        out.append(v)
                    elif isinstance(v, list):
                        for it in v:
                            if isinstance(it, str): out.append(it)
                            elif isinstance(it, dict):
                                for _, subv in it.items():
                                    if isinstance(subv, str): out.append(subv)
                    elif isinstance(v, dict):
                        for _, subv in v.items():
                            if isinstance(subv, str): out.append(subv)
                _walk(v, out)
        elif isinstance(d, list):
            for it in d:
                _walk(it, out)

    bucket = []
    _walk(data, bucket)

    lines, seen = [], set()
    for t in bucket:
        s = _safe_text(t)
        if len(s) > 2 and s not in seen:
            seen.add(s); lines.append(s)
    return "\n".join(lines[:400])

# -----------------------------------------------------------------------------
# Selenium ì „ìš© í˜ì´ì§€ ìˆ˜ì§‘
# -----------------------------------------------------------------------------
def selenium_only_get_html(url: str, timeout: int = 14) -> str:
    driver = _build_chrome(headless=True)
    try:
        driver.set_page_load_timeout(timeout)
        driver.get(url)
        try:
            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, "//*")))
        except TimeoutException:
            pass

        host = urllib.parse.urlsplit(url).netloc.lower()

        # ê³µí†µ í™•ì¥ ì‹œë„
        _click_by_text_candidates(driver, ["ë”ë³´ê¸°","ìƒì„¸ë³´ê¸°","ìì„¸íˆ ë³´ê¸°","ìì„¸íˆ","ì „ì²´ë³´ê¸°","í¼ì¹˜ê¸°","ëª¨ë‘ ë³´ê¸°","Read more","More"])
        _click_by_text_candidates(driver, ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","ìê²©ìš”ê±´","ì£¼ìš”ì—…ë¬´","Requirements","Responsibilities","Preferred"])

        # ë„ë©”ì¸ ì „ìš© í™•ì¥
        if "wanted.co.kr" in host:
            _expand_wanted(driver)
        if "saramin.co.kr" in host or "saramin" in host:
            _expand_saramin(driver)
        if "jobkorea.co.kr" in host:
            _expand_jobkorea(driver)

        # ì§€ì—°ë¡œë”© ëŒ€ë¹„ ìŠ¤í¬ë¡¤
        for _ in range(6):
            try:
                driver.execute_script("window.scrollBy(0, 1000);"); time.sleep(0.2)
            except Exception:
                break

        html = driver.page_source or ""
        return html
    finally:
        try: driver.quit()
        except Exception: pass

def fetch_all_text_selenium_only(url: str, timeout: int = 14) -> Tuple[str, Dict, Optional[str]]:
    url_n = normalize_url(url)
    if not url_n:
        return "", {"error": "invalid_url"}, None
    try:
        html_dyn = selenium_only_get_html(url_n, timeout=timeout)
    except Exception as e:
        st.error(f"Selenium ë“œë¼ì´ë²„ ì‹œì‘/ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.code("".join(traceback.format_exc()))
        return "", {"source": "selenium_error", "len": 0, "url_final": url_n}, None

    if not html_dyn or len(html_dyn) < 200:
        return "", {"source": "selenium_failed", "len": 0, "url_final": url_n}, None

    # 1) DOM â†’ í…ìŠ¤íŠ¸
    txt_dom = html_to_text(html_dyn)

    # 2) Wantedë©´ __NEXT_DATA__ë„ íŒŒì‹±í•´ì„œ í…ìŠ¤íŠ¸ì— ì§ì ‘ ë§ë¶™ì„
    host = urllib.parse.urlsplit(url_n).netloc.lower()
    txt_next = ""
    if "wanted.co.kr" in host:
        try:
            txt_next = extract_wanted_from_next_html(html_dyn)
        except Exception:
            txt_next = ""

    # 3) ìµœì¢… ë³‘í•©
    final_txt = (txt_dom + ("\n\n" + txt_next if txt_next else "")).strip()
    return final_txt, {"source": "selenium", "len": len(final_txt), "url_final": url_n}, html_dyn

# -----------------------------------------------------------------------------
# ë©”íƒ€/ì •ì œ/ê·œì¹™ ê¸°ë°˜
# -----------------------------------------------------------------------------
def extract_company_meta(soup_html: Optional[str]) -> Dict[str, str]:
    meta = {"company_name": "", "company_intro": "", "job_title": ""}
    if not soup_html: return meta
    try:
        soup = BeautifulSoup(soup_html, "html.parser")   # lxml ë¯¸ì‚¬ìš©
        cand = []
        og = soup.find("meta", {"property": "og:site_name"})
        if og and og.get("content"): cand.append(og["content"])
        app = soup.find("meta", {"name": "application-name"})
        if app and app.get("content"): cand.append(app["content"])
        if soup.title and soup.title.string: cand.append(soup.title.string)
        cand = [re.split(r"[\-\|\Â·\â€”]", c)[0].strip() for c in cand if c]
        cand = [c for c in cand if 2 <= len(c) <= 40]
        meta["company_name"] = cand[0] if cand else ""
        md = soup.find("meta", {"name": "description"}) or soup.find("meta", {"property": "og:description"})
        if md and md.get("content"):
            meta["company_intro"] = re.sub(r"\s+", " ", md["content"]).strip()[:500]
        jt = ""
        ogt = soup.find("meta", {"property": "og:title"})
        if ogt and ogt.get("content"): jt = ogt["content"]
        if not jt:
            h1 = soup.find("h1")
            if h1 and h1.get_text(): jt = h1.get_text(strip=True)
        if not jt:
            h2 = soup.find("h2")
            if h2 and h2.get_text(): jt = h2.get_text(strip=True)
        meta["job_title"] = re.sub(r"\s+", " ", jt).strip()[:120]
    except Exception:
        pass
    return meta

def clean_bullets(arr):
    clean = []; seen = set()
    for it in arr:
        t = re.sub(r"\s+", " ", str(it)).strip(" -â€¢Â·â–¶â–ªï¸").strip()
        if t and t not in seen:
            seen.add(t); clean.append(t[:180])
    return clean[:12]

def rule_based_sections(raw_text: str) -> dict:
    txt = clean_text(raw_text, 16000)
    lines = [re.sub(r"\s+", " ", l).strip(" -â€¢Â·â–¶â–ªï¸") for l in txt.split("\n") if l.strip()]
    hdr_resp = re.compile(r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Role|Responsibilities?)", re.I)
    hdr_qual = re.compile(r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements?|Qualifications?)", re.I)
    hdr_pref = re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|ì„ í˜¸|Preferred|Nice\s*to\s*have|Plus)", re.I)
    bucket = None
    out = {"responsibilities": [], "qualifications": [], "preferences": []}

    def push(line, b):
        if line and len(line) > 1 and line not in out[b]:
            out[b].append(line[:180])

    for l in lines:
        if hdr_resp.search(l): bucket = "responsibilities"; continue
        if hdr_qual.search(l): bucket = "qualifications"; continue
        if hdr_pref.search(l): bucket = "preferences"; continue
        if bucket is None:
            if hdr_pref.search(l): bucket = "preferences"
            elif any(k in l.lower() for k in ["java","python","spark","airflow","kafka","ml","sql"]):
                bucket = "responsibilities"
            else:
                continue
        push(l, bucket)

    kw_pref = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
    remain_qual = []
    for q in out["qualifications"]:
        (out["preferences"] if kw_pref.search(q) else remain_qual).append(q)
    out["qualifications"] = remain_qual

    for k in out:
        out[k] = list(dict.fromkeys([re.sub(r"\s+"," ", s).strip(" -â€¢Â·â–¶â–ªï¸").strip() for s in out[k]]))[:12]
    return out

PROMPT_SYSTEM_STRUCT = (
    "ë„ˆëŠ” ì±„ìš© ê³µê³ ë¥¼ ê¹”ë”í•˜ê²Œ êµ¬ì¡°í™”í•˜ëŠ” ë³´ì¡°ì›ì´ë‹¤. "
    "ì…ë ¥ í…ìŠ¤íŠ¸ëŠ” ì¡ë‹¤í•œ ê´‘ê³ /UIì”ì¬ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆë‹¤. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì¤‘ë³µ ì—†ì´ ì •ì œí•˜ë¼."
)

def llm_structurize(raw_text: str, meta_hint: Dict[str, str], model: str) -> Dict:
    ctx = clean_text(raw_text, 14000)
    user_msg = {"role": "user", "content": (
        "ë‹¤ìŒ ì±„ìš© ê³µê³  ì›ë¬¸ì„ êµ¬ì¡°í™”í•´ì¤˜.\n\n"
        f"[íŒíŠ¸] íšŒì‚¬ëª… í›„ë³´: {meta_hint.get('company_name','')}\n"
        f"[íŒíŠ¸] ì§ë¬´ëª… í›„ë³´: {meta_hint.get('job_title','')}\n"
        "--- ì›ë¬¸ ì‹œì‘ ---\n"
        f"{ctx}\n"
        "--- ì›ë¬¸ ë ---\n\n"
        "JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , í‚¤ëŠ” ë°˜ë“œì‹œ ì•„ë˜ë§Œ í¬í•¨:\n"
        "{"
        "\"company_name\": str, "
        "\"company_intro\": str, "
        "\"job_title\": str, "
        "\"responsibilities\": [str], "
        "\"qualifications\": [str], "
        "\"preferences\": [str]"
        "}\n"
        "- 'ìš°ëŒ€ ì‚¬í•­(preferences)'ì€ í‘œì‹œê°€ ìˆëŠ” í•­ëª©ë§Œ í¬í•¨.\n"
        "- ë¶ˆë¦¿/ì´ëª¨ì§€ ì œê±°, ê°„ê²°í™”, ì¤‘ë³µ ì œê±°."
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2, max_tokens=900,
            response_format={"type": "json_object"},
            messages=[{"role": "system", "content": PROMPT_SYSTEM_STRUCT}, user_msg]
        )
        data = json.loads(resp.choices[0].message.content)
    except Exception as e:
        data = {"company_name": meta_hint.get("company_name",""),
                "company_intro": meta_hint.get("company_intro","ì›ë¬¸ì´ ì •ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."),
                "job_title": meta_hint.get("job_title",""),
                "responsibilities": [], "qualifications": [], "preferences": [], "error": str(e)}

    for k in ["responsibilities","qualifications","preferences"]:
        arr = data.get(k, [])
        if not isinstance(arr, list): arr = []
        data[k] = clean_bullets(arr)

    if len(data.get("preferences", [])) < 1:
        rb = rule_based_sections(ctx)
        if rb.get("preferences"):
            data["preferences"] = clean_bullets(list(dict.fromkeys(data.get("preferences", []) + rb["preferences"])))
        else:
            kw_pref = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
            remain=[]; moved=[]
            for q in data.get("qualifications", []):
                (moved if kw_pref.search(q) else remain).append(q)
            data["preferences"] = clean_bullets(moved)
            data["qualifications"] = clean_bullets(remain)

    for k in ["company_name","company_intro","job_title"]:
        if isinstance(data.get(k), str):
            data[k] = re.sub(r"\s+", " ", data[k]).strip()
    return data

# -----------------------------------------------------------------------------
# íŒŒì¼ íŒŒì„œ / RAG
# -----------------------------------------------------------------------------
def read_pdf(data: bytes) -> str:
    if pypdf is None: return ""
    try:
        reader = pypdf.PdfReader(io.BytesIO(data))
        return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
    except Exception:
        return ""

def read_docx(data: bytes) -> str:
    if docx2txt is None: return ""
    try:
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=True) as tmp:
            tmp.write(data); tmp.flush()
            return docx2txt.process(tmp.name) or ""
    except Exception:
        return ""

def read_file_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt",".md")):
        for enc in ("utf-8","cp949","euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        return read_pdf(data)
    elif name.endswith(".docx"):
        return read_docx(data)
    return ""

def chunk(text: str, size: int = 600, overlap: int = 120) -> List[str]:
    t = re.sub(r"\s+"," ", text or "").strip()
    if not t: return []
    out, start = [], 0
    L = len(t)
    while start < L:
        end = min(L, start+size)
        out.append(t[start:end])
        if end == L: break
        start = max(0, end-overlap)
    return out

def embed_texts(texts: List[str], model_name: str) -> np.ndarray:
    if not texts: return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=model_name, input=texts)
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    return vecs

def l2_normalize(mat: np.ndarray) -> np.ndarray:
    if mat.size == 0: return mat
    n = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12
    return mat / n

def cosine_topk(matrix_n: np.ndarray, query_vec_n: np.ndarray, k: int = 4):
    if matrix_n.size == 0: return np.array([]), np.array([], dtype=int)
    sims = matrix_n @ query_vec_n.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_resume_chunks(query: str, chunks: List[str], embeds_norm: np.ndarray, k: int = 4):
    if not chunks or embeds_norm is None or embeds_norm.size == 0:
        return []
    qv = embed_texts([query], EMBED_MODEL)
    qv_n = l2_normalize(qv)
    scores, idxs = cosine_topk(embeds_norm, qv_n, k=k)
    return [(float(s), chunks[int(i)]) for s, i in zip(scores, idxs)]

PROMPT_SYSTEM_Q = (
    "ë„ˆëŠ” ì±„ìš©ë‹´ë‹¹ìë‹¤. íšŒì‚¬/ì§ë¬´ ë§¥ë½ê³¼ ì±„ìš©ìš”ê±´, ê·¸ë¦¬ê³  ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½ì„ í•¨ê»˜ ê³ ë ¤í•´ "
    "ë©´ì ‘ ì§ˆë¬¸ì„ í•œêµ­ì–´ë¡œ ìƒì„±í•œë‹¤. ì§ˆë¬¸ì€ ì„œë¡œ ê²¹ì¹˜ì§€ ì•Šê²Œ ë‹¤ì–‘í™”í•˜ê³ , ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ê·œëª¨/ë¦¬ìŠ¤í¬/íŠ¸ë ˆì´ë“œì˜¤í”„ë„ ì„ì–´ë¼."
)
PROMPT_SYSTEM_DRAFT = (
    "ë„ˆëŠ” ë©´ì ‘ ë‹µë³€ ì½”ì¹˜ë‹¤. íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´ê³¼ ì§€ì›ìì˜ ì´ë ¥ì„œ ìš”ì•½ì„ ê²°í•©í•´ "
    "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì´ˆì•ˆì„ STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)ë¡œ 8~12ë¬¸ì¥, í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤. "
    "ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì§€í‘œ/ìˆ˜ì¹˜/ê¸°ê°„/ì„íŒ©íŠ¸ë¥¼ í¬í•¨í•˜ë¼."
)
PROMPT_SYSTEM_SCORE_STRICT = (
    "ë„ˆëŠ” ë§¤ìš° ì—„ê²©í•œ í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ë¼. "
    "ê° ê¸°ì¤€ì€ 0~20 ì •ìˆ˜ì´ë©°, ì´ì ì€ ê¸°ì¤€ í•©ê³„(ìµœëŒ€ 100)ì™€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•œë‹¤. "
    "ê³¼ì¥/ëª¨í˜¸í•¨/ê·¼ê±° ë¶€ì¬/ìˆ«ì ì—†ëŠ” ì£¼ì¥/ì±…ì„ íšŒí”¼ ë“±ì„ ê°•í•˜ê²Œ ê°ì í•˜ë¼."
)
CRITERIA = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]

def llm_generate_one_question_with_resume(clean: Dict, level: str, model: str,
                                          resume_chunks: List[str], resume_embeds_norm: np.ndarray) -> str:
    hits = retrieve_resume_chunks("í•µì‹¬ í”„ë¡œì íŠ¸ì™€ ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½", resume_chunks, resume_embeds_norm, k=4)
    resume_context = "\n".join([f"- {t[:350]}" for _, t in hits])[:1200]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ìš”ê±´]\n{ctx}\n\n"
        f"[ì§€ì›ì ì´ë ¥ì„œ ìš”ì•½(ë°œì·Œ)]\n{resume_context}\n\n"
        f"[ìš”ì²­]\n- ë‚œì´ë„/ì—°ì°¨: {level}\n"
        f"- ì¤‘ë³µ/ìœ ì‚¬ë„ ì§€ì–‘, êµì§‘í•© ë˜ëŠ” ê³µë°±ì˜ì—­ ê²¨ëƒ¥\n"
        f"- í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë§Œ í•œ ì¤„ë¡œ ì¶œë ¥"
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.7, max_tokens=120,
            messages=[{"role":"system","content":PROMPT_SYSTEM_Q}, user_msg]
        )
        q = resp.choices[0].message.content.strip()
        q = re.sub(r"^\s*\d+[\).\s-]*","", q).split("\n")[0].strip()
        return q
    except Exception:
        return ""

def llm_draft_answer(clean: Dict, question: str, model: str,
                     resume_chunks: List[str], resume_embeds_norm: np.ndarray) -> str:
    hits = retrieve_resume_chunks(question, resume_chunks, resume_embeds_norm, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
        f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
        f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ STAR ê¸°ë°˜ í•œêµ­ì–´ ë‹µë³€ **ì´ˆì•ˆ**ì„ ì‘ì„±í•´ì¤˜."
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.5, max_tokens=700,
            messages=[{"role":"system","content":PROMPT_SYSTEM_DRAFT}, user_msg]
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        return ""

def llm_score_and_coach_strict(clean: Dict, question: str, answer: str, model: str,
                               resume_chunks: List[str], resume_embeds_norm: np.ndarray) -> Dict:
    hits = retrieve_resume_chunks(question + "\n" + answer[:800], resume_chunks, resume_embeds_norm, k=4)
    resume_text = "\n".join([f"- {t[:400]}" for _, t in hits])[:1600]
    ctx = json.dumps(clean, ensure_ascii=False)
    user_msg = {"role":"user","content":(
        f"[íšŒì‚¬/ì§ë¬´/ì±„ìš©ìš”ê±´]\n{ctx}\n\n"
        f"[ì§€ì›ì ì´ë ¥ì„œ ë°œì·Œ]\n{resume_text}\n\n"
        f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n"
        f"[ì§€ì›ì ë‹µë³€]\n{answer}\n\n"
        "ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ í•œêµ­ì–´ ì‘ë‹µ:\n"
        "{"
        "\"overall_score\": 0~100 ì •ìˆ˜,"
        "\"criteria\": [{\"name\":\"ë¬¸ì œì •ì˜\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ë°ì´í„°/ì§€í‘œ\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ì‹¤í–‰ë ¥/ì£¼ë„ì„±\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜\",\"score\":0~20,\"comment\":\"...\"},"
        "{\"name\":\"ê³ ê°ê°€ì¹˜\",\"score\":0~20,\"comment\":\"...\"}],"
        "\"strengths\": [\"...\"],"
        "\"risks\": [\"...\"],"
        "\"improvements\": [\"...\",\"...\",\"...\"],"
        "\"revised_answer\": \"STAR êµ¬ì¡°ë¡œ ê°„ê²°íˆ\""
        "}"
    )}
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2, max_tokens=900,
            response_format={"type":"json_object"},
            messages=[{"role":"system","content":PROMPT_SYSTEM_SCORE_STRICT}, user_msg]
        )
        data = json.loads(resp.choices[0].message.content)
        fixed=[]
        for name in CRITERIA:
            found=None
            for it in data.get("criteria", []):
                if str(it.get("name","")).strip()==name:
                    found=it; break
            if not found: found={"name":name,"score":0,"comment":""}
            sc = int(found.get("score",0)); sc=max(0,min(20,sc))
            found["score"]=sc; found["comment"]=str(found.get("comment","")).strip()
            fixed.append(found)
        total=sum(x["score"] for x in fixed)
        data["criteria"]=fixed
        data["overall_score"]=total
        for k in ["strengths","risks","improvements"]:
            arr=data.get(k,[]); 
            if not isinstance(arr,list): arr=[]
            data[k]=[str(x).strip() for x in arr if str(x).strip()][:5]
        data["revised_answer"]=str(data.get("revised_answer","")).strip()
        return data
    except Exception as e:
        return {"overall_score": 0,
                "criteria": [{"name": n, "score": 0, "comment": ""} for n in CRITERIA],
                "strengths": [], "risks": [], "improvements": [], "revised_answer": "",
                "error": str(e)}

# -----------------------------------------------------------------------------
# ì„¸ì…˜ ìƒíƒœ
# -----------------------------------------------------------------------------
def _init_state():
    defaults = {
        "clean_struct": None,
        "resume_raw": "",
        "resume_chunks": [],
        "resume_embeds": None,
        "resume_embeds_norm": None,
        "current_question": "",
        "answer_text": "",
        "last_result": None,
    }
    for k, v in defaults.items():
        if k not in st.session_state: st.session_state[k] = v
_init_state()

# -----------------------------------------------------------------------------
# UI: 1) ì±„ìš© ê³µê³  URL â†’ ìˆ˜ì§‘/ì •ì œ
# -----------------------------------------------------------------------------
st.header("1) ì±„ìš© ê³µê³  URL (Selenium ì „ìš©)")
url = st.text_input("ì±„ìš© ê³µê³  ìƒì„¸ URL", placeholder="ì›í‹°ë“œ/ì‚¬ëŒì¸/ì¡ì½”ë¦¬ì•„/ê¸°ì—… ì±„ìš© í˜ì´ì§€ URL")

if st.button("ì›ë¬¸ ìˆ˜ì§‘ â†’ ì •ì œ (Selenium ONLY)", type="primary"):
    if not url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("Seleniumìœ¼ë¡œ ì›ë¬¸ ìˆ˜ì§‘ ì¤‘..."):
            raw, meta, soup_html = fetch_all_text_selenium_only(url.strip(), timeout=SELENIUM_TIMEOUT)
            hint = extract_company_meta(soup_html)

        st.caption(f"ìˆ˜ì§‘ ì†ŒìŠ¤: {meta.get('source')} Â· ê¸¸ì´: {meta.get('len')}")
        if not raw:
            st.error("Selenium ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒë‹¨ ì—ëŸ¬ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            with st.spinner("LLMìœ¼ë¡œ ì •ì œ ì¤‘..."):
                clean = llm_structurize(raw, hint, CHAT_MODEL)
            if not clean.get("preferences"):
                rb = rule_based_sections(raw)
                if rb.get("preferences"): clean["preferences"] = clean_bullets(rb["preferences"])
            st.session_state.clean_struct = clean
            try:
                kw_cnt = len(re.findall(r"(ìš°ëŒ€|ìš°ëŒ€ì‚¬í•­|Preferred)", raw, flags=re.I))
                st.caption(f"ì§„ë‹¨: ì›ë¬¸ ë‚´ 'ìš°ëŒ€' ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì • ë“±ì¥ ìˆ˜ â‰ˆ {kw_cnt}")
            except Exception:
                pass
            st.success("ì •ì œ ì™„ë£Œ!")

# -----------------------------------------------------------------------------
# UI: 2) íšŒì‚¬ ìš”ì•½
# -----------------------------------------------------------------------------
st.header("2) íšŒì‚¬ ìš”ì•½")
clean = st.session_state.clean_struct
if clean:
    st.markdown(f"**íšŒì‚¬ëª…:** {clean.get('company_name','-')}")
    st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½):** {clean.get('company_intro','-')}")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼(ì§ë¬´ëª…):** {clean.get('job_title','-')}")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´**")
        for b in clean.get("responsibilities", []): st.markdown(f"- {b}")
    with c2:
        st.markdown("**ìê²© ìš”ê±´**")
        for b in clean.get("qualifications", []): st.markdown(f"- {b}")
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­**")
        prefs = clean.get("preferences", [])
        if prefs:
            for b in prefs: st.markdown(f"- {b}")
        else:
            st.caption("ìš°ëŒ€ ì‚¬í•­ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
else:
    st.info("ë¨¼ì € URLì„ ì •ì œí•´ ì£¼ì„¸ìš”.")

st.divider()

# -----------------------------------------------------------------------------
# UI: 3) ì´ë ¥ì„œ ì—…ë¡œë“œ/ì¸ë±ì‹±
# -----------------------------------------------------------------------------
st.header("3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ")
uploads = st.file_uploader("ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ ê°€ëŠ¥", type=["pdf","txt","md","docx"], accept_multiple_files=True)
_RESUME_CHUNK = 500; _RESUME_OVLP = 100

if st.button("ì´ë ¥ì„œ ì¸ë±ì‹±", type="secondary"):
    if not uploads:
        st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        all_text=[]
        for up in uploads:
            t = read_file_text(up)
            if t: all_text.append(t)
        resume_text = "\n\n".join(all_text)
        if not resume_text.strip():
            st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            chunks = chunk(resume_text, size=_RESUME_CHUNK, overlap=_RESUME_OVLP)
            with st.spinner("ì´ë ¥ì„œ ë²¡í„°í™” ì¤‘..."):
                embeds = embed_texts(chunks, EMBED_MODEL)
                embeds_norm = l2_normalize(embeds)
            st.session_state.resume_raw = resume_text
            st.session_state.resume_chunks = chunks
            st.session_state.resume_embeds = embeds
            st.session_state.resume_embeds_norm = embeds_norm
            st.success(f"ì¸ë±ì‹± ì™„ë£Œ (ì²­í¬ {len(chunks)}ê°œ)")

st.divider()

# -----------------------------------------------------------------------------
# UI: 4) ìì†Œì„œ ìƒì„±
# -----------------------------------------------------------------------------
st.header("4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„±")
topic = st.text_input("íšŒì‚¬ ìš”ì²­ ì£¼ì œ(ì„ íƒ)", placeholder="ì˜ˆ: ì§ë¬´ ì§€ì›ë™ê¸° / í˜‘ì—… ê²½í—˜ / ë¬¸ì œí•´ê²° ì‚¬ë¡€ ë“±")

def build_cover_letter(clean_struct: Dict, resume_text: str, topic_hint: str, model: str) -> str:
    company = json.dumps({"clean":clean_struct}, ensure_ascii=False)
    resume_snippet = resume_text.strip()[:9000]
    system = ("ë„ˆëŠ” í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì „ë¬¸ê°€ë‹¤. ì±„ìš© ê³µê³ ì˜ íšŒì‚¬/ì§ë¬´ ìš”ê±´ê³¼ í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ì°¸ê³ í•´ "
              "íšŒì‚¬ íŠ¹í™” ìì†Œì„œë¥¼ ì‘ì„±í•œë‹¤. ê³¼ì¥/í—ˆìœ„ëŠ” ê¸ˆì§€í•˜ê³ , ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ì„íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´í™”í•œë‹¤.")
    req = (f"íšŒì‚¬ ì¸¡ ìš”ì²­ ì£¼ì œëŠ” '{topic_hint.strip()}' ì´ë‹¤. ì´ ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•˜ë¼."
           if topic_hint and topic_hint.strip()
           else "íŠ¹ì • ì£¼ì œ ìš”ì²­ì´ ì—†ìœ¼ë¯€ë¡œ, ì±„ìš© ê³µê³ ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì§€ì›ë™ê¸°ì™€ ì§ë¬´ì í•©ì„±ì„ ê°•ì¡°í•˜ë¼.")
    user = (f"[íšŒì‚¬/ì§ë¬´ ìš”ì•½(JSON)]\n{company}\n\n"
            f"[í›„ë³´ì ì´ë ¥ì„œ(ìš”ì•½ ê°€ëŠ¥)]\n{resume_snippet}\n\n"
            f"[ì‘ì„± ì§€ì‹œ]\n- {req}\n"
            "- ë¶„ëŸ‰: 600~900ì\n"
            "- êµ¬ì„±: 1) ì§€ì› ë™ê¸° 2) ì§ë¬´ ê´€ë ¨ í•µì‹¬ ì—­ëŸ‰Â·ê²½í—˜ 3) ì„±ê³¼/ì§€í‘œ 4) ì…ì‚¬ í›„ ê¸°ì—¬ ë°©ì•ˆ 5) ë§ˆë¬´ë¦¬\n"
            "- ìì—°ìŠ¤ëŸ½ê³  ì§„ì •ì„± ìˆëŠ” 1ì¸ì¹­ ì„œìˆ . ë¬¸ì¥ê³¼ ë¬¸ë‹¨ ê°€ë…ì„±ì„ ìœ ì§€.\n"
            "- ë¶ˆí•„ìš”í•œ ë¯¸ì‚¬ì—¬êµ¬/ì¤‘ë³µ/ê´‘ê³  ë¬¸êµ¬ ì‚­ì œ.")
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.4, max_tokens=800,
            messages=[{"role":"system","content":system},{"role":"user","content":user}]
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(ìì†Œì„œ ìƒì„± ì‹¤íŒ¨: {e})"

if st.button("ìì†Œì„œ ìƒì„±", type="primary"):
    if not st.session_state.clean_struct:
        st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
    elif not st.session_state.resume_raw.strip():
        st.warning("ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ì´ë ¥ì„œ ì¸ë±ì‹±'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ìì†Œì„œ ìƒì„± ì¤‘..."):
            cover = build_cover_letter(st.session_state.clean_struct, st.session_state.resume_raw, topic, CHAT_MODEL)
        st.subheader("ìì†Œì„œ (ìƒì„± ê²°ê³¼)")
        st.write(cover)
        st.download_button("ìì†Œì„œ TXT ë‹¤ìš´ë¡œë“œ", data=cover.encode("utf-8"), file_name="cover_letter.txt", mime="text/plain")

st.divider()

# -----------------------------------------------------------------------------
# UI: 5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ
# -----------------------------------------------------------------------------
st.header("5) ì§ˆë¬¸ ìƒì„± & ë‹µë³€ ì´ˆì•ˆ (RAG ê²°í•©)")
level = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"], index=0)

col1, col2 = st.columns(2)
with col1:
    if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", type="primary"):
        if not st.session_state.clean_struct:
            st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
        else:
            q = llm_generate_one_question_with_resume(
                st.session_state.clean_struct, level, CHAT_MODEL,
                st.session_state.resume_chunks, st.session_state.resume_embeds_norm
            )
            if q:
                st.session_state.current_question = q
                st.session_state.answer_text = ""
                st.session_state.last_result = None
                st.success("ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")

with col2:
    if st.button("RAGë¡œ ë‹µë³€ ì´ˆì•ˆ ìƒì„±", type="secondary"):
        if not st.session_state.current_question:
            st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        else:
            draft = llm_draft_answer(
                st.session_state.clean_struct, st.session_state.current_question, CHAT_MODEL,
                st.session_state.resume_chunks, st.session_state.resume_embeds_norm
            )
            if draft:
                st.session_state.answer_text = draft
                st.success("ì´ˆì•ˆ ìƒì„± ì™„ë£Œ!")
            else:
                st.error("ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨")

st.text_area("ì§ˆë¬¸", value=st.session_state.current_question, height=100)
st.text_area("ë‚˜ì˜ ë‹µë³€ (ì´ˆì•ˆì„ í¸ì§‘í•´ ì™„ì„±í•˜ì„¸ìš”)", height=200, key="answer_text")

# -----------------------------------------------------------------------------
# UI: 6) ì±„ì  & ì½”ì¹­
# -----------------------------------------------------------------------------
st.header("6) ì±„ì  & ì½”ì¹­")
if st.button("ì±„ì  & ì½”ì¹­ ì‹¤í–‰", type="primary"):
    if not st.session_state.current_question:
        st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì±„ì /ì½”ì¹­ ì¤‘..."):
            res = llm_score_and_coach_strict(
                st.session_state.clean_struct,
                st.session_state.current_question,
                st.session_state.answer_text,
                CHAT_MODEL,
                st.session_state.resume_chunks,
                st.session_state.resume_embeds_norm
            )
        st.session_state.last_result = res
        st.success("ì±„ì /ì½”ì¹­ ì™„ë£Œ!")

st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")
last = st.session_state.last_result
if last:
    st.metric("ì´ì (/100)", last.get("overall_score", 0))
    st.markdown("**ê¸°ì¤€ë³„ ì ìˆ˜ & ì½”ë©˜íŠ¸**")
    for it in last.get("criteria", []):
        st.markdown(f"- **{it['name']}**: {it['score']}/20 â€” {it.get('comment','')}")
    if last.get("strengths"):
        st.markdown("**ê°•ì **")
        for s in last["strengths"]: st.markdown(f"- {s}")
    if last.get("risks"):
        st.markdown("**ê°ì  ìš”ì¸/ë¦¬ìŠ¤í¬**")
        for r in last["risks"]: st.markdown(f"- {r}")
    if last.get("improvements"):
        st.markdown("**ê°œì„  í¬ì¸íŠ¸**")
        for im in last["improvements"]: st.markdown(f"- {im}")
    if last.get("revised_answer"):
        st.markdown("**ìˆ˜ì •ë³¸ ë‹µë³€ (STAR)**")
        st.write(last["revised_answer"])
